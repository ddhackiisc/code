{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Draw, Descriptors, AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('data/trainDILI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        BrC(Cl)C(F)(F)F\n",
       "1                            Br[C@@H](C(C)C)C(=O)NC(=O)N\n",
       "2      Brc1[nH]c2c3c1CC1N(CC(C=C1c3ccc2)C(=O)N[C@]1(O...\n",
       "3        Brc1c(nc(nc1Oc1c(cc(cc1C)C#N)C)Nc1ccc(cc1)C#N)N\n",
       "4                             Brc1c2nccnc2ccc1NC=1NCCN=1\n",
       "                             ...                        \n",
       "961                           s1cccc1CN(CCN(C)C)c1ncccc1\n",
       "962    s1cccc1C\\C(=C/c1n(Cc2ccc(cc2)C(O)=O)c(nc1)CCCC...\n",
       "963                               s1cccc1\\C=C\\C1=NCCCN1C\n",
       "964          s1ccnc1NC(=O)C=1N(S(=O)(=O)c2c(cccc2)C=1O)C\n",
       "965    s1cncc1COC(=O)NC(Cc1ccccc1)C(O)CC(NC(=O)C(NC(=...\n",
       "Name: SMILES, Length: 966, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(smiles):\n",
    "    return Chem.MolToSmiles(Chem.MolFromSmiles(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['cleanSMILES'] = traindata['SMILES'].apply(canonicalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Drug_Name</th>\n",
       "      <th>IsDILI</th>\n",
       "      <th>cleanSMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BrC(Cl)C(F)(F)F</td>\n",
       "      <td>Halothane</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FC(F)(F)C(Cl)Br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Br[C@@H](C(C)C)C(=O)NC(=O)N</td>\n",
       "      <td>Bromisoval</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CC(C)[C@H](Br)C(=O)NC(N)=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brc1[nH]c2c3c1CC1N(CC(C=C1c3ccc2)C(=O)N[C@]1(O...</td>\n",
       "      <td>Bromocriptine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CC(C)CC1C(=O)N2CCCC2[C@@]2(O)O[C@@](NC(=O)C3C=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brc1c(nc(nc1Oc1c(cc(cc1C)C#N)C)Nc1ccc(cc1)C#N)N</td>\n",
       "      <td>Etravirine</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cc1cc(C#N)cc(C)c1Oc1nc(Nc2ccc(C#N)cc2)nc(N)c1Br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brc1c2nccnc2ccc1NC=1NCCN=1</td>\n",
       "      <td>Brimonidine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Brc1c(NC2=NCCN2)ccc2nccnc12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>s1cccc1CN(CCN(C)C)c1ncccc1</td>\n",
       "      <td>Methapyrilene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CN(C)CCN(Cc1cccs1)c1ccccn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>s1cccc1C\\C(=C/c1n(Cc2ccc(cc2)C(O)=O)c(nc1)CCCC...</td>\n",
       "      <td>Eprosartan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCCCc1ncc(/C=C(\\Cc2cccs2)C(=O)O)n1Cc1ccc(C(=O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>s1cccc1\\C=C\\C1=NCCCN1C</td>\n",
       "      <td>Pyrantel</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CN1CCCN=C1/C=C/c1cccs1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>s1ccnc1NC(=O)C=1N(S(=O)(=O)c2c(cccc2)C=1O)C</td>\n",
       "      <td>Sudoxicam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CN1C(C(=O)Nc2nccs2)=C(O)c2ccccc2S1(=O)=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>s1cncc1COC(=O)NC(Cc1ccccc1)C(O)CC(NC(=O)C(NC(=...</td>\n",
       "      <td>Ritonavir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CC(C)c1nc(CN(C)C(=O)NC(C(=O)NC(Cc2ccccc2)CC(O)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>966 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                SMILES      Drug_Name  IsDILI  \\\n",
       "0                                      BrC(Cl)C(F)(F)F      Halothane     1.0   \n",
       "1                          Br[C@@H](C(C)C)C(=O)NC(=O)N     Bromisoval     1.0   \n",
       "2    Brc1[nH]c2c3c1CC1N(CC(C=C1c3ccc2)C(=O)N[C@]1(O...  Bromocriptine     0.0   \n",
       "3      Brc1c(nc(nc1Oc1c(cc(cc1C)C#N)C)Nc1ccc(cc1)C#N)N     Etravirine     1.0   \n",
       "4                           Brc1c2nccnc2ccc1NC=1NCCN=1    Brimonidine     0.0   \n",
       "..                                                 ...            ...     ...   \n",
       "961                         s1cccc1CN(CCN(C)C)c1ncccc1  Methapyrilene     1.0   \n",
       "962  s1cccc1C\\C(=C/c1n(Cc2ccc(cc2)C(O)=O)c(nc1)CCCC...     Eprosartan     0.0   \n",
       "963                             s1cccc1\\C=C\\C1=NCCCN1C       Pyrantel     0.0   \n",
       "964        s1ccnc1NC(=O)C=1N(S(=O)(=O)c2c(cccc2)C=1O)C      Sudoxicam     1.0   \n",
       "965  s1cncc1COC(=O)NC(Cc1ccccc1)C(O)CC(NC(=O)C(NC(=...      Ritonavir     1.0   \n",
       "\n",
       "                                           cleanSMILES  \n",
       "0                                      FC(F)(F)C(Cl)Br  \n",
       "1                           CC(C)[C@H](Br)C(=O)NC(N)=O  \n",
       "2    CC(C)CC1C(=O)N2CCCC2[C@@]2(O)O[C@@](NC(=O)C3C=...  \n",
       "3      Cc1cc(C#N)cc(C)c1Oc1nc(Nc2ccc(C#N)cc2)nc(N)c1Br  \n",
       "4                          Brc1c(NC2=NCCN2)ccc2nccnc12  \n",
       "..                                                 ...  \n",
       "961                         CN(C)CCN(Cc1cccs1)c1ccccn1  \n",
       "962  CCCCc1ncc(/C=C(\\Cc2cccs2)C(=O)O)n1Cc1ccc(C(=O)...  \n",
       "963                             CN1CCCN=C1/C=C/c1cccs1  \n",
       "964           CN1C(C(=O)Nc2nccs2)=C(O)c2ccccc2S1(=O)=O  \n",
       "965  CC(C)c1nc(CN(C)C(=O)NC(C(=O)NC(Cc2ccccc2)CC(O)...  \n",
       "\n",
       "[966 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validSMILES(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = traindata['cleanSMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        FC(F)(F)C(Cl)Br\n",
       "1                             CC(C)[C@H](Br)C(=O)NC(N)=O\n",
       "2      CC(C)CC1C(=O)N2CCCC2[C@@]2(O)O[C@@](NC(=O)C3C=...\n",
       "3        Cc1cc(C#N)cc(C)c1Oc1nc(Nc2ccc(C#N)cc2)nc(N)c1Br\n",
       "4                            Brc1c(NC2=NCCN2)ccc2nccnc12\n",
       "                             ...                        \n",
       "961                           CN(C)CCN(Cc1cccs1)c1ccccn1\n",
       "962    CCCCc1ncc(/C=C(\\Cc2cccs2)C(=O)O)n1Cc1ccc(C(=O)...\n",
       "963                               CN1CCCN=C1/C=C/c1cccs1\n",
       "964             CN1C(C(=O)Nc2nccs2)=C(O)c2ccccc2S1(=O)=O\n",
       "965    CC(C)c1nc(CN(C)C(=O)NC(C(=O)NC(Cc2ccccc2)CC(O)...\n",
       "Name: cleanSMILES, Length: 966, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLANK_CHAR = '&'\n",
    "def get_alphabet(d_set):\n",
    "    letters = set()\n",
    "    for word in d_set:\n",
    "        letters.update(set(word))\n",
    "    return list([ BLANK_CHAR ]) + sorted(list(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['&',\n",
       " '#',\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " '-',\n",
       " '/',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '=',\n",
       " '@',\n",
       " 'B',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'I',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'S',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " 'c',\n",
       " 'l',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alphabet(data)\n",
    "\n",
    "use content from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#inp_alphabet = \".ab@_yz\"\n",
    "#out_alphabet = \".FT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_encode_strlist(X, max, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    #example: {'.': 0, 'a': 1, 'b': 2}\n",
    "    X = [ [char_to_int[char] for char in x1.strip()] for x1 in X]\n",
    "    X1 = [ x + [0] * (max - len(x)) for x in X]      # pad to max\n",
    "    #example: simple_encode_inp([\"aa\",\"bb\"],3) gives [[1, 1, 0], [2, 2, 0]]\n",
    "    return X1\n",
    "\n",
    "def simple_decode_idxlist1(enc_data, alphabet):\n",
    "    idx2char = alphabet\n",
    "    out = [idx2char[int(x)] for x in enc_data]\n",
    "    out = ''.join(out)\n",
    "    return out\n",
    "\n",
    "def simple_decode_idxlist(d_arr, alphabet):\n",
    "    return [simple_decode_idxlist1(d, alphabet) for d in d_arr]\n",
    "\n",
    "def load_data_file(filename):\n",
    "    data = pandas.read_csv(filename, header=None)\n",
    "    print (\"=== loaded data shape: \", data.shape)\n",
    "    print (\"=== data \", data)\n",
    "\n",
    "    X_pandas = data[0]\n",
    "    X_pandas = [ x.strip() for x in X_pandas]\n",
    "    X_max = max([ len(x) for x in X_pandas])\n",
    "\n",
    "    Y_pandas = data[1]\n",
    "    Y_pandas = [ y.strip() for y in Y_pandas]\n",
    "    Y_max = max([ len(y) for y in Y_pandas])\n",
    "\n",
    "    if X_max != Y_max:\n",
    "        print(\"max len mismatch in X and Y\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    seq_max = X_max\n",
    "    return X_pandas, Y_pandas, seq_max\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "def ohe_singleletter(val, max):\n",
    "    letter = [0 for _ in range(max)]\n",
    "    letter[val] = 1\n",
    "    return letter\n",
    "\n",
    "\n",
    "def simple_to_onehot(D, alphabet):\n",
    "    itemlist = list()\n",
    "    for d in D:\n",
    "        item = list()\n",
    "        for e in d:\n",
    "            l = ohe_singleletter(e, len(alphabet))\n",
    "            item.append(l)\n",
    "            #print(e, l)\n",
    "        #print(d, \"------\",  item)\n",
    "        itemlist.append(item)\n",
    "    return itemlist\n",
    "\n",
    "def onehot_decode_to_simple1(d):\n",
    "    arr = np.array(d)\n",
    "    idx = arr.argmax(1)\n",
    "    return idx\n",
    "\n",
    "def onehot_decode_to_simple(D):\n",
    "    arr = np.array(D)\n",
    "    idx = arr.argmax(2)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def decode_inp(input):\n",
    "    input = onehot_decode_to_simple1(input)\n",
    "    input = simple_decode_idxlist1(input, Inp_alphabet)\n",
    "    return input\n",
    "\n",
    "def decode_out(output):\n",
    "    output = simple_decode_idxlist1(output, Out_alphabet)\n",
    "    return output\n",
    "\n",
    "#################################\n",
    "\n",
    "X_data, Y_data, train_Seq_max = load_data_file('rnn3-train-data.txt')\n",
    "Num_io_data = len(X_data)\n",
    "\n",
    "# set Seq_max to  max in both test and train\n",
    "_, _, test_Seq_max = load_data_file('rnn3-test-data.txt')\n",
    "Seq_max = max([train_Seq_max, test_Seq_max])\n",
    "\n",
    "# if Seq_max <= Arg_seqmaxlen:\n",
    "#     print(\"=== updating Seq_max from\", Seq_max, \" to \", Arg_seqmaxlen)\n",
    "#     Seq_max = Arg_seqmaxlen\n",
    "# else:\n",
    "#     print(\"=== seq max len mismatch\", Seq_max, Arg_seqmaxlen)\n",
    "#     sys.exit(1)\n",
    "\n",
    "if Arg_dataset_name == 'tls':\n",
    "    Inp_alphabet =  [BLANK_CHAR, 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    Out_alphabet =  [ 'E', 'S', 'X', 'H', 'U', 'F', 'W', 'I', 'A', 'C' ]\n",
    "    Out_alphabet =  list([ BLANK_CHAR ]) + sorted(Out_alphabet)\n",
    "\n",
    "else:\n",
    "    Inp_alphabet = get_alphabet(X_data)\n",
    "    Out_alphabet = get_alphabet(Y_data)\n",
    "#print(''.join(Inp_alphabet))\n",
    "#print(''.join(Out_alphabet))\n",
    "print(\"=== inp_alphabet:\", Inp_alphabet)\n",
    "print(\"=== out_alphabet:\", Out_alphabet)\n",
    "\n",
    "X_data = simple_encode_strlist(X_data, Seq_max, Inp_alphabet)\n",
    "Y_data = simple_encode_strlist(Y_data, Seq_max, Out_alphabet)\n",
    "\n",
    "#print(X_data)\n",
    "\n",
    "Num_samples = len(X_data)\n",
    "\n",
    "#print(simple_decode_idxlist1(X_data[0], Inp_alphabet))\n",
    "\n",
    "#print(simple_decode_idxlist(X_data, Inp_alphabet))\n",
    "#print(simple_decode_idxlist(Y_data, Out_alphabet))\n",
    "\n",
    "X_ohe = simple_to_onehot(X_data, Inp_alphabet)\n",
    "#print(X_ohe)\n",
    "\n",
    "#print(onehot_decode_to_simple(X_ohe))\n",
    "#print(\"------------\")\n",
    "\n",
    "#################################\n",
    "class Model_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'RNN'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'LSTM'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        # LSTM hidden state is a tuple (h_0, c_0)\n",
    "        h_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        c_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        hidden = (h_0, c_0)\n",
    "        return hidden\n",
    "\n",
    "#################################\n",
    "\n",
    "Inp_size = len(Inp_alphabet)\n",
    "Out_size = len(Out_alphabet)\n",
    "torch.manual_seed(2.7321)\n",
    "\n",
    "#################################\n",
    "num_classes = Out_size          #  XXX why need a separate num_classes,  when hidden_size would do?\n",
    "input_size = Inp_size  # this parameter is for the view function to know how large\n",
    "#the one hot vector is supposed to be\n",
    "\n",
    "hidden_size = Out_size #\n",
    "batch_size = 1   # id dont understand this\n",
    "seq_len = Seq_max\n",
    "num_layers = Arg_num_layers  # num-layers of rnn\n",
    "#learning_rate = 0.01\n",
    "momentum = 0.1\n",
    "\n",
    "\n",
    "\n",
    "vizdelay = 30\n",
    "################################\n",
    "\n",
    "# function to reduce learning rate based on accuracy.\n",
    "# it returns a new optimizer\n",
    "def get_new_optimizer(learning_rate, accuracy):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if accuracy >= 50:\n",
    "        new_learning_rate = learning_rate * 0.9\n",
    "    if accuracy >= 60:\n",
    "        new_learning_rate = learning_rate * 0.8\n",
    "    if accuracy >= 70:\n",
    "        new_learning_rate = learning_rate * 0.7\n",
    "    if accuracy >= 80:\n",
    "        new_learning_rate = learning_rate * 0.5\n",
    "    if accuracy >= 90:\n",
    "        new_learning_rate = learning_rate * 0.4\n",
    "    if accuracy >= 95:\n",
    "        new_learning_rate = learning_rate * 0.2\n",
    "    if accuracy >= 98:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=new_learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "############## Model Training code ##########################\n",
    "def train_phase():\n",
    "    global Optimizer\n",
    "    MIN_LOSS = float('inf')\n",
    "    MIN_LOSS_epoch_counter = 0\n",
    "    MAX_ACC_epoch_counter = 0\n",
    "\n",
    "    final_train_epoch=0\n",
    "    final_train_accuracy=0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        Optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        errcount = 0\n",
    "        # if epoch % 30 ==0:\n",
    "        #     evaluate_model(inputs,labels)\n",
    "        for input, label in zip(Inputs, Labels):\n",
    "            # input = input.unsqueeze(0)\n",
    "            hidden = model.init_hidden() #we reset the RNN to its initial state\n",
    "            hidden, output = model(input, hidden) #run the model\n",
    "\n",
    "            val, idx = output.max(1)\n",
    "            expected = decode_out(label)\n",
    "            trained = decode_out(idx)\n",
    "\n",
    "            if trained != expected:\n",
    "                errcount += 1\n",
    "\n",
    "            if epoch % vizdelay == 0:\n",
    "                if (trained != expected):\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained, \"   *****\")\n",
    "                else:\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained)\n",
    "            loss += Criterion(output, label) #add the current sample error to loss\n",
    "\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "\n",
    "        ########### Within loop plotting and logging ########\n",
    "        if loss.data < MIN_LOSS:\n",
    "            MIN_LOSS = loss.data\n",
    "            MIN_LOSS_epoch_counter = epoch   # reset\n",
    "\n",
    "        accuracy = 100.0 * (Num_io_data - errcount) / Num_io_data\n",
    "        print(\"Epoch: %d, loss: %1.3f         errcount: %d  accuracy: %1.1f%%\\n\" % (epoch+1, loss.data, errcount, accuracy))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        plot_data.append([loss.data.item(), accuracy])\n",
    "        t = [ x for x in range(0, epoch)]\n",
    "        p = np.array(plot_data)\n",
    "        plt_ax1.plot(p[:, 0], color='red')\n",
    "        plt_ax2.plot(p[:, 1], color='blue')\n",
    "        plt.pause(0.001)\n",
    "        #plt.show()\n",
    "\n",
    "        final_train_epoch = epoch\n",
    "        final_train_accuracy = accuracy\n",
    "        if BREAK_EARLY == True:\n",
    "            global Train_end_reason\n",
    "            if (epoch - MIN_LOSS_epoch_counter) >= 20:\n",
    "                Train_end_reason = \"=== no new low of training_loss seen for last 20 epochs; stop training\"\n",
    "                print(Train_end_reason)\n",
    "                break\n",
    "\n",
    "            if accuracy >= 99.9:\n",
    "                MAX_ACC_epoch_counter += 1\n",
    "                # stop, if accuracy stays at ~100 for 10 epochs\n",
    "                if MAX_ACC_epoch_counter >= 10:\n",
    "                    Train_end_reason = \"=== maximal accuracy seen for last 10 epochs; stop training\"\n",
    "                    print(Train_end_reason)\n",
    "                    time.sleep(4)\n",
    "                    break\n",
    "\n",
    "        # update optimizer with changed lr - depending on accuracy\n",
    "        Optimizer = get_new_optimizer(learning_rate, accuracy)\n",
    "    return final_train_accuracy, final_train_epoch\n",
    "\n",
    "\n",
    "####################### Model evaluation code ################\n",
    "def evaluate_model_1(xdata, ydata):\n",
    "    x_one_hot = simple_to_onehot(xdata, Inp_alphabet)\n",
    "    inputs = Variable(torch.Tensor(x_one_hot))\n",
    "    labels = Variable(torch.LongTensor(ydata))\n",
    "\n",
    "    err_count = 0\n",
    "    for input, label in zip(inputs, labels):\n",
    "        expected = decode_out(label)\n",
    "        hidden = model.init_hidden()\n",
    "        hidden, output = model(input, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        predicted = decode_out(idx)\n",
    "        if (expected != predicted):\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted, \"   *****\")\n",
    "            err_count += 1\n",
    "        else:\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted)\n",
    "    return (err_count)\n",
    "\n",
    "def evaluate_model(xdata, ydata, seqmax):\n",
    "    xdata = simple_encode_strlist(xdata, seqmax, Inp_alphabet)\n",
    "    ydata = simple_encode_strlist(ydata, seqmax, Out_alphabet)\n",
    "    return evaluate_model_1(xdata, ydata)\n",
    "\n",
    "################################################################################\n",
    "def validation_phase():\n",
    "    print(\"============================= validation inputs ===========================\")\n",
    "    validation_inputs, validation_outputs, seqmax = load_data_file('rnn3-train-data.txt')\n",
    "    num_io_data = len(validation_inputs)\n",
    "    if seqmax != Seq_max:\n",
    "        print(\"seqmax mismatch\", seqmax, Seq_max)\n",
    "        sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "    errcount = evaluate_model(validation_inputs, validation_outputs, seqmax)\n",
    "    val_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    if errcount > 0:\n",
    "        print(\"VALIDATION FAILED:  errors: \", errcount, \"accuracy:\", val_accuracy)\n",
    "    else:\n",
    "        print(\"Validation Passed\")\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def test_phase():\n",
    "    print(\"============================= test inputs ===========================\")\n",
    "    test_inputs, test_outputs, seqmax = load_data_file('rnn3-test-data.txt')\n",
    "    num_io_data = len(test_inputs)\n",
    "    #if seqmax != Seq_max: #this could happen just due to randomness #todo\n",
    "    #    print(\"WARN: seqmax mismatch\")     ## possible to have diff seq max from the train data\n",
    "    #    sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "\n",
    "    errcount = evaluate_model(test_inputs, test_outputs, seqmax)\n",
    "    test_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    print(\"Testset size: \", len(test_inputs),\"Test errors: \", errcount, \"accuracy:\", test_accuracy)\n",
    "    return test_accuracy\n",
    "\n",
    "################################################################################\n",
    "\n",
    "########################################################\n",
    "#Instantiate RNN model\n",
    "#model = Model_RNN()\n",
    "Rnn_type = '###'\n",
    "model = Model_LSTM()\n",
    "Rnn_type = model.Rnn_type\n",
    "\n",
    "####################### Plotting code ##################\n",
    "\n",
    "plt.ion()\n",
    "plt_fig, plt_ax1 = plt.subplots()\n",
    "plt_ax2 = plt_ax1.twinx()\n",
    "plt_ax1.set_xlabel('epoch')\n",
    "plt_ax1.set_ylabel('loss', color='red')\n",
    "plt_ax2.set_ylabel('accuracy', color='blue')\n",
    "\n",
    "txt = \"Dataset: \" + Arg_dataset_name + \"\\nrnn_type: \"+ Rnn_type + \"\\nnum_layers: \"+ str(num_layers) + \"\\nnum_io_samples: \"+ str(Num_io_data) + \"\\nseq_max_len: \"+ str(Seq_max)\n",
    "txt += \"\\nBase_LR: \" + str(learning_rate)\n",
    "\n",
    "#plt_fig = plt.figure()\n",
    "plt_fig.text(.5, .2, txt, ha='center', transform=plt_ax1.transAxes)\n",
    "plt.pause(0.001)\n",
    "\n",
    "plot_data = []\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "################## Load trained model ###############\n",
    "from pathlib import Path\n",
    "modelimagefile = \"rnn3model.\" + Imagesuffix + \".pt\"\n",
    "if Path(modelimagefile).is_file():\n",
    "    # file exists\n",
    "    model.load_state_dict(torch.load(modelimagefile))\n",
    "    model.eval()\n",
    "    print(\"=== Model was loaded from \" + modelimagefile)\n",
    "\n",
    "####################################################\n",
    "Criterion = torch.nn.CrossEntropyLoss()\n",
    "Optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "# X is OH encoded\n",
    "# Y is simple encoded\n",
    "# this is the format needed by pytorch\n",
    "Inputs = Variable(torch.Tensor(X_ohe))\n",
    "Labels = Variable(torch.LongTensor(Y_data))\n",
    "\n",
    "print(\"Input shape: \", Inputs.size())\n",
    "print(\"Output shape: \", Labels.size())\n",
    "\n",
    "final_train_accuracy, final_train_epoch = train_phase()\n",
    "\n",
    "############ final plotting and logging ################\n",
    "#plt.plot(plot_data)\n",
    "#plt.waitforbuttonpress()\n",
    "plt.savefig('rnn3-ttt.png',  bbox_inches='tight')\n",
    "plt.show()\n",
    "########################################################\n",
    "\n",
    "val_accuracy = validation_phase()\n",
    "test_accuracy = test_phase()\n",
    "\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"=== rnn_type:\", Rnn_type,\n",
    "        \"num_layers:\", num_layers,\n",
    "        \"num_io_samples (train): \", Num_io_data,\n",
    "        \"seq_max_len:\", Seq_max)\n",
    "\n",
    "print(\"\\n Training end due to: \", Train_end_reason)\n",
    "print(\"\\n=== final_train_epoch:\", final_train_epoch)\n",
    "print(\"=== final_train_accuracy:  %1.2f%%       \\n=== val_accuracy:  %1.2f%%        \\n=== test_accuracy:  %1.2f%%\\n\"\n",
    "        %  (final_train_accuracy, val_accuracy, test_accuracy))\n",
    "\n",
    "\n",
    "######################### save trained model #######################\n",
    "torch.save(model.state_dict(), modelimagefile)\n",
    "print(\"=== Model was saved as \" + modelimagefile)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train proportion:  0.7494824016563147\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "print(\"Train proportion: \",len(Xtrain)/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9378453038674033\n",
      "Test accuracy:  0.6074380165289256\n",
      "F1 score:  0.6215139442231075\n",
      "Matthews correlation coefficient:  0.21430796208172556\n"
     ]
    }
   ],
   "source": [
    "skmodel = LogisticRegression(class_weight='balanced')\n",
    "#skmodel = LogisticRegression()\n",
    "#skmodel = GaussianNB()\n",
    "skmodel.fit(Xtrain, ytrain)                  \n",
    "y_model = skmodel.predict(Xtest)   \n",
    "ytrain_model = skmodel.predict(Xtrain)\n",
    "print('Train accuracy: ',accuracy_score(ytrain, ytrain_model))\n",
    "print('Test accuracy: ',accuracy_score(ytest, y_model))\n",
    "print('F1 score: ',f1_score(ytest, y_model))\n",
    "print('Matthews correlation coefficient: ',matthews_corrcoef(ytest,y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Logistic regression\n",
    "Train accuracy:  0.9972375690607734\n",
    "Test accuracy:  0.5867768595041323\n",
    "F1 score:  0.5934959349593496\n",
    "Matthews correlation coefficient:  0.17332786773245884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_sizes = [128, 64]\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Sigmoid())\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#CrossEntropyLoss() requires logits as the output and class labels as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape:  torch.Size([724, 512])\n"
     ]
    }
   ],
   "source": [
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "print(\"Xtrain.shape: \",Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.0852215439081192\n",
      "Training loss:  0.07720500975847244\n",
      "Training loss:  0.08967221528291702\n",
      "Training loss:  0.10272926092147827\n",
      "Training loss:  0.07113324850797653\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #weight_decay=1e-5\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    output = model(Xtrain)\n",
    "    labels = ytrain.unsqueeze(1)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if e % 2 == 0:\n",
    "        print(\"Training loss: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = output.detach().numpy()\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testoutput = model(Xtest)\n",
    "testpredictions = testoutput.detach().numpy()\n",
    "testpredictions = np.where(testpredictions > 0.5, 1, 0)\n",
    "testpredictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9406077348066298\n",
      "Test accuracy:  0.5661157024793388\n",
      "F1 score:  0.5291479820627804\n",
      "Matthews:  0.13720062149495627\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy: ',accuracy_score(predictions,ytrain))\n",
    "print('Test accuracy: ',accuracy_score(testpredictions, ytest))\n",
    "print('F1 score: ',f1_score(testpredictions, ytest))\n",
    "print('Matthews: ',matthews_corrcoef(testpredictions, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
