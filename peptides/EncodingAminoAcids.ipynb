{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('Tkagg')    ## to avoid focus stealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AVPdb_data.csv', skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVP0001</td>\n",
       "      <td>PYVGSGLYRR</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVP0002</td>\n",
       "      <td>SMIENLEYM</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVP0003</td>\n",
       "      <td>ECRSTSYAGAVVNDL</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVP0004</td>\n",
       "      <td>STSYAGAVVNDL</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVP0005</td>\n",
       "      <td>YAGAVVNDL</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>AVP2058</td>\n",
       "      <td>LFRLIKSLIKRLVSAFK</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>AVP2059</td>\n",
       "      <td>SLIGGLVSAFK</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>AVP2060</td>\n",
       "      <td>VSAFK</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>AVP2061</td>\n",
       "      <td>KHMHWHPPALNT</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>AVP2062</td>\n",
       "      <td>SLIGRL</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2059 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                seq  len\n",
       "0     AVP0001         PYVGSGLYRR   10\n",
       "1     AVP0002          SMIENLEYM    9\n",
       "2     AVP0003    ECRSTSYAGAVVNDL   15\n",
       "3     AVP0004       STSYAGAVVNDL   12\n",
       "4     AVP0005          YAGAVVNDL    9\n",
       "...       ...                ...  ...\n",
       "2054  AVP2058  LFRLIKSLIKRLVSAFK   17\n",
       "2055  AVP2059        SLIGGLVSAFK   11\n",
       "2056  AVP2060              VSAFK    5\n",
       "2057  AVP2061       KHMHWHPPALNT   12\n",
       "2058  AVP2062             SLIGRL    6\n",
       "\n",
       "[2059 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "#seq = np.asarray(data['seq'])\n",
    "#print(seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of 20 canonical amino acids\n",
    "\n",
    "CHAR_TO_IND = {'_': 0,\n",
    " 'R': 1,\n",
    " 'F': 2,\n",
    " 'L': 3,\n",
    " 'D': 4,\n",
    " 'S': 5,\n",
    " 'T': 6,\n",
    " 'E': 7,\n",
    " 'I': 8,\n",
    " 'N': 9,\n",
    " 'C': 10,\n",
    " 'W': 11,\n",
    " 'Y': 12,\n",
    " 'A': 13,\n",
    " 'V': 14,\n",
    " 'P': 15,\n",
    " 'G': 16,\n",
    " 'Q': 17,\n",
    " 'H': 18,\n",
    " 'M': 19,\n",
    " 'K': 20}\n",
    "\n",
    "IND_TO_CHAR = {\n",
    "    CHAR_TO_IND[c]: c\n",
    "    for c in CHAR_TO_IND\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PEPTIDE_LENGTH = 5 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
    "NUM_AMINO_ACIDS = len(CHAR_TO_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to think about how to deal with smaller sequences\n",
    "def peptide_to_vector(peptide):\n",
    "    \"\"\"Takes an input which is a string of amino acids ie 'AAYS' and returns an array of one-hot vectors of shape \n",
    "    (MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS)\"\"\"\n",
    "    default = np.zeros([MAX_SEQUENCE_LENGTH, len(CHAR_TO_IND)])\n",
    "    for i, character in enumerate(peptide[:MAX_PEPTIDE_LENGTH]):\n",
    "        default[i][CHAR_TO_IND[character]] = 1\n",
    "    return default\n",
    "\n",
    "#think about how to deal with non one hot vectors\n",
    "def vector_to_peptide(one_hot):\n",
    "    \"\"\"Takes a one hot vector (MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS) and returns the peptide it represents\n",
    "    Note that argmax on equal values defaults to the smallest index\"\"\"\n",
    "    if one_hot.ndim == 1:\n",
    "        one_hot = one_hot.reshape((-1,NUM_AMINO_ACIDS))\n",
    "    \n",
    "    return ''.join([IND_TO_CHAR[one_hot[i].argmax()]for i in range(len(one_hot))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['AAAAA','AAAII','AAAHH','AAAFF','AAARR','AAAGG','AAAYY','AAAKK','AAAQQ','AAAPP','AAAVV' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_function(examples,batch_size, iteration):\n",
    "    input_array = []\n",
    "    for j in range(iteration * batch_size, (iteration+1) * batch_size):\n",
    "        embedding = peptide_to_vector(examples[j])\n",
    "        embedding = embedding.reshape(-1)\n",
    "        input_array.append(embedding)\n",
    "    return input_array\n",
    "\n",
    "#fix random blanks appearing in the middle\n",
    "def noise_function(batch_size):\n",
    "    \n",
    "    input_array = []\n",
    "    for j in range(batch_size):\n",
    "        a = np.zeros([MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS ])\n",
    "        for i in range(MAX_PEPTIDE_LENGTH):\n",
    "            \n",
    "            x = np.random.randint(NUM_AMINO_ACIDS) \n",
    "            a[i][x] = np.random.normal(0.,1.)\n",
    "        a = a.reshape(-1)\n",
    "        input_array.append(a)\n",
    "    return input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_function(seq,2,0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__Q__', '___N_', 'H__TY', 'AIG__', '_DRYM', 'TNL__']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(vector_to_peptide,noise_function(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_length,output_length):\n",
    "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
    "        Args:\n",
    "            input_length (int array): max_length * number_of_characters \n",
    "                                      (\"noise vector\")\n",
    "            layers (List[int]): A list of layer widths including output width\n",
    "            output_activation: torch activation function or None\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_length, 1800)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(1800, 1440)\n",
    "        self.linear3 = nn.Linear(1440, output_length)\n",
    "        self.linear4 = nn.Linear(1080, 720)\n",
    "        self.linear5 = nn.Linear(720, 360)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        intermediate = self.linear1(input_tensor)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear2(intermediate)\n",
    "        intermediate = self.relu(intermediate)\n",
    "        intermediate = self.linear3(intermediate)\n",
    "        \"\"\"intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear4(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear5(intermediate)\"\"\"\n",
    "        if self.output_activation is not None:\n",
    "            intermediate = self.output_activation(intermediate)\n",
    "        return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, layers):\n",
    "        \"\"\"A discriminator for discerning real from generated samples.\n",
    "        params:\n",
    "            input_dim (int): width of the input\n",
    "            layers (List[int]): A list of layer widths including output width\n",
    "        Output activation is Sigmoid.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self._init_layers(layers)\n",
    "\n",
    "    def _init_layers(self, layers):\n",
    "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
    "        self.module_list = nn.ModuleList()\n",
    "        last_layer = self.input_dim\n",
    "        for index, width in enumerate(layers):\n",
    "            self.module_list.append(nn.Linear(last_layer, width))\n",
    "            last_layer = width\n",
    "            if index + 1 != len(layers):\n",
    "                self.module_list.append(nn.LeakyReLU())\n",
    "            else:\n",
    "                self.module_list.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "        intermediate = input_tensor\n",
    "        for layer in self.module_list:\n",
    "            intermediate = layer(intermediate)\n",
    "        return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGAN(batch_size: int = 25, epochs: int = 5, max_data: int = len(seq), print_every: int = 10):\n",
    "\n",
    "    #Array to monitor losses\n",
    "    loss_g = []\n",
    "    loss_d = []\n",
    "    input_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "    output_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(input_length,output_length)\n",
    "    discriminator = Discriminator(input_length, [64, 32, 1])  \n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.1)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.1)\n",
    "\n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for j in range(int(max_data/batch_size)):\n",
    "            # zero the gradients on each iteration\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Create noisy input for generator\n",
    "            # Need float type instead of int\n",
    "            noise = noise_function(batch_size)\n",
    "            noise_data = torch.tensor(noise).float()\n",
    "            generated_data = generator(noise_data)\n",
    "            #print(\"Generated data in loop\")\n",
    "            #print(generated_data)\n",
    "\n",
    "            # Generate examples of even real data\n",
    "            true_data = data_function(seq,batch_size, j)\n",
    "            if i % print_every ==0:\n",
    "                print(\"True data used: \", list(map(vector_to_peptide,true_data)) )\n",
    "            true_labels = torch.tensor(np.ones(batch_size)).float()\n",
    "            true_data = torch.tensor(true_data).float()\n",
    "\n",
    "            # Train the generator\n",
    "            # We invert the labels here and don't train the discriminator because we want the generator\n",
    "            # to make things the discriminator classifies as true.\n",
    "            generator_discriminator_out = discriminator(generated_data)\n",
    "            generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            # Train the discriminator on the true/generated data\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            true_discriminator_out = discriminator(true_data)\n",
    "            true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "            # add .detach() here think about this\n",
    "            generator_discriminator_out = discriminator(generated_data.detach())\n",
    "            generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "            discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        generated_data = generated_data.detach().numpy()\n",
    "        if i % print_every == 0:\n",
    "            print(\"Epoch: \",i,)\n",
    "            print(\"generated data:\")\n",
    "            print(generated_data)\n",
    "            print(\"peptide of noisedata[0]\")\n",
    "            print(vector_to_peptide(noise_data.numpy()[0]))\n",
    "            print(\"peptide of generated data[0]\")\n",
    "            print(vector_to_peptide(generated_data[0]))\n",
    "        \"\"\"This threshold only makes sense if there's a sigmoid activation for the Generator\"\"\"\n",
    "        #generated_data[generated_data > 0.5] = 1\n",
    "        #generated_data[generated_data <= 0.5] = 0           \n",
    "        sequence = np.reshape(generated_data[0], [MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS])\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(\"sequence:\")\n",
    "            print(sequence)\n",
    "            print(\"peptide of sequence\")\n",
    "            print(vector_to_peptide(sequence))\n",
    "            print(\"Generator Loss :\",generator_loss.item())\n",
    "            print(\"Discriminator Loss: \", discriminator_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  0\n",
      "generated data:\n",
      "[[0.50014126 0.5030785  0.49608636 ... 0.50473934 0.49990073 0.50308037]\n",
      " [0.49648586 0.49489465 0.49600834 ... 0.49837855 0.4978191  0.5011022 ]\n",
      " [0.5016147  0.4979055  0.4948813  ... 0.497995   0.5013907  0.5027408 ]\n",
      " ...\n",
      " [0.5000933  0.5041413  0.49377877 ... 0.49386957 0.49332958 0.49800912]\n",
      " [0.48935634 0.4971755  0.5030058  ... 0.49008027 0.5044937  0.5099868 ]\n",
      " [0.49490184 0.4989096  0.50248766 ... 0.4921616  0.50085753 0.5068339 ]]\n",
      "peptide of noisedata[0]\n",
      "K_IAA\n",
      "peptide of generated data[0]\n",
      "HHFGT\n",
      "sequence:\n",
      "[[0.50014126 0.5030785  0.49608636 0.50269157 0.5046573  0.50238925\n",
      "  0.5053858  0.49650666 0.5052731  0.4986546  0.5006881  0.5075788\n",
      "  0.49783307 0.5050796  0.4949454  0.50169545 0.499763   0.49433225\n",
      "  0.5129779  0.5053514  0.5068786 ]\n",
      " [0.49709252 0.49281713 0.48931667 0.5022155  0.49631658 0.49923655\n",
      "  0.4953828  0.49465007 0.50665945 0.4973114  0.499763   0.5033623\n",
      "  0.50382173 0.5043855  0.4982302  0.49979618 0.5023342  0.50567013\n",
      "  0.50844514 0.4957583  0.50159067]\n",
      " [0.4898489  0.49943808 0.51330644 0.49062598 0.4973865  0.5034863\n",
      "  0.49627137 0.5027312  0.4931672  0.4996515  0.5047943  0.49124926\n",
      "  0.49178046 0.5073244  0.49306375 0.51205903 0.504449   0.49964857\n",
      "  0.5083005  0.49376506 0.49957013]\n",
      " [0.51130164 0.49499616 0.48876277 0.49640235 0.5080704  0.49565694\n",
      "  0.496332   0.50849295 0.49878892 0.5040739  0.50270444 0.4998944\n",
      "  0.50678    0.49807262 0.49849364 0.4990362  0.51352745 0.4974299\n",
      "  0.5016935  0.50205505 0.4993835 ]\n",
      " [0.4962608  0.4893717  0.5028614  0.48910445 0.5055995  0.49719986\n",
      "  0.5066606  0.48732308 0.50322986 0.50082844 0.49373847 0.49730358\n",
      "  0.49923792 0.5051213  0.50010175 0.49507287 0.49677074 0.4997826\n",
      "  0.50473934 0.49990073 0.50308037]]\n",
      "peptide of sequence\n",
      "HHFGT\n",
      "Generator Loss : 0.6403994560241699\n",
      "Discriminator Loss:  0.6917756795883179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  20\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_WR_L\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  40\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_AL__\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  60\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "G_H_M\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  80\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_I_L_\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "simpleGAN(batch_size=10, epochs=100, print_every = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "max_data= len(seq)\n",
    "batch_size = 2\n",
    "print_every = 100\n",
    "loss_g = []\n",
    "loss_d = []\n",
    "input_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "output_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "\n",
    "# Models\n",
    "generator = Generator(input_length,output_length)\n",
    "discriminator = Discriminator(input_length, [64, 32, 1])  \n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.1)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.1)\n",
    "\n",
    "# loss\n",
    "loss = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 38.396087646484375\n",
      "Discriminator Loss:  0.06820950657129288\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    for j in range(int(max_data/batch_size)):\n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # Need float type instead of int\n",
    "        noise = noise_function(batch_size)\n",
    "        noise_data = torch.tensor(noise).float()\n",
    "        generated_data = generator(noise_data)\n",
    "        #print(\"Generated data in loop\")\n",
    "        #print(generated_data)\n",
    "\n",
    "        # Generate examples of even real data\n",
    "        true_data = data_function(seq,batch_size, j)\n",
    "        \"\"\"if i % print_every ==0:\n",
    "            print(\"True data used: \", list(map(vector_to_peptide,true_data)) )\"\"\"\n",
    "        true_labels = torch.tensor(np.ones(batch_size)).float()\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "\n",
    "        # Train the generator\n",
    "        # We invert the labels here and don't train the discriminator because we want the generator\n",
    "        # to make things the discriminator classifies as true.\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "    generated_data = generated_data.detach().numpy()\n",
    "    \"\"\"if i % print_every == 0:\n",
    "        print(\"Epoch: \",i,)\n",
    "        print(\"generated data:\")\n",
    "        print(generated_data)\n",
    "        print(\"peptide of noisedata[0]\")\n",
    "        print(vector_to_peptide(noise_data.numpy()[0]))\n",
    "        print(\"peptide of generated data[0]\")\n",
    "        print(vector_to_peptide(generated_data[0]))\"\"\"\n",
    "    \n",
    "    \"\"\"This threshold only makes sense if there's a sigmoid activation for the Generator\"\"\"\n",
    "    #generated_data[generated_data > 0.5] = 1\n",
    "    #generated_data[generated_data <= 0.5] = 0           \n",
    "    sequence = np.reshape(generated_data[0], [MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS])\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print(\"sequence:\")\n",
    "        print(sequence)\n",
    "        print(\"peptide of sequence\")\n",
    "        print(vector_to_peptide(sequence))\n",
    "        print(\"Generator Loss :\",generator_loss.item())\n",
    "        print(\"Discriminator Loss: \", discriminator_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data = data_function(seq,2,0)\n",
    "true_data = torch.tensor(true_data).float()\n",
    "true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator(true_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3637,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2313,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1523,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.1494,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3479,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.8233,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.8060,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9097,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.8831,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  1.4132,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.6513,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.3653, -0.1038,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4611,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.6056,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1110,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          2.6655,  0.0000,  0.0000,  0.0000,  0.0000,  0.8465,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0663,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.3697,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4484,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -1.5670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.7660,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.2988,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.8098,  0.0000,\n",
       "          0.0000]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = noise_function(5)\n",
    "fake_data = torch.tensor(fake_data).float()\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "# unset BREAK_EARLY if we must not stop training earlier than max_epochs\n",
    "\n",
    "BREAK_EARLY = True\n",
    "max_epochs = 100\n",
    "BLANK_CHAR = '~'\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "##################################\n",
    "# argv\n",
    "# either pass 0 args or 1 args\n",
    "if (len(sys.argv)) == 1:\n",
    "    Arg_dataset_name = \"none\"\n",
    "    Arg_num_layers = 1\n",
    "    Arg_seqmaxlen = 22\n",
    "else:\n",
    "    _, Arg_dataset_name, Arg_num_layers, Arg_seqmaxlen = sys.argv\n",
    "    Arg_num_layers = int(Arg_num_layers)\n",
    "    Arg_seqmaxlen = int(Arg_seqmaxlen)\n",
    "\n",
    "Imagesuffix = Arg_dataset_name + \".L\" + str(Arg_num_layers) + \".s\" + str(Arg_seqmaxlen)\n",
    "##################################\n",
    "\n",
    "if Arg_dataset_name == 'coffee':\n",
    "    learning_rate = 0.1\n",
    "elif Arg_dataset_name == 'emails':\n",
    "    learning_rate = 0.01\n",
    "elif Arg_dataset_name == 'tls':\n",
    "    learning_rate = 0.02\n",
    "##################################\n",
    "\n",
    "\n",
    "Train_end_reason = \"Max_epoch done\"\n",
    "\n",
    "#inp_alphabet = \".ab@_yz\"\n",
    "#out_alphabet = \".FT\"\n",
    "\n",
    "def get_alphabet(d_set):\n",
    "    letters = set()\n",
    "    for word in d_set:\n",
    "        letters.update(set(word))\n",
    "\n",
    "    return list([ BLANK_CHAR ]) + sorted(list(letters))\n",
    "\n",
    "\n",
    "def simple_encode_strlist(X, max, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    #example: {'.': 0, 'a': 1, 'b': 2}\n",
    "    X = [ [char_to_int[char] for char in x1.strip()] for x1 in X]\n",
    "    X1 = [ x + [0] * (max - len(x)) for x in X]      # pad to max\n",
    "    #example: simple_encode_inp([\"aa\",\"bb\"],3) gives [[1, 1, 0], [2, 2, 0]]\n",
    "    return X1\n",
    "\n",
    "def simple_decode_idxlist1(enc_data, alphabet):\n",
    "    idx2char = alphabet\n",
    "    out = [idx2char[int(x)] for x in enc_data]\n",
    "    out = ''.join(out)\n",
    "    return out\n",
    "\n",
    "def simple_decode_idxlist(d_arr, alphabet):\n",
    "    return [simple_decode_idxlist1(d, alphabet) for d in d_arr]\n",
    "\n",
    "def load_data_file(filename):\n",
    "    data = pandas.read_csv(filename, header=None)\n",
    "    print (\"=== loaded data shape: \", data.shape)\n",
    "    print (\"=== data \", data)\n",
    "\n",
    "    X_pandas = data[0]\n",
    "    X_pandas = [ x.strip() for x in X_pandas]\n",
    "    X_max = max([ len(x) for x in X_pandas])\n",
    "\n",
    "    Y_pandas = data[1]\n",
    "    Y_pandas = [ y.strip() for y in Y_pandas]\n",
    "    Y_max = max([ len(y) for y in Y_pandas])\n",
    "\n",
    "    if X_max != Y_max:\n",
    "        print(\"max len mismatch in X and Y\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    seq_max = X_max\n",
    "    return X_pandas, Y_pandas, seq_max\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "def ohe_singleletter(val, max):\n",
    "    letter = [0 for _ in range(max)]\n",
    "    letter[val] = 1\n",
    "    return letter\n",
    "\n",
    "\n",
    "def simple_to_onehot(D, alphabet):\n",
    "    itemlist = list()\n",
    "    for d in D:\n",
    "        item = list()\n",
    "        for e in d:\n",
    "            l = ohe_singleletter(e, len(alphabet))\n",
    "            item.append(l)\n",
    "            #print(e, l)\n",
    "        #print(d, \"------\",  item)\n",
    "        itemlist.append(item)\n",
    "    return itemlist\n",
    "\n",
    "def onehot_decode_to_simple1(d):\n",
    "    arr = np.array(d)\n",
    "    idx = arr.argmax(1)\n",
    "    return idx\n",
    "\n",
    "def onehot_decode_to_simple(D):\n",
    "    arr = np.array(D)\n",
    "    idx = arr.argmax(2)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def decode_inp(input):\n",
    "    input = onehot_decode_to_simple1(input)\n",
    "    input = simple_decode_idxlist1(input, Inp_alphabet)\n",
    "    return input\n",
    "\n",
    "def decode_out(output):\n",
    "    output = simple_decode_idxlist1(output, Out_alphabet)\n",
    "    return output\n",
    "\n",
    "#################################\n",
    "\n",
    "X_data, Y_data, train_Seq_max = load_data_file('rnn3-train-data.txt')\n",
    "Num_io_data = len(X_data)\n",
    "\n",
    "# set Seq_max to  max in both test and train\n",
    "_, _, test_Seq_max = load_data_file('rnn3-test-data.txt')\n",
    "Seq_max = max([train_Seq_max, test_Seq_max])\n",
    "\n",
    "# if Seq_max <= Arg_seqmaxlen:\n",
    "#     print(\"=== updating Seq_max from\", Seq_max, \" to \", Arg_seqmaxlen)\n",
    "#     Seq_max = Arg_seqmaxlen\n",
    "# else:\n",
    "#     print(\"=== seq max len mismatch\", Seq_max, Arg_seqmaxlen)\n",
    "#     sys.exit(1)\n",
    "\n",
    "if Arg_dataset_name == 'tls':\n",
    "    Inp_alphabet =  [BLANK_CHAR, 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    Out_alphabet =  [ 'E', 'S', 'X', 'H', 'U', 'F', 'W', 'I', 'A', 'C' ]\n",
    "    Out_alphabet =  list([ BLANK_CHAR ]) + sorted(Out_alphabet)\n",
    "\n",
    "else:\n",
    "    Inp_alphabet = get_alphabet(X_data)\n",
    "    Out_alphabet = get_alphabet(Y_data)\n",
    "#print(''.join(Inp_alphabet))\n",
    "#print(''.join(Out_alphabet))\n",
    "print(\"=== inp_alphabet:\", Inp_alphabet)\n",
    "print(\"=== out_alphabet:\", Out_alphabet)\n",
    "\n",
    "X_data = simple_encode_strlist(X_data, Seq_max, Inp_alphabet)\n",
    "Y_data = simple_encode_strlist(Y_data, Seq_max, Out_alphabet)\n",
    "\n",
    "#print(X_data)\n",
    "\n",
    "Num_samples = len(X_data)\n",
    "\n",
    "#print(simple_decode_idxlist1(X_data[0], Inp_alphabet))\n",
    "\n",
    "#print(simple_decode_idxlist(X_data, Inp_alphabet))\n",
    "#print(simple_decode_idxlist(Y_data, Out_alphabet))\n",
    "\n",
    "X_ohe = simple_to_onehot(X_data, Inp_alphabet)\n",
    "#print(X_ohe)\n",
    "\n",
    "#print(onehot_decode_to_simple(X_ohe))\n",
    "#print(\"------------\")\n",
    "\n",
    "#################################\n",
    "class Model_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'RNN'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'LSTM'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        # LSTM hidden state is a tuple (h_0, c_0)\n",
    "        h_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        c_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        hidden = (h_0, c_0)\n",
    "        return hidden\n",
    "\n",
    "#################################\n",
    "\n",
    "Inp_size = len(Inp_alphabet)\n",
    "Out_size = len(Out_alphabet)\n",
    "torch.manual_seed(2.7321)\n",
    "\n",
    "#################################\n",
    "num_classes = Out_size          #  XXX why need a separate num_classes,  when hidden_size would do?\n",
    "input_size = Inp_size  # this parameter is for the view function to know how large\n",
    "#the one hot vector is supposed to be\n",
    "\n",
    "hidden_size = Out_size #\n",
    "batch_size = 1   # id dont understand this\n",
    "seq_len = Seq_max\n",
    "num_layers = Arg_num_layers  # num-layers of rnn\n",
    "#learning_rate = 0.01\n",
    "momentum = 0.1\n",
    "\n",
    "\n",
    "\n",
    "vizdelay = 30\n",
    "################################\n",
    "\n",
    "# function to reduce learning rate based on accuracy.\n",
    "# it returns a new optimizer\n",
    "def get_new_optimizer(learning_rate, accuracy):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if accuracy >= 50:\n",
    "        new_learning_rate = learning_rate * 0.9\n",
    "    if accuracy >= 60:\n",
    "        new_learning_rate = learning_rate * 0.8\n",
    "    if accuracy >= 70:\n",
    "        new_learning_rate = learning_rate * 0.7\n",
    "    if accuracy >= 80:\n",
    "        new_learning_rate = learning_rate * 0.5\n",
    "    if accuracy >= 90:\n",
    "        new_learning_rate = learning_rate * 0.4\n",
    "    if accuracy >= 95:\n",
    "        new_learning_rate = learning_rate * 0.2\n",
    "    if accuracy >= 98:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=new_learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "############## Model Training code ##########################\n",
    "def train_phase():\n",
    "    global Optimizer\n",
    "    MIN_LOSS = float('inf')\n",
    "    MIN_LOSS_epoch_counter = 0\n",
    "    MAX_ACC_epoch_counter = 0\n",
    "\n",
    "    final_train_epoch=0\n",
    "    final_train_accuracy=0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        Optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        errcount = 0\n",
    "        # if epoch % 30 ==0:\n",
    "        #     evaluate_model(inputs,labels)\n",
    "        for input, label in zip(Inputs, Labels):\n",
    "            # input = input.unsqueeze(0)\n",
    "            hidden = model.init_hidden() #we reset the RNN to its initial state\n",
    "            hidden, output = model(input, hidden) #run the model\n",
    "\n",
    "            val, idx = output.max(1)\n",
    "            expected = decode_out(label)\n",
    "            trained = decode_out(idx)\n",
    "\n",
    "            if trained != expected:\n",
    "                errcount += 1\n",
    "\n",
    "            if epoch % vizdelay == 0:\n",
    "                if (trained != expected):\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained, \"   *****\")\n",
    "                else:\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained)\n",
    "            loss += Criterion(output, label) #add the current sample error to loss\n",
    "\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "\n",
    "        ########### Within loop plotting and logging ########\n",
    "        if loss.data < MIN_LOSS:\n",
    "            MIN_LOSS = loss.data\n",
    "            MIN_LOSS_epoch_counter = epoch   # reset\n",
    "\n",
    "        accuracy = 100.0 * (Num_io_data - errcount) / Num_io_data\n",
    "        print(\"Epoch: %d, loss: %1.3f         errcount: %d  accuracy: %1.1f%%\\n\" % (epoch+1, loss.data, errcount, accuracy))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        plot_data.append([loss.data.item(), accuracy])\n",
    "        t = [ x for x in range(0, epoch)]\n",
    "        p = np.array(plot_data)\n",
    "        plt_ax1.plot(p[:, 0], color='red')\n",
    "        plt_ax2.plot(p[:, 1], color='blue')\n",
    "        plt.pause(0.001)\n",
    "        #plt.show()\n",
    "\n",
    "        final_train_epoch = epoch\n",
    "        final_train_accuracy = accuracy\n",
    "        if BREAK_EARLY == True:\n",
    "            global Train_end_reason\n",
    "            if (epoch - MIN_LOSS_epoch_counter) >= 20:\n",
    "                Train_end_reason = \"=== no new low of training_loss seen for last 20 epochs; stop training\"\n",
    "                print(Train_end_reason)\n",
    "                break\n",
    "\n",
    "            if accuracy >= 99.9:\n",
    "                MAX_ACC_epoch_counter += 1\n",
    "                # stop, if accuracy stays at ~100 for 10 epochs\n",
    "                if MAX_ACC_epoch_counter >= 10:\n",
    "                    Train_end_reason = \"=== maximal accuracy seen for last 10 epochs; stop training\"\n",
    "                    print(Train_end_reason)\n",
    "                    time.sleep(4)\n",
    "                    break\n",
    "\n",
    "        # update optimizer with changed lr - depending on accuracy\n",
    "        Optimizer = get_new_optimizer(learning_rate, accuracy)\n",
    "    return final_train_accuracy, final_train_epoch\n",
    "\n",
    "\n",
    "####################### Model evaluation code ################\n",
    "def evaluate_model_1(xdata, ydata):\n",
    "    x_one_hot = simple_to_onehot(xdata, Inp_alphabet)\n",
    "    inputs = Variable(torch.Tensor(x_one_hot))\n",
    "    labels = Variable(torch.LongTensor(ydata))\n",
    "\n",
    "    err_count = 0\n",
    "    for input, label in zip(inputs, labels):\n",
    "        expected = decode_out(label)\n",
    "        hidden = model.init_hidden()\n",
    "        hidden, output = model(input, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        predicted = decode_out(idx)\n",
    "        if (expected != predicted):\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted, \"   *****\")\n",
    "            err_count += 1\n",
    "        else:\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted)\n",
    "    return (err_count)\n",
    "\n",
    "def evaluate_model(xdata, ydata, seqmax):\n",
    "    xdata = simple_encode_strlist(xdata, seqmax, Inp_alphabet)\n",
    "    ydata = simple_encode_strlist(ydata, seqmax, Out_alphabet)\n",
    "    return evaluate_model_1(xdata, ydata)\n",
    "\n",
    "################################################################################\n",
    "def validation_phase():\n",
    "    print(\"============================= validation inputs ===========================\")\n",
    "    validation_inputs, validation_outputs, seqmax = load_data_file('rnn3-train-data.txt')\n",
    "    num_io_data = len(validation_inputs)\n",
    "    if seqmax != Seq_max:\n",
    "        print(\"seqmax mismatch\", seqmax, Seq_max)\n",
    "        sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "    errcount = evaluate_model(validation_inputs, validation_outputs, seqmax)\n",
    "    val_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    if errcount > 0:\n",
    "        print(\"VALIDATION FAILED:  errors: \", errcount, \"accuracy:\", val_accuracy)\n",
    "    else:\n",
    "        print(\"Validation Passed\")\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def test_phase():\n",
    "    print(\"============================= test inputs ===========================\")\n",
    "    test_inputs, test_outputs, seqmax = load_data_file('rnn3-test-data.txt')\n",
    "    num_io_data = len(test_inputs)\n",
    "    #if seqmax != Seq_max: #this could happen just due to randomness #todo\n",
    "    #    print(\"WARN: seqmax mismatch\")     ## possible to have diff seq max from the train data\n",
    "    #    sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "\n",
    "    errcount = evaluate_model(test_inputs, test_outputs, seqmax)\n",
    "    test_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    print(\"Testset size: \", len(test_inputs),\"Test errors: \", errcount, \"accuracy:\", test_accuracy)\n",
    "    return test_accuracy\n",
    "\n",
    "################################################################################\n",
    "\n",
    "########################################################\n",
    "#Instantiate RNN model\n",
    "#model = Model_RNN()\n",
    "Rnn_type = '###'\n",
    "model = Model_LSTM()\n",
    "Rnn_type = model.Rnn_type\n",
    "\n",
    "####################### Plotting code ##################\n",
    "\n",
    "plt.ion()\n",
    "plt_fig, plt_ax1 = plt.subplots()\n",
    "plt_ax2 = plt_ax1.twinx()\n",
    "plt_ax1.set_xlabel('epoch')\n",
    "plt_ax1.set_ylabel('loss', color='red')\n",
    "plt_ax2.set_ylabel('accuracy', color='blue')\n",
    "\n",
    "txt = \"Dataset: \" + Arg_dataset_name + \"\\nrnn_type: \"+ Rnn_type + \"\\nnum_layers: \"+ str(num_layers) + \"\\nnum_io_samples: \"+ str(Num_io_data) + \"\\nseq_max_len: \"+ str(Seq_max)\n",
    "txt += \"\\nBase_LR: \" + str(learning_rate)\n",
    "\n",
    "#plt_fig = plt.figure()\n",
    "plt_fig.text(.5, .2, txt, ha='center', transform=plt_ax1.transAxes)\n",
    "plt.pause(0.001)\n",
    "\n",
    "plot_data = []\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "################## Load trained model ###############\n",
    "from pathlib import Path\n",
    "modelimagefile = \"rnn3model.\" + Imagesuffix + \".pt\"\n",
    "if Path(modelimagefile).is_file():\n",
    "    # file exists\n",
    "    model.load_state_dict(torch.load(modelimagefile))\n",
    "    model.eval()\n",
    "    print(\"=== Model was loaded from \" + modelimagefile)\n",
    "\n",
    "####################################################\n",
    "Criterion = torch.nn.CrossEntropyLoss()\n",
    "Optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "# X is OH encoded\n",
    "# Y is simple encoded\n",
    "# this is the format needed by pytorch\n",
    "Inputs = Variable(torch.Tensor(X_ohe))\n",
    "Labels = Variable(torch.LongTensor(Y_data))\n",
    "\n",
    "print(\"Input shape: \", Inputs.size())\n",
    "print(\"Output shape: \", Labels.size())\n",
    "\n",
    "final_train_accuracy, final_train_epoch = train_phase()\n",
    "\n",
    "############ final plotting and logging ################\n",
    "#plt.plot(plot_data)\n",
    "#plt.waitforbuttonpress()\n",
    "plt.savefig('rnn3-ttt.png',  bbox_inches='tight')\n",
    "plt.show()\n",
    "########################################################\n",
    "\n",
    "val_accuracy = validation_phase()\n",
    "test_accuracy = test_phase()\n",
    "\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"=== rnn_type:\", Rnn_type,\n",
    "        \"num_layers:\", num_layers,\n",
    "        \"num_io_samples (train): \", Num_io_data,\n",
    "        \"seq_max_len:\", Seq_max)\n",
    "\n",
    "print(\"\\n Training end due to: \", Train_end_reason)\n",
    "print(\"\\n=== final_train_epoch:\", final_train_epoch)\n",
    "print(\"=== final_train_accuracy:  %1.2f%%       \\n=== val_accuracy:  %1.2f%%        \\n=== test_accuracy:  %1.2f%%\\n\"\n",
    "        %  (final_train_accuracy, val_accuracy, test_accuracy))\n",
    "\n",
    "\n",
    "######################### save trained model #######################\n",
    "torch.save(model.state_dict(), modelimagefile)\n",
    "print(\"=== Model was saved as \" + modelimagefile)\n",
    "####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
