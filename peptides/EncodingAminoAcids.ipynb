{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AVPdb_data.csv', skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVP0001</td>\n",
       "      <td>PYVGSGLYRR</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVP0002</td>\n",
       "      <td>SMIENLEYM</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVP0003</td>\n",
       "      <td>ECRSTSYAGAVVNDL</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVP0004</td>\n",
       "      <td>STSYAGAVVNDL</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVP0005</td>\n",
       "      <td>YAGAVVNDL</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>AVP2058</td>\n",
       "      <td>LFRLIKSLIKRLVSAFK</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>AVP2059</td>\n",
       "      <td>SLIGGLVSAFK</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>AVP2060</td>\n",
       "      <td>VSAFK</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>AVP2061</td>\n",
       "      <td>KHMHWHPPALNT</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>AVP2062</td>\n",
       "      <td>SLIGRL</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2059 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                seq  len\n",
       "0     AVP0001         PYVGSGLYRR   10\n",
       "1     AVP0002          SMIENLEYM    9\n",
       "2     AVP0003    ECRSTSYAGAVVNDL   15\n",
       "3     AVP0004       STSYAGAVVNDL   12\n",
       "4     AVP0005          YAGAVVNDL    9\n",
       "...       ...                ...  ...\n",
       "2054  AVP2058  LFRLIKSLIKRLVSAFK   17\n",
       "2055  AVP2059        SLIGGLVSAFK   11\n",
       "2056  AVP2060              VSAFK    5\n",
       "2057  AVP2061       KHMHWHPPALNT   12\n",
       "2058  AVP2062             SLIGRL    6\n",
       "\n",
       "[2059 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "#seq = np.asarray(data['seq'])\n",
    "#print(seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of 20 canonical amino acids\n",
    "\n",
    "CHAR_TO_IND = {'_': 0,\n",
    " 'R': 1,\n",
    " 'F': 2,\n",
    " 'L': 3,\n",
    " 'D': 4,\n",
    " 'S': 5,\n",
    " 'T': 6,\n",
    " 'E': 7,\n",
    " 'I': 8,\n",
    " 'N': 9,\n",
    " 'C': 10,\n",
    " 'W': 11,\n",
    " 'Y': 12,\n",
    " 'A': 13,\n",
    " 'V': 14,\n",
    " 'P': 15,\n",
    " 'G': 16,\n",
    " 'Q': 17,\n",
    " 'H': 18,\n",
    " 'M': 19,\n",
    " 'K': 20}\n",
    "\n",
    "IND_TO_CHAR = {\n",
    "    CHAR_TO_IND[c]: c\n",
    "    for c in CHAR_TO_IND\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PEPTIDE_LENGTH = 5 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
    "NUM_AMINO_ACIDS = len(CHAR_TO_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to think about how to deal with smaller sequences\n",
    "def peptide_to_vector(peptide):\n",
    "    \"\"\"Takes an input which is a string of amino acids ie 'AAYS' and returns an array of one-hot vectors of shape \n",
    "    (MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS)\"\"\"\n",
    "    default = np.zeros([MAX_PEPTIDE_LENGTH, len(CHAR_TO_IND)])\n",
    "    for i, character in enumerate(peptide[:MAX_PEPTIDE_LENGTH]):\n",
    "        default[i][CHAR_TO_IND[character]] = 1\n",
    "    return default\n",
    "\n",
    "#think about how to deal with non one hot vectors\n",
    "def vector_to_peptide(one_hot):\n",
    "    \"\"\"Takes a one hot vector (MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS) and returns the peptide it represents\n",
    "    Note that argmax on equal values defaults to the smallest index\"\"\"\n",
    "    if one_hot.ndim == 1:\n",
    "        one_hot = one_hot.reshape((-1,NUM_AMINO_ACIDS))\n",
    "    \n",
    "    return ''.join([IND_TO_CHAR[one_hot[i].argmax()]for i in range(len(one_hot))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['AAAAA','AAAII','AAAHH','AAAFF','AAARR','AAAGG','AAAYY','AAAKK','AAAQQ','AAAPP','AAAVV' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_function(examples,batch_size, iteration):\n",
    "    input_array = []\n",
    "    for j in range(iteration * batch_size, (iteration+1) * batch_size):\n",
    "        embedding = peptide_to_vector(examples[j])\n",
    "        embedding = embedding.reshape(-1)\n",
    "        input_array.append(embedding)\n",
    "    return input_array\n",
    "\n",
    "def sequence_data_function(examples,batch_size, iteration):\n",
    "    input_array = []\n",
    "    for j in range(iteration * batch_size, (iteration+1) * batch_size):\n",
    "        embedding = peptide_to_vector(examples[j])\n",
    "        #embedding = embedding.reshape(-1)\n",
    "        input_array.append(embedding)\n",
    "    return input_array\n",
    "\n",
    "#fix random blanks appearing in the middle\n",
    "def noise_function(batch_size):\n",
    "    \n",
    "    input_array = []\n",
    "    for j in range(batch_size):\n",
    "        a = np.zeros([MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS ])\n",
    "        for i in range(MAX_PEPTIDE_LENGTH):\n",
    "            \n",
    "            x = np.random.randint(NUM_AMINO_ACIDS) \n",
    "            a[i][x] = np.random.normal(0.,1.)\n",
    "        a = a.reshape(-1)\n",
    "        input_array.append(a)\n",
    "    return input_array\n",
    "\n",
    "def sequence_noise_function(batch_size):\n",
    "    \n",
    "    input_array = []\n",
    "    for j in range(batch_size):\n",
    "        a = np.zeros([MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS ])\n",
    "        for i in range(MAX_PEPTIDE_LENGTH):\n",
    "            \n",
    "            x = np.random.randint(NUM_AMINO_ACIDS) \n",
    "            a[i][x] = np.random.normal(0.,1.)\n",
    "        #a = a.reshape(-1)\n",
    "        input_array.append(a)\n",
    "    return input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_function(seq,2,0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 21])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(sequence_data_function(seq,2,0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__Q__', '___N_', 'H__TY', 'AIG__', '_DRYM', 'TNL__']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(vector_to_peptide,noise_function(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_length,output_length):\n",
    "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
    "        Args:\n",
    "            input_length (int array): max_length * number_of_characters \n",
    "                                      (\"noise vector\")\n",
    "            layers (List[int]): A list of layer widths including output width\n",
    "            output_activation: torch activation function or None\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_length, 1800)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(1800, 1440)\n",
    "        self.linear3 = nn.Linear(1440, output_length)\n",
    "        self.linear4 = nn.Linear(1080, 720)\n",
    "        self.linear5 = nn.Linear(720, 360)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        intermediate = self.linear1(input_tensor)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear2(intermediate)\n",
    "        intermediate = self.relu(intermediate)\n",
    "        intermediate = self.linear3(intermediate)\n",
    "        \"\"\"intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear4(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.linear5(intermediate)\"\"\"\n",
    "        if self.output_activation is not None:\n",
    "            intermediate = self.output_activation(intermediate)\n",
    "        return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, layers):\n",
    "        \"\"\"A discriminator for discerning real from generated samples.\n",
    "        params:\n",
    "            input_dim (int): width of the input\n",
    "            layers (List[int]): A list of layer widths including output width\n",
    "        Output activation is Sigmoid.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self._init_layers(layers)\n",
    "\n",
    "    def _init_layers(self, layers):\n",
    "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
    "        self.module_list = nn.ModuleList()\n",
    "        last_layer = self.input_dim\n",
    "        for index, width in enumerate(layers):\n",
    "            self.module_list.append(nn.Linear(last_layer, width))\n",
    "            last_layer = width\n",
    "            if index + 1 != len(layers):\n",
    "                self.module_list.append(nn.LeakyReLU())\n",
    "            else:\n",
    "                self.module_list.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "        intermediate = input_tensor\n",
    "        for layer in self.module_list:\n",
    "            intermediate = layer(intermediate)\n",
    "        return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGAN(batch_size: int = 25, epochs: int = 5, max_data: int = len(seq), print_every: int = 10):\n",
    "\n",
    "    #Array to monitor losses\n",
    "    loss_g = []\n",
    "    loss_d = []\n",
    "    input_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "    output_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(input_length,output_length)\n",
    "    discriminator = Discriminator(input_length, [64, 32, 1])  \n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.1)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.1)\n",
    "\n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for j in range(int(max_data/batch_size)):\n",
    "            # zero the gradients on each iteration\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Create noisy input for generator\n",
    "            # Need float type instead of int\n",
    "            noise = noise_function(batch_size)\n",
    "            noise_data = torch.tensor(noise).float()\n",
    "            generated_data = generator(noise_data)\n",
    "            #print(\"Generated data in loop\")\n",
    "            #print(generated_data)\n",
    "\n",
    "            # Generate examples of even real data\n",
    "            true_data = data_function(seq,batch_size, j)\n",
    "            if i % print_every ==0:\n",
    "                print(\"True data used: \", list(map(vector_to_peptide,true_data)) )\n",
    "            true_labels = torch.tensor(np.ones(batch_size)).float()\n",
    "            true_data = torch.tensor(true_data).float()\n",
    "\n",
    "            # Train the generator\n",
    "            # We invert the labels here and don't train the discriminator because we want the generator\n",
    "            # to make things the discriminator classifies as true.\n",
    "            generator_discriminator_out = discriminator(generated_data)\n",
    "            generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            # Train the discriminator on the true/generated data\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            true_discriminator_out = discriminator(true_data)\n",
    "            true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "            # add .detach() here think about this\n",
    "            generator_discriminator_out = discriminator(generated_data.detach())\n",
    "            generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "            discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        generated_data = generated_data.detach().numpy()\n",
    "        if i % print_every == 0:\n",
    "            print(\"Epoch: \",i,)\n",
    "            print(\"generated data:\")\n",
    "            print(generated_data)\n",
    "            print(\"peptide of noisedata[0]\")\n",
    "            print(vector_to_peptide(noise_data.numpy()[0]))\n",
    "            print(\"peptide of generated data[0]\")\n",
    "            print(vector_to_peptide(generated_data[0]))\n",
    "        \"\"\"This threshold only makes sense if there's a sigmoid activation for the Generator\"\"\"\n",
    "        #generated_data[generated_data > 0.5] = 1\n",
    "        #generated_data[generated_data <= 0.5] = 0           \n",
    "        sequence = np.reshape(generated_data[0], [MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS])\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(\"sequence:\")\n",
    "            print(sequence)\n",
    "            print(\"peptide of sequence\")\n",
    "            print(vector_to_peptide(sequence))\n",
    "            print(\"Generator Loss :\",generator_loss.item())\n",
    "            print(\"Discriminator Loss: \", discriminator_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  0\n",
      "generated data:\n",
      "[[0.50014126 0.5030785  0.49608636 ... 0.50473934 0.49990073 0.50308037]\n",
      " [0.49648586 0.49489465 0.49600834 ... 0.49837855 0.4978191  0.5011022 ]\n",
      " [0.5016147  0.4979055  0.4948813  ... 0.497995   0.5013907  0.5027408 ]\n",
      " ...\n",
      " [0.5000933  0.5041413  0.49377877 ... 0.49386957 0.49332958 0.49800912]\n",
      " [0.48935634 0.4971755  0.5030058  ... 0.49008027 0.5044937  0.5099868 ]\n",
      " [0.49490184 0.4989096  0.50248766 ... 0.4921616  0.50085753 0.5068339 ]]\n",
      "peptide of noisedata[0]\n",
      "K_IAA\n",
      "peptide of generated data[0]\n",
      "HHFGT\n",
      "sequence:\n",
      "[[0.50014126 0.5030785  0.49608636 0.50269157 0.5046573  0.50238925\n",
      "  0.5053858  0.49650666 0.5052731  0.4986546  0.5006881  0.5075788\n",
      "  0.49783307 0.5050796  0.4949454  0.50169545 0.499763   0.49433225\n",
      "  0.5129779  0.5053514  0.5068786 ]\n",
      " [0.49709252 0.49281713 0.48931667 0.5022155  0.49631658 0.49923655\n",
      "  0.4953828  0.49465007 0.50665945 0.4973114  0.499763   0.5033623\n",
      "  0.50382173 0.5043855  0.4982302  0.49979618 0.5023342  0.50567013\n",
      "  0.50844514 0.4957583  0.50159067]\n",
      " [0.4898489  0.49943808 0.51330644 0.49062598 0.4973865  0.5034863\n",
      "  0.49627137 0.5027312  0.4931672  0.4996515  0.5047943  0.49124926\n",
      "  0.49178046 0.5073244  0.49306375 0.51205903 0.504449   0.49964857\n",
      "  0.5083005  0.49376506 0.49957013]\n",
      " [0.51130164 0.49499616 0.48876277 0.49640235 0.5080704  0.49565694\n",
      "  0.496332   0.50849295 0.49878892 0.5040739  0.50270444 0.4998944\n",
      "  0.50678    0.49807262 0.49849364 0.4990362  0.51352745 0.4974299\n",
      "  0.5016935  0.50205505 0.4993835 ]\n",
      " [0.4962608  0.4893717  0.5028614  0.48910445 0.5055995  0.49719986\n",
      "  0.5066606  0.48732308 0.50322986 0.50082844 0.49373847 0.49730358\n",
      "  0.49923792 0.5051213  0.50010175 0.49507287 0.49677074 0.4997826\n",
      "  0.50473934 0.49990073 0.50308037]]\n",
      "peptide of sequence\n",
      "HHFGT\n",
      "Generator Loss : 0.6403994560241699\n",
      "Discriminator Loss:  0.6917756795883179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  20\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_WR_L\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  40\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_AL__\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  60\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "G_H_M\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n",
      "True data used:  ['AAAAA', 'AAAII', 'AAAHH', 'AAAFF', 'AAARR', 'AAAGG', 'AAAYY', 'AAAKK', 'AAAQQ', 'AAAPP']\n",
      "Epoch:  80\n",
      "generated data:\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "peptide of noisedata[0]\n",
      "_I_L_\n",
      "peptide of generated data[0]\n",
      "D_RR_\n",
      "sequence:\n",
      "[[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1.]]\n",
      "peptide of sequence\n",
      "D_RR_\n",
      "Generator Loss : 27.631023406982422\n",
      "Discriminator Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "simpleGAN(batch_size=10, epochs=100, print_every = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "max_data= len(seq)\n",
    "batch_size = 2\n",
    "print_every = 100\n",
    "loss_g = []\n",
    "loss_d = []\n",
    "input_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "output_length = MAX_PEPTIDE_LENGTH*NUM_AMINO_ACIDS\n",
    "\n",
    "# Models\n",
    "generator = Generator(input_length,output_length)\n",
    "discriminator = Discriminator(input_length, [64, 32, 1])  \n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.1)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.1)\n",
    "\n",
    "# loss\n",
    "loss = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 38.396087646484375\n",
      "Discriminator Loss:  0.06820950657129288\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n",
      "sequence:\n",
      "[[0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.]]\n",
      "peptide of sequence\n",
      "LRE__\n",
      "Generator Loss : 27.63102149963379\n",
      "Discriminator Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    for j in range(int(max_data/batch_size)):\n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # Need float type instead of int\n",
    "        noise = noise_function(batch_size)\n",
    "        noise_data = torch.tensor(noise).float()\n",
    "        generated_data = generator(noise_data)\n",
    "        #print(\"Generated data in loop\")\n",
    "        #print(generated_data)\n",
    "\n",
    "        # Generate examples of even real data\n",
    "        true_data = data_function(seq,batch_size, j)\n",
    "        \"\"\"if i % print_every ==0:\n",
    "            print(\"True data used: \", list(map(vector_to_peptide,true_data)) )\"\"\"\n",
    "        true_labels = torch.tensor(np.ones(batch_size)).float()\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "\n",
    "        # Train the generator\n",
    "        # We invert the labels here and don't train the discriminator because we want the generator\n",
    "        # to make things the discriminator classifies as true.\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "    generated_data = generated_data.detach().numpy()\n",
    "    \"\"\"if i % print_every == 0:\n",
    "        print(\"Epoch: \",i,)\n",
    "        print(\"generated data:\")\n",
    "        print(generated_data)\n",
    "        print(\"peptide of noisedata[0]\")\n",
    "        print(vector_to_peptide(noise_data.numpy()[0]))\n",
    "        print(\"peptide of generated data[0]\")\n",
    "        print(vector_to_peptide(generated_data[0]))\"\"\"\n",
    "    \n",
    "    \"\"\"This threshold only makes sense if there's a sigmoid activation for the Generator\"\"\"\n",
    "    #generated_data[generated_data > 0.5] = 1\n",
    "    #generated_data[generated_data <= 0.5] = 0           \n",
    "    sequence = np.reshape(generated_data[0], [MAX_PEPTIDE_LENGTH,NUM_AMINO_ACIDS])\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print(\"sequence:\")\n",
    "        print(sequence)\n",
    "        print(\"peptide of sequence\")\n",
    "        print(vector_to_peptide(sequence))\n",
    "        print(\"Generator Loss :\",generator_loss.item())\n",
    "        print(\"Discriminator Loss: \", discriminator_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data = data_function(seq,2,0)\n",
    "true_data = torch.tensor(true_data).float()\n",
    "true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator(true_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3637,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2313,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1523,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.1494,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3479,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.8233,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.8060,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9097,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.8831,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  1.4132,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.6513,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.3653, -0.1038,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4611,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.6056,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1110,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          2.6655,  0.0000,  0.0000,  0.0000,  0.0000,  0.8465,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0663,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.3697,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4484,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -1.5670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.7660,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.2988,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.8098,  0.0000,\n",
       "          0.0000]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = noise_function(5)\n",
    "fake_data = torch.tensor(fake_data).float()\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=21, nhead=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.Tensor(sequence_data_function(seq, 6,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 21])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 21])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4481e-01,  6.6617e-01, -1.3970e-01, -1.0282e+00,  6.0554e-01,\n",
       "           2.3346e-01, -4.1403e-01, -5.6244e-01,  5.1799e-01, -1.7779e-02,\n",
       "          -9.2551e-01, -6.5734e-01,  4.9031e-02,  3.7309e+00, -7.2491e-02,\n",
       "           6.0904e-01, -7.5979e-01, -2.9054e-01, -1.1108e-02,  1.1842e-01,\n",
       "          -1.4068e+00],\n",
       "         [-2.6120e-01,  3.1106e-01, -6.6197e-02, -7.9244e-01,  4.8832e-01,\n",
       "           1.7164e-01, -1.0420e+00, -3.9347e-01,  4.5693e-01,  2.0043e-01,\n",
       "          -7.9391e-01, -7.0516e-01, -7.1452e-02,  4.0520e+00, -2.1984e-01,\n",
       "           1.3717e-01, -6.1475e-01, -3.2489e-01,  1.5479e-01, -1.4005e-01,\n",
       "          -5.4697e-01],\n",
       "         [-1.4397e-01,  6.1190e-01, -9.2641e-02, -1.4272e-01,  4.4921e-01,\n",
       "           1.5151e-01, -1.1872e+00, -6.5009e-01,  2.7911e-01,  2.3175e-01,\n",
       "          -2.3851e-01, -9.1004e-01, -1.4608e-01,  3.8973e+00, -1.5794e-01,\n",
       "           3.4007e-01, -4.3282e-01, -2.0688e-01, -1.8729e-01, -9.4356e-02,\n",
       "          -1.3703e+00],\n",
       "         [-6.6073e-01, -1.8597e-01, -5.7522e-01, -2.5094e-01,  2.1614e-01,\n",
       "           2.0701e-03, -2.4116e-01,  2.4240e-01, -3.6305e-01, -2.9703e-01,\n",
       "          -5.9182e-01, -6.4755e-01,  3.0350e-01,  4.2674e+00, -1.8381e-01,\n",
       "           8.5346e-02, -3.5998e-01, -1.1740e-01,  2.7146e-01, -4.9897e-01,\n",
       "          -4.1465e-01],\n",
       "         [-3.8792e-01, -1.1454e-01, -4.8155e-01, -4.5339e-01, -7.5520e-02,\n",
       "          -1.6946e-02, -2.9397e-01,  3.1970e-02,  8.6003e-02, -4.9006e-01,\n",
       "          -3.4794e-01, -4.5459e-01,  1.5280e-01,  4.3288e+00, -4.0287e-01,\n",
       "           2.2986e-01, -4.6513e-01, -3.4993e-01,  1.3719e-01, -3.3454e-03,\n",
       "          -6.2891e-01]],\n",
       "\n",
       "        [[-1.0341e-01,  8.4961e-01, -4.7386e-02, -1.2698e+00,  5.4294e-01,\n",
       "           2.8382e-01, -1.2587e+00,  8.5762e-02,  1.3847e-02,  1.2707e-01,\n",
       "          -9.3016e-01, -5.4004e-01,  1.2022e-01,  3.6046e+00,  5.3052e-02,\n",
       "           5.1461e-01, -4.9255e-01, -2.5458e-01,  9.2210e-02, -4.8768e-03,\n",
       "          -1.3862e+00],\n",
       "         [-2.4082e-01,  5.2641e-01, -2.2415e-01, -1.0804e+00,  4.7725e-01,\n",
       "           1.7751e-01, -9.8499e-01, -6.1782e-01,  4.4201e-01,  2.4850e-01,\n",
       "          -1.6298e-01, -1.9282e-01, -7.1542e-02,  3.8604e+00, -2.5729e-02,\n",
       "           3.2071e-01, -5.4487e-01, -4.8861e-01,  6.5764e-02, -8.5057e-02,\n",
       "          -1.3988e+00],\n",
       "         [-4.7954e-02,  8.6835e-01, -1.6700e-01, -1.2270e+00,  5.5460e-01,\n",
       "           1.9076e-01, -1.0896e+00, -5.7033e-01,  7.6282e-01,  9.1720e-02,\n",
       "          -1.0731e+00, -7.4772e-01,  7.0968e-02,  3.6568e+00, -2.6815e-01,\n",
       "           4.7998e-01, -7.6993e-01, -4.2362e-01, -2.9884e-02,  5.0568e-02,\n",
       "          -3.1226e-01],\n",
       "         [-3.7634e-01, -4.6867e-01,  2.2715e-01, -4.3363e-01,  3.3315e-01,\n",
       "           1.8450e-02, -3.4179e-01, -4.0655e-01,  4.2814e+00, -4.7118e-01,\n",
       "          -8.8259e-02, -1.4590e-01, -2.2127e-01, -9.8399e-01, -3.7195e-01,\n",
       "           2.8793e-01, -2.8255e-01, -9.5507e-02, -1.5222e-01, -4.0775e-02,\n",
       "          -2.6754e-01],\n",
       "         [-3.6601e-01,  4.3815e-01,  1.3812e-01, -3.3907e-01,  1.8939e-01,\n",
       "          -6.7905e-02, -4.6086e-01,  4.3259e-02,  4.2048e+00, -4.0061e-01,\n",
       "          -4.8927e-01, -1.0894e-01, -1.2331e-01, -9.6390e-01, -6.1434e-01,\n",
       "           4.8640e-01, -5.3636e-01, -1.4791e-01, -1.7270e-01, -2.0688e-01,\n",
       "          -5.0211e-01]],\n",
       "\n",
       "        [[-9.2933e-03,  7.3882e-01, -1.1607e-01, -1.1630e+00,  5.1562e-01,\n",
       "           2.5679e-01, -1.1026e+00, -5.2506e-01, -1.6799e-01,  4.6594e-01,\n",
       "          -9.5323e-01, -1.3096e-01, -8.3433e-02,  3.6007e+00, -8.1666e-02,\n",
       "           5.8521e-01, -5.5556e-01, -3.0637e-01,  2.0575e-01,  3.1830e-01,\n",
       "          -1.4919e+00],\n",
       "         [-2.2989e-01,  8.3665e-01, -1.0191e-01, -9.0675e-01,  5.9066e-02,\n",
       "           6.1011e-02, -9.6379e-01, -3.1930e-01,  5.9015e-01,  3.6436e-01,\n",
       "          -6.9059e-01, -6.6538e-01, -6.4670e-02,  3.8120e+00, -8.8518e-02,\n",
       "           3.9137e-01, -4.5217e-01, -3.0395e-01,  3.3651e-02,  4.5613e-02,\n",
       "          -1.4069e+00],\n",
       "         [-1.5229e-01,  6.0320e-01, -2.2758e-01, -1.2309e+00,  5.6871e-01,\n",
       "           2.4596e-01, -9.5950e-01, -5.2409e-01,  4.3936e-01,  3.0254e-01,\n",
       "          -7.5954e-01, -5.1340e-01,  1.9288e-02,  3.8716e+00,  4.6780e-02,\n",
       "           4.0240e-01, -6.6053e-01, -2.2974e-01, -6.4130e-02, -5.8023e-01,\n",
       "          -5.9795e-01],\n",
       "         [-7.1584e-01,  1.5160e-03, -3.7437e-03, -5.7932e-01, -1.5077e-01,\n",
       "          -4.1187e-02, -8.4600e-02,  5.0635e-01, -5.1791e-01, -4.1913e-01,\n",
       "          -5.3021e-01, -3.3306e-01,  3.2770e-01, -3.6551e-01, -2.4935e-01,\n",
       "          -1.8757e-01, -3.4374e-01, -2.2763e-01,  4.2947e+00, -2.2920e-01,\n",
       "          -1.5144e-01],\n",
       "         [-7.2939e-01, -1.2664e-01,  2.1811e-01, -2.4764e-01, -4.4854e-01,\n",
       "           7.4078e-02, -1.2607e-01,  2.5213e-01, -3.7456e-01, -7.6332e-02,\n",
       "          -5.2383e-01, -1.9504e-01, -3.3918e-02, -6.6231e-01, -9.7860e-02,\n",
       "          -4.2958e-02, -1.7175e-01, -4.1594e-01,  4.3286e+00, -1.8305e-01,\n",
       "          -4.1710e-01]],\n",
       "\n",
       "        [[ 1.6561e-02,  1.1354e+00,  1.1871e-01, -1.0668e+00,  2.8267e-01,\n",
       "           3.4024e-01, -1.2085e+00, -3.5024e-01,  5.5827e-01, -1.2598e-01,\n",
       "          -1.9972e-01, -7.8234e-01, -2.8516e-01,  3.5069e+00, -1.9334e-01,\n",
       "           5.2899e-01, -5.1949e-01, -3.6912e-01, -8.2997e-03,  2.4501e-01,\n",
       "          -1.6238e+00],\n",
       "         [-1.7631e-01,  9.1578e-01, -6.6008e-02, -1.0018e+00,  3.5221e-01,\n",
       "           9.9408e-03, -1.0691e+00, -5.5327e-01,  4.9008e-01, -6.7814e-03,\n",
       "          -2.2288e-01, -2.1229e-01, -8.7973e-02,  3.8188e+00, -2.7498e-01,\n",
       "           4.3697e-01, -6.7823e-01, -5.2292e-01,  1.3575e-01, -2.0390e-02,\n",
       "          -1.2666e+00],\n",
       "         [-7.5353e-02,  9.8189e-01, -5.7003e-02, -1.0117e+00, -4.1235e-02,\n",
       "           3.1792e-01, -1.1933e+00, -5.0649e-01,  8.1537e-01,  4.6844e-01,\n",
       "          -8.6481e-01, -6.8441e-01,  1.1061e-02,  3.4890e+00, -1.3371e-01,\n",
       "           6.5569e-01, -6.7514e-01, -2.6868e-01,  6.9855e-02,  1.0094e-01,\n",
       "          -1.3983e+00],\n",
       "         [-3.5536e-01,  4.6202e-01,  4.2969e+00, -3.2092e-01, -2.6009e-01,\n",
       "          -2.1810e-01, -3.5520e-01,  1.8108e-01, -1.9141e-01, -1.9188e-01,\n",
       "          -7.7119e-01, -2.6928e-01,  2.1006e-01, -2.9577e-01, -4.1758e-01,\n",
       "           4.4968e-03, -3.0118e-03, -5.0535e-01, -5.3088e-01,  1.6307e-02,\n",
       "          -4.8487e-01],\n",
       "         [-2.1767e-01,  3.8749e-01,  4.2088e+00, -3.3711e-01, -1.6076e-01,\n",
       "          -1.4335e-01, -2.6571e-02, -4.8383e-02, -2.5285e-01, -6.3203e-02,\n",
       "          -9.3680e-01, -3.0218e-01,  2.4522e-01, -8.7106e-01, -6.2217e-01,\n",
       "           2.2614e-01,  1.7071e-01, -4.6781e-01, -3.0673e-01,  5.3507e-02,\n",
       "          -5.3521e-01]],\n",
       "\n",
       "        [[-3.2041e-01,  9.9721e-01, -2.4390e-01, -3.4242e-02,  1.1954e-03,\n",
       "           2.0505e-01, -1.2412e+00, -5.5394e-01,  3.6096e-01, -2.8482e-02,\n",
       "          -8.6232e-01, -5.6050e-01, -3.3463e-02,  3.7236e+00,  7.6788e-02,\n",
       "           3.9565e-01, -8.0638e-01, -6.3069e-02,  1.8205e-01,  2.1929e-01,\n",
       "          -1.4139e+00],\n",
       "         [-1.9848e-01,  8.8648e-01, -1.2761e-01, -1.1321e+00,  4.9741e-01,\n",
       "           1.3718e-01, -1.2310e+00, -7.2609e-01,  7.9614e-01,  1.0727e-01,\n",
       "          -2.6074e-01, -5.3447e-01, -1.3772e-01,  3.5378e+00, -1.1953e-01,\n",
       "           5.9294e-01, -3.7359e-01, -3.2407e-01,  4.4675e-02,  1.1725e-01,\n",
       "          -1.5517e+00],\n",
       "         [-4.3598e-01,  3.9626e-01, -1.1565e-01, -9.4919e-01,  5.5547e-01,\n",
       "          -1.1624e-01, -9.4503e-01, -5.0099e-01,  2.7710e-01,  9.0475e-02,\n",
       "          -5.6780e-01, -6.7281e-01,  9.1826e-02,  3.9147e+00,  8.5199e-02,\n",
       "           2.7822e-01, -3.8952e-01,  2.7972e-01,  1.6504e-01, -1.2921e-01,\n",
       "          -1.3116e+00],\n",
       "         [ 1.1039e-01,  4.1922e+00, -3.1032e-01, -7.2490e-01,  8.7133e-02,\n",
       "           3.0273e-01, -7.3389e-02,  2.1083e-03,  1.4910e-01, -7.8712e-01,\n",
       "          -5.8771e-01, -8.8041e-01,  1.5780e-01, -4.7886e-01, -4.1859e-01,\n",
       "           2.1775e-01, -2.2886e-01,  8.9377e-02,  1.2154e-02, -5.6024e-01,\n",
       "          -2.7035e-01],\n",
       "         [-2.0086e-02,  4.2849e+00, -4.3249e-01, -4.8972e-01,  1.3073e-01,\n",
       "          -1.3248e-01, -3.1014e-03,  1.1129e-01, -2.7954e-02, -5.3008e-01,\n",
       "          -5.7301e-01, -8.3084e-01,  2.0596e-01, -6.9384e-01, -3.5656e-01,\n",
       "           8.1776e-03, -9.3177e-02, -1.2104e-03,  6.5514e-02, -3.7120e-01,\n",
       "          -2.5083e-01]],\n",
       "\n",
       "        [[ 1.6331e-02,  8.1210e-01, -3.6260e-01, -9.9534e-01,  3.9306e-01,\n",
       "           3.9279e-01, -1.1475e+00, -4.1572e-01,  6.4899e-01, -1.1145e-01,\n",
       "          -8.7839e-01, -4.8976e-01, -2.2343e-01,  3.5916e+00,  6.0271e-02,\n",
       "           7.3148e-01, -6.8084e-01, -1.5065e-01,  1.9861e-01,  1.0509e-02,\n",
       "          -1.4001e+00],\n",
       "         [-2.2452e-01,  1.0121e+00, -1.1319e-01, -1.0037e+00,  4.0378e-01,\n",
       "           1.8794e-01, -1.2142e+00, -6.2722e-01,  6.6294e-01,  8.1339e-02,\n",
       "          -8.0031e-01, -5.8549e-01, -8.9096e-02,  3.6847e+00,  4.3429e-05,\n",
       "           2.8692e-01, -4.6716e-01, -7.5343e-02,  1.0166e-01,  6.7583e-03,\n",
       "          -1.2279e+00],\n",
       "         [-1.2865e-01,  5.6663e-01,  3.7005e-02, -9.3354e-01,  6.1020e-01,\n",
       "          -5.1233e-02, -9.2762e-01, -7.3172e-02,  4.7247e-01,  8.0137e-02,\n",
       "          -9.6022e-01, -5.7820e-01,  3.7028e-02,  3.8083e+00, -1.0647e-01,\n",
       "           4.1159e-01, -4.6486e-01, -2.6242e-01, -1.2814e-02, -8.0075e-02,\n",
       "          -1.4441e+00],\n",
       "         [-3.4236e-01,  5.2007e-01,  2.4180e-01, -7.9111e-01, -4.0008e-01,\n",
       "          -8.3961e-02, -1.1552e-01,  1.1072e-01,  9.7456e-04,  5.1123e-02,\n",
       "          -6.6549e-01, -6.1874e-01, -5.4925e-02, -6.2513e-01, -2.3417e-01,\n",
       "          -1.7955e-01,  4.2251e+00, -4.6122e-01, -4.5611e-01,  2.0954e-01,\n",
       "          -3.3099e-01],\n",
       "         [-4.0497e-01,  5.9672e-01,  2.5036e-01, -8.2229e-01, -2.9439e-01,\n",
       "           1.9381e-02,  2.7753e-02,  1.3201e-01, -1.5752e-01,  2.2960e-02,\n",
       "          -6.2704e-01, -6.4111e-01, -1.2545e-02, -7.2404e-01, -3.1996e-01,\n",
       "           2.8348e-01,  4.1350e+00, -5.4313e-01, -7.2019e-01,  1.8872e-01,\n",
       "          -3.8918e-01]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "# unset BREAK_EARLY if we must not stop training earlier than max_epochs\n",
    "\n",
    "BREAK_EARLY = True\n",
    "max_epochs = 100\n",
    "BLANK_CHAR = '~'\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "##################################\n",
    "# argv\n",
    "# either pass 0 args or 1 args\n",
    "if (len(sys.argv)) == 1:\n",
    "    Arg_dataset_name = \"none\"\n",
    "    Arg_num_layers = 1\n",
    "    Arg_seqmaxlen = 22\n",
    "else:\n",
    "    _, Arg_dataset_name, Arg_num_layers, Arg_seqmaxlen = sys.argv\n",
    "    Arg_num_layers = int(Arg_num_layers)\n",
    "    Arg_seqmaxlen = int(Arg_seqmaxlen)\n",
    "\n",
    "Imagesuffix = Arg_dataset_name + \".L\" + str(Arg_num_layers) + \".s\" + str(Arg_seqmaxlen)\n",
    "##################################\n",
    "\n",
    "if Arg_dataset_name == 'coffee':\n",
    "    learning_rate = 0.1\n",
    "elif Arg_dataset_name == 'emails':\n",
    "    learning_rate = 0.01\n",
    "elif Arg_dataset_name == 'tls':\n",
    "    learning_rate = 0.02\n",
    "##################################\n",
    "\n",
    "\n",
    "Train_end_reason = \"Max_epoch done\"\n",
    "\n",
    "#inp_alphabet = \".ab@_yz\"\n",
    "#out_alphabet = \".FT\"\n",
    "\n",
    "def get_alphabet(d_set):\n",
    "    letters = set()\n",
    "    for word in d_set:\n",
    "        letters.update(set(word))\n",
    "\n",
    "    return list([ BLANK_CHAR ]) + sorted(list(letters))\n",
    "\n",
    "\n",
    "def simple_encode_strlist(X, max, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    #example: {'.': 0, 'a': 1, 'b': 2}\n",
    "    X = [ [char_to_int[char] for char in x1.strip()] for x1 in X]\n",
    "    X1 = [ x + [0] * (max - len(x)) for x in X]      # pad to max\n",
    "    #example: simple_encode_inp([\"aa\",\"bb\"],3) gives [[1, 1, 0], [2, 2, 0]]\n",
    "    return X1\n",
    "\n",
    "def simple_decode_idxlist1(enc_data, alphabet):\n",
    "    idx2char = alphabet\n",
    "    out = [idx2char[int(x)] for x in enc_data]\n",
    "    out = ''.join(out)\n",
    "    return out\n",
    "\n",
    "def simple_decode_idxlist(d_arr, alphabet):\n",
    "    return [simple_decode_idxlist1(d, alphabet) for d in d_arr]\n",
    "\n",
    "def load_data_file(filename):\n",
    "    data = pandas.read_csv(filename, header=None)\n",
    "    print (\"=== loaded data shape: \", data.shape)\n",
    "    print (\"=== data \", data)\n",
    "\n",
    "    X_pandas = data[0]\n",
    "    X_pandas = [ x.strip() for x in X_pandas]\n",
    "    X_max = max([ len(x) for x in X_pandas])\n",
    "\n",
    "    Y_pandas = data[1]\n",
    "    Y_pandas = [ y.strip() for y in Y_pandas]\n",
    "    Y_max = max([ len(y) for y in Y_pandas])\n",
    "\n",
    "    if X_max != Y_max:\n",
    "        print(\"max len mismatch in X and Y\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    seq_max = X_max\n",
    "    return X_pandas, Y_pandas, seq_max\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "def ohe_singleletter(val, max):\n",
    "    letter = [0 for _ in range(max)]\n",
    "    letter[val] = 1\n",
    "    return letter\n",
    "\n",
    "\n",
    "def simple_to_onehot(D, alphabet):\n",
    "    itemlist = list()\n",
    "    for d in D:\n",
    "        item = list()\n",
    "        for e in d:\n",
    "            l = ohe_singleletter(e, len(alphabet))\n",
    "            item.append(l)\n",
    "            #print(e, l)\n",
    "        #print(d, \"------\",  item)\n",
    "        itemlist.append(item)\n",
    "    return itemlist\n",
    "\n",
    "def onehot_decode_to_simple1(d):\n",
    "    arr = np.array(d)\n",
    "    idx = arr.argmax(1)\n",
    "    return idx\n",
    "\n",
    "def onehot_decode_to_simple(D):\n",
    "    arr = np.array(D)\n",
    "    idx = arr.argmax(2)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def decode_inp(input):\n",
    "    input = onehot_decode_to_simple1(input)\n",
    "    input = simple_decode_idxlist1(input, Inp_alphabet)\n",
    "    return input\n",
    "\n",
    "def decode_out(output):\n",
    "    output = simple_decode_idxlist1(output, Out_alphabet)\n",
    "    return output\n",
    "\n",
    "#################################\n",
    "\n",
    "X_data, Y_data, train_Seq_max = load_data_file('rnn3-train-data.txt')\n",
    "Num_io_data = len(X_data)\n",
    "\n",
    "# set Seq_max to  max in both test and train\n",
    "_, _, test_Seq_max = load_data_file('rnn3-test-data.txt')\n",
    "Seq_max = max([train_Seq_max, test_Seq_max])\n",
    "\n",
    "# if Seq_max <= Arg_seqmaxlen:\n",
    "#     print(\"=== updating Seq_max from\", Seq_max, \" to \", Arg_seqmaxlen)\n",
    "#     Seq_max = Arg_seqmaxlen\n",
    "# else:\n",
    "#     print(\"=== seq max len mismatch\", Seq_max, Arg_seqmaxlen)\n",
    "#     sys.exit(1)\n",
    "\n",
    "if Arg_dataset_name == 'tls':\n",
    "    Inp_alphabet =  [BLANK_CHAR, 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    Out_alphabet =  [ 'E', 'S', 'X', 'H', 'U', 'F', 'W', 'I', 'A', 'C' ]\n",
    "    Out_alphabet =  list([ BLANK_CHAR ]) + sorted(Out_alphabet)\n",
    "\n",
    "else:\n",
    "    Inp_alphabet = get_alphabet(X_data)\n",
    "    Out_alphabet = get_alphabet(Y_data)\n",
    "#print(''.join(Inp_alphabet))\n",
    "#print(''.join(Out_alphabet))\n",
    "print(\"=== inp_alphabet:\", Inp_alphabet)\n",
    "print(\"=== out_alphabet:\", Out_alphabet)\n",
    "\n",
    "X_data = simple_encode_strlist(X_data, Seq_max, Inp_alphabet)\n",
    "Y_data = simple_encode_strlist(Y_data, Seq_max, Out_alphabet)\n",
    "\n",
    "#print(X_data)\n",
    "\n",
    "Num_samples = len(X_data)\n",
    "\n",
    "#print(simple_decode_idxlist1(X_data[0], Inp_alphabet))\n",
    "\n",
    "#print(simple_decode_idxlist(X_data, Inp_alphabet))\n",
    "#print(simple_decode_idxlist(Y_data, Out_alphabet))\n",
    "\n",
    "X_ohe = simple_to_onehot(X_data, Inp_alphabet)\n",
    "#print(X_ohe)\n",
    "\n",
    "#print(onehot_decode_to_simple(X_ohe))\n",
    "#print(\"------------\")\n",
    "\n",
    "#################################\n",
    "class Model_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'RNN'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True)\n",
    "        self.Rnn_type = 'LSTM'\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Reshape input in (bs, seqlen, inpsz)\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "\n",
    "        # Propagete inp thru RNN\n",
    "        #   Input: (batchsz, seq_len, inpsz)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Output: (batchsz, seq_len, hiddensz)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Init hidden and cell states\n",
    "        # (num_layers * num_dir, batch, hidden_sz)\n",
    "        # LSTM hidden state is a tuple (h_0, c_0)\n",
    "        h_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        c_0 = Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        hidden = (h_0, c_0)\n",
    "        return hidden\n",
    "\n",
    "#################################\n",
    "\n",
    "Inp_size = len(Inp_alphabet)\n",
    "Out_size = len(Out_alphabet)\n",
    "torch.manual_seed(2.7321)\n",
    "\n",
    "#################################\n",
    "num_classes = Out_size          #  XXX why need a separate num_classes,  when hidden_size would do?\n",
    "input_size = Inp_size  # this parameter is for the view function to know how large\n",
    "#the one hot vector is supposed to be\n",
    "\n",
    "hidden_size = Out_size #\n",
    "batch_size = 1   # id dont understand this\n",
    "seq_len = Seq_max\n",
    "num_layers = Arg_num_layers  # num-layers of rnn\n",
    "#learning_rate = 0.01\n",
    "momentum = 0.1\n",
    "\n",
    "\n",
    "\n",
    "vizdelay = 30\n",
    "################################\n",
    "\n",
    "# function to reduce learning rate based on accuracy.\n",
    "# it returns a new optimizer\n",
    "def get_new_optimizer(learning_rate, accuracy):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if accuracy >= 50:\n",
    "        new_learning_rate = learning_rate * 0.9\n",
    "    if accuracy >= 60:\n",
    "        new_learning_rate = learning_rate * 0.8\n",
    "    if accuracy >= 70:\n",
    "        new_learning_rate = learning_rate * 0.7\n",
    "    if accuracy >= 80:\n",
    "        new_learning_rate = learning_rate * 0.5\n",
    "    if accuracy >= 90:\n",
    "        new_learning_rate = learning_rate * 0.4\n",
    "    if accuracy >= 95:\n",
    "        new_learning_rate = learning_rate * 0.2\n",
    "    if accuracy >= 98:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=new_learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "############## Model Training code ##########################\n",
    "def train_phase():\n",
    "    global Optimizer\n",
    "    MIN_LOSS = float('inf')\n",
    "    MIN_LOSS_epoch_counter = 0\n",
    "    MAX_ACC_epoch_counter = 0\n",
    "\n",
    "    final_train_epoch=0\n",
    "    final_train_accuracy=0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        Optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        errcount = 0\n",
    "        # if epoch % 30 ==0:\n",
    "        #     evaluate_model(inputs,labels)\n",
    "        for input, label in zip(Inputs, Labels):\n",
    "            # input = input.unsqueeze(0)\n",
    "            hidden = model.init_hidden() #we reset the RNN to its initial state\n",
    "            hidden, output = model(input, hidden) #run the model\n",
    "\n",
    "            val, idx = output.max(1)\n",
    "            expected = decode_out(label)\n",
    "            trained = decode_out(idx)\n",
    "\n",
    "            if trained != expected:\n",
    "                errcount += 1\n",
    "\n",
    "            if epoch % vizdelay == 0:\n",
    "                if (trained != expected):\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained, \"   *****\")\n",
    "                else:\n",
    "                    print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", trained)\n",
    "            loss += Criterion(output, label) #add the current sample error to loss\n",
    "\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "\n",
    "        ########### Within loop plotting and logging ########\n",
    "        if loss.data < MIN_LOSS:\n",
    "            MIN_LOSS = loss.data\n",
    "            MIN_LOSS_epoch_counter = epoch   # reset\n",
    "\n",
    "        accuracy = 100.0 * (Num_io_data - errcount) / Num_io_data\n",
    "        print(\"Epoch: %d, loss: %1.3f         errcount: %d  accuracy: %1.1f%%\\n\" % (epoch+1, loss.data, errcount, accuracy))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        plot_data.append([loss.data.item(), accuracy])\n",
    "        t = [ x for x in range(0, epoch)]\n",
    "        p = np.array(plot_data)\n",
    "        plt_ax1.plot(p[:, 0], color='red')\n",
    "        plt_ax2.plot(p[:, 1], color='blue')\n",
    "        plt.pause(0.001)\n",
    "        #plt.show()\n",
    "\n",
    "        final_train_epoch = epoch\n",
    "        final_train_accuracy = accuracy\n",
    "        if BREAK_EARLY == True:\n",
    "            global Train_end_reason\n",
    "            if (epoch - MIN_LOSS_epoch_counter) >= 20:\n",
    "                Train_end_reason = \"=== no new low of training_loss seen for last 20 epochs; stop training\"\n",
    "                print(Train_end_reason)\n",
    "                break\n",
    "\n",
    "            if accuracy >= 99.9:\n",
    "                MAX_ACC_epoch_counter += 1\n",
    "                # stop, if accuracy stays at ~100 for 10 epochs\n",
    "                if MAX_ACC_epoch_counter >= 10:\n",
    "                    Train_end_reason = \"=== maximal accuracy seen for last 10 epochs; stop training\"\n",
    "                    print(Train_end_reason)\n",
    "                    time.sleep(4)\n",
    "                    break\n",
    "\n",
    "        # update optimizer with changed lr - depending on accuracy\n",
    "        Optimizer = get_new_optimizer(learning_rate, accuracy)\n",
    "    return final_train_accuracy, final_train_epoch\n",
    "\n",
    "\n",
    "####################### Model evaluation code ################\n",
    "def evaluate_model_1(xdata, ydata):\n",
    "    x_one_hot = simple_to_onehot(xdata, Inp_alphabet)\n",
    "    inputs = Variable(torch.Tensor(x_one_hot))\n",
    "    labels = Variable(torch.LongTensor(ydata))\n",
    "\n",
    "    err_count = 0\n",
    "    for input, label in zip(inputs, labels):\n",
    "        expected = decode_out(label)\n",
    "        hidden = model.init_hidden()\n",
    "        hidden, output = model(input, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        predicted = decode_out(idx)\n",
    "        if (expected != predicted):\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted, \"   *****\")\n",
    "            err_count += 1\n",
    "        else:\n",
    "            print(\"check : \", decode_inp(input), \" -> expected: \", expected, \"        predicted: \", predicted)\n",
    "    return (err_count)\n",
    "\n",
    "def evaluate_model(xdata, ydata, seqmax):\n",
    "    xdata = simple_encode_strlist(xdata, seqmax, Inp_alphabet)\n",
    "    ydata = simple_encode_strlist(ydata, seqmax, Out_alphabet)\n",
    "    return evaluate_model_1(xdata, ydata)\n",
    "\n",
    "################################################################################\n",
    "def validation_phase():\n",
    "    print(\"============================= validation inputs ===========================\")\n",
    "    validation_inputs, validation_outputs, seqmax = load_data_file('rnn3-train-data.txt')\n",
    "    num_io_data = len(validation_inputs)\n",
    "    if seqmax != Seq_max:\n",
    "        print(\"seqmax mismatch\", seqmax, Seq_max)\n",
    "        sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "    errcount = evaluate_model(validation_inputs, validation_outputs, seqmax)\n",
    "    val_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    if errcount > 0:\n",
    "        print(\"VALIDATION FAILED:  errors: \", errcount, \"accuracy:\", val_accuracy)\n",
    "    else:\n",
    "        print(\"Validation Passed\")\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def test_phase():\n",
    "    print(\"============================= test inputs ===========================\")\n",
    "    test_inputs, test_outputs, seqmax = load_data_file('rnn3-test-data.txt')\n",
    "    num_io_data = len(test_inputs)\n",
    "    #if seqmax != Seq_max: #this could happen just due to randomness #todo\n",
    "    #    print(\"WARN: seqmax mismatch\")     ## possible to have diff seq max from the train data\n",
    "    #    sys.exit(1)\n",
    "    seqmax = Seq_max\n",
    "\n",
    "\n",
    "    errcount = evaluate_model(test_inputs, test_outputs, seqmax)\n",
    "    test_accuracy = 100.0 * (num_io_data - errcount) / num_io_data\n",
    "\n",
    "    print(\"Testset size: \", len(test_inputs),\"Test errors: \", errcount, \"accuracy:\", test_accuracy)\n",
    "    return test_accuracy\n",
    "\n",
    "################################################################################\n",
    "\n",
    "########################################################\n",
    "#Instantiate RNN model\n",
    "#model = Model_RNN()\n",
    "Rnn_type = '###'\n",
    "model = Model_LSTM()\n",
    "Rnn_type = model.Rnn_type\n",
    "\n",
    "####################### Plotting code ##################\n",
    "\n",
    "plt.ion()\n",
    "plt_fig, plt_ax1 = plt.subplots()\n",
    "plt_ax2 = plt_ax1.twinx()\n",
    "plt_ax1.set_xlabel('epoch')\n",
    "plt_ax1.set_ylabel('loss', color='red')\n",
    "plt_ax2.set_ylabel('accuracy', color='blue')\n",
    "\n",
    "txt = \"Dataset: \" + Arg_dataset_name + \"\\nrnn_type: \"+ Rnn_type + \"\\nnum_layers: \"+ str(num_layers) + \"\\nnum_io_samples: \"+ str(Num_io_data) + \"\\nseq_max_len: \"+ str(Seq_max)\n",
    "txt += \"\\nBase_LR: \" + str(learning_rate)\n",
    "\n",
    "#plt_fig = plt.figure()\n",
    "plt_fig.text(.5, .2, txt, ha='center', transform=plt_ax1.transAxes)\n",
    "plt.pause(0.001)\n",
    "\n",
    "plot_data = []\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "################## Load trained model ###############\n",
    "from pathlib import Path\n",
    "modelimagefile = \"rnn3model.\" + Imagesuffix + \".pt\"\n",
    "if Path(modelimagefile).is_file():\n",
    "    # file exists\n",
    "    model.load_state_dict(torch.load(modelimagefile))\n",
    "    model.eval()\n",
    "    print(\"=== Model was loaded from \" + modelimagefile)\n",
    "\n",
    "####################################################\n",
    "Criterion = torch.nn.CrossEntropyLoss()\n",
    "Optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "# X is OH encoded\n",
    "# Y is simple encoded\n",
    "# this is the format needed by pytorch\n",
    "Inputs = Variable(torch.Tensor(X_ohe))\n",
    "Labels = Variable(torch.LongTensor(Y_data))\n",
    "\n",
    "print(\"Input shape: \", Inputs.size())\n",
    "print(\"Output shape: \", Labels.size())\n",
    "\n",
    "final_train_accuracy, final_train_epoch = train_phase()\n",
    "\n",
    "############ final plotting and logging ################\n",
    "#plt.plot(plot_data)\n",
    "#plt.waitforbuttonpress()\n",
    "plt.savefig('rnn3-ttt.png',  bbox_inches='tight')\n",
    "plt.show()\n",
    "########################################################\n",
    "\n",
    "val_accuracy = validation_phase()\n",
    "test_accuracy = test_phase()\n",
    "\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"=== rnn_type:\", Rnn_type,\n",
    "        \"num_layers:\", num_layers,\n",
    "        \"num_io_samples (train): \", Num_io_data,\n",
    "        \"seq_max_len:\", Seq_max)\n",
    "\n",
    "print(\"\\n Training end due to: \", Train_end_reason)\n",
    "print(\"\\n=== final_train_epoch:\", final_train_epoch)\n",
    "print(\"=== final_train_accuracy:  %1.2f%%       \\n=== val_accuracy:  %1.2f%%        \\n=== test_accuracy:  %1.2f%%\\n\"\n",
    "        %  (final_train_accuracy, val_accuracy, test_accuracy))\n",
    "\n",
    "\n",
    "######################### save trained model #######################\n",
    "torch.save(model.state_dict(), modelimagefile)\n",
    "print(\"=== Model was saved as \" + modelimagefile)\n",
    "####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
