{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "antiviral_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yAb5ZeeIA4e0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddhackiisc/code/blob/master/antiviral_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAb5ZeeIA4e0",
        "colab_type": "text"
      },
      "source": [
        "##SimpleGAN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvVxFsCD9mT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwA9cE0NHuh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/preetham-v/antiviralGAN/master/AVPdb_data.csv'\n",
        "\n",
        "data = pd.read_csv(url, skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPnBUMVMJEK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "720e239a-2934-4ed8-c4d9-8e397b36c85a"
      },
      "source": [
        "all_sequences = np.asarray(data['seq'])\n",
        "print(all_sequences[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PYVGSGLYRR' 'SMIENLEYM' 'ECRSTSYAGAVVNDL' 'STSYAGAVVNDL' 'YAGAVVNDL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQinJOXdHrU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary of 20 canonical amino acids\n",
        "CHARACTER_DICT = set([\n",
        "    u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K',\n",
        "    u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W',\n",
        "    u'V', u'Y']\n",
        ")\n",
        "\n",
        "CHARACTER_TO_INDEX = {\n",
        "    character: i\n",
        "    for i, character in enumerate(CHARACTER_DICT)\n",
        "}\n",
        "\n",
        "INDEX_TO_CHARACTER = {\n",
        "    CHARACTER_TO_INDEX[c]: c\n",
        "    for c in CHARACTER_TO_INDEX\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqm5jJ_NG1pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 18 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
        "num_amino_acids = len(CHARACTER_DICT) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCHSqHfIOg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequence_to_vector(sequence, embed_dict=None):\n",
        "    if embed_dict==None:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(CHARACTER_TO_INDEX)+1])\n",
        "        default[:,len(CHARACTER_TO_INDEX)] = 1\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            default[i][CHARACTER_TO_INDEX[character]] = 1\n",
        "            default[i][len(CHARACTER_TO_INDEX)] = 0\n",
        "        return default\n",
        "    else:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])])\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            for k,val in enumerate(embed_dict[character]):\n",
        "                default[i,k] = val\n",
        "        return default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KFAejGUmdnQz",
        "colab": {}
      },
      "source": [
        "def vector_to_sequence(vector):\n",
        "    seq = ''\n",
        "\n",
        "    for i in range(18):\n",
        "        arg = np.argmax(vector[i])\n",
        "        if arg == 20:\n",
        "          seq += 'X'\n",
        "        else:\n",
        "          seq += INDEX_TO_CHARACTER[arg]\n",
        "    return seq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYPLrCaQnXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
        "        Args:\n",
        "            input_length (int array): max_length * number_of_characters \n",
        "                                      (\"noise vector\")\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "            output_activation: torch activation function or None\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_length, 40)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(40, 120)\n",
        "        self.linear3 = nn.Linear(120, 240)\n",
        "        self.linear4 = nn.Linear(240, 378)\n",
        "        self.output_activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        intermediate = self.linear1(input_tensor)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear2(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear3(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear4(intermediate)\n",
        "        intermediate = self.output_activation(intermediate)\n",
        "\n",
        "        view = intermediate.view(-1, 18)\n",
        "        (view == view.max(dim=1, keepdim=True)[0]).view_as(intermediate).int()\n",
        "        \n",
        "        return intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3i4Xoq9mqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, layers):\n",
        "        \"\"\"A discriminator for discerning real from generated samples.\n",
        "        params:\n",
        "            input_dim (int): width of the input\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "        Output activation is Sigmoid.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self._init_layers(layers)\n",
        "\n",
        "    def _init_layers(self, layers):\n",
        "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
        "        self.module_list = nn.ModuleList()\n",
        "        last_layer = self.input_dim\n",
        "        for index, width in enumerate(layers):\n",
        "            self.module_list.append(nn.Linear(last_layer, width))\n",
        "            last_layer = width\n",
        "            if index + 1 != len(layers):\n",
        "                self.module_list.append(nn.LeakyReLU())\n",
        "            else:\n",
        "                self.module_list.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
        "        intermediate = input_tensor\n",
        "        for layer in self.module_list:\n",
        "            intermediate = layer(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgni6k90HhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "19ef5423-da91-4e1f-e987-dfa640e69d9f"
      },
      "source": [
        "all_inputs = []\n",
        "for j in range(len(all_sequences)):\n",
        "  embedding = sequence_to_vector(all_sequences[j])\n",
        "  all_inputs.append(np.reshape(embedding,378))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2e9453131e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mall_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m378\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a9d860619a17>\u001b[0m in \u001b[0;36msequence_to_vector\u001b[0;34m(sequence, embed_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membed_dict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARACTER_TO_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARACTER_TO_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CHARACTER_TO_INDEX' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIaAh98vmoNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_function(batch_size, iteration):\n",
        "  input_array = all_inputs[batch_size*iteration : batch_size*(iteration+1)]\n",
        "  return input_array\n",
        "\n",
        "def noise_function(batch_size):\n",
        "  input_array = np.random.randn(batch_size,1,20)\n",
        "    # a = np.zeros([18,21])\n",
        "    # for i in range(18):\n",
        "    #   x = np.random.randint(21)\n",
        "    #   a[i][x] = 1\n",
        "    # input_array.append(np.reshape(a,378))\n",
        "  \n",
        "  return input_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphmNsOt-ikc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class VanillaGAN():\n",
        "    def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
        "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
        "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
        "        Args:\n",
        "            generator: a Ganerator network\n",
        "            discriminator: A Discriminator network\n",
        "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
        "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
        "            batch_size: training batch size\n",
        "            device: cpu or CUDA\n",
        "            lr_d: learning rate for the discriminator\n",
        "            lr_g: learning rate for the generator\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.generator = self.generator.to(device)\n",
        "        self.discriminator = discriminator\n",
        "        self.discriminator = self.discriminator.to(device)\n",
        "        self.noise_fn = noise_fn\n",
        "        self.data_fn = data_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optim_d = optim.Adam(discriminator.parameters(),\n",
        "                                  lr=lr_d, betas=(0.5, 0.999))\n",
        "        self.optim_g = optim.Adam(generator.parameters(),\n",
        "                                  lr=lr_g, betas=(0.5, 0.999))\n",
        "        self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
        "        self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "    def generate_samples(self, latent_vec=None, num=None):\n",
        "        \"\"\"Sample from the generator.\n",
        "        Args:\n",
        "            latent_vec: A pytorch latent vector or None\n",
        "            num: The number of samples to generate if latent_vec is None\n",
        "        If latent_vec and num are None then us self.batch_size random latent\n",
        "        vectors.\n",
        "        \"\"\"\n",
        "        num = self.batch_size if num is None else num\n",
        "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
        "        with torch.no_grad():\n",
        "            samples = self.generator(latent_vec)\n",
        "        return samples\n",
        "\n",
        "    def train_step_generator(self):\n",
        "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
        "        self.generator.zero_grad()\n",
        "\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        generated = self.generator(latent_vec)\n",
        "        classifications = self.discriminator(generated)\n",
        "        loss = self.criterion(classifications, self.target_ones)\n",
        "        loss.backward()\n",
        "        self.optim_g.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def train_step_discriminator(self):\n",
        "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
        "        self.discriminator.zero_grad()\n",
        "\n",
        "        # real samples\n",
        "        real_samples = self.data_fn(self.batch_size)\n",
        "        pred_real = self.discriminator(real_samples)\n",
        "        loss_real = self.criterion(pred_real, self.target_ones)\n",
        "\n",
        "        # generated samples\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            fake_samples = self.generator(latent_vec)\n",
        "        pred_fake = self.discriminator(fake_samples)\n",
        "        loss_fake = self.criterion(pred_fake, self.target_zeros)\n",
        "\n",
        "        # combine\n",
        "        loss = (loss_real + loss_fake) / 2\n",
        "        loss.backward()\n",
        "        self.optim_d.step()\n",
        "        return loss_real.item(), loss_fake.item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Train both networks and return the losses.\"\"\"\n",
        "        loss_d = self.train_step_discriminator()\n",
        "        loss_g = self.train_step_generator()\n",
        "        return loss_g, loss_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC7m2z-iXxGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simpleGAN(generator_func, discriminator_func, \n",
        "              batch_size: int = 25, epochs: int = 5, \n",
        "              max_data: int = len(all_sequences), print_every: int = 10, ):\n",
        "\n",
        "  #Array to monitor losses\n",
        "  loss_g = []\n",
        "  loss_dreal = []\n",
        "  loss_dfake = []\n",
        "  outputs = []\n",
        "  input_length = 20\n",
        "\n",
        "  # Models\n",
        "  generator = generator_func\n",
        "  discriminator = discriminator_func\n",
        "\n",
        "  # Optimizers\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), \n",
        "                                          lr=0.001)\n",
        "\n",
        "  # loss\n",
        "  loss = nn.BCELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    for j in range(int(max_data/batch_size)):\n",
        "\n",
        "      # zero the gradients on each iteration\n",
        "      generator_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      # Create noisy input for generator\n",
        "      # Need float type instead of int\n",
        "      noise = noise_function(batch_size)\n",
        "      noise_data = torch.tensor(noise).float()\n",
        "      generated_data = generator(noise_data)  \n",
        "\n",
        "      # Generate examples of real data\n",
        "      true_data = data_function(batch_size, j)\n",
        "      true_labels = torch.tensor(np.ones(batch_size)).float()\n",
        "      true_data = torch.tensor(true_data).float()\n",
        "\n",
        "      # Train the generator\n",
        "      # We invert the labels here and don't train the discriminator because we want the generator\n",
        "      # to make things the discriminator classifies as true.\n",
        "      generator_discriminator_out = discriminator(generated_data)\n",
        "      generator_loss = loss(generator_discriminator_out, true_labels)\n",
        "      generator_loss.backward()\n",
        "      generator_optimizer.step()\n",
        "\n",
        "      # Train the discriminator on the true/generated data\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      true_discriminator_out = discriminator(true_data)\n",
        "      true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
        "\n",
        "      # add .detach() here think about this\n",
        "      generator_discriminator_out = discriminator(generated_data.detach())\n",
        "      generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
        "      discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
        "      discriminator_loss.backward()\n",
        "      discriminator_optimizer.step()\n",
        "\n",
        "      loss_g.append(generator_loss.detach().numpy())\n",
        "      loss_dreal.append(true_discriminator_loss.detach().numpy())\n",
        "      loss_dfake.append(generator_discriminator_loss.detach().numpy())\n",
        "  \n",
        "\n",
        "    \n",
        "    generated_data = generated_data.detach().numpy()\n",
        "#      x = random.randint(0,len(generated_data))\n",
        "    sequences = set()\n",
        "    \n",
        "    for j in range(batch_size):\n",
        "      sequence = np.reshape(generated_data[j], [18,21])\n",
        "      sequences.add(vector_to_sequence(sequence))\n",
        "\n",
        "    seq_update = list(sequences)\n",
        "    outputs.append(seq_update)\n",
        "\n",
        "    if i % print_every == 0:\n",
        "\n",
        "      print(\"Currently at epoch: \" + str(i))\n",
        "\n",
        "  return loss_g, loss_dreal, loss_dfake, outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jUxoWZeFFy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "def main():\n",
        "    from time import time\n",
        "    epochs = 600\n",
        "    batches = 10\n",
        "    generator = Generator(360)\n",
        "    discriminator = Discriminator(1, [64, 32, 1])\n",
        "    noise_fn = noise_function()\n",
        "    data_fn = data_function()\n",
        "    gan = VanillaGAN(generator, discriminator, noise_fn, data_fn, device='cpu')\n",
        "    loss_g, loss_d_real, loss_d_fake = [], [], []\n",
        "    start = time()\n",
        "    for epoch in range(epochs):\n",
        "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
        "        for batch in range(batches):\n",
        "            lg_, (ldr_, ldf_) = gan.train_step()\n",
        "            loss_g_running += lg_\n",
        "            loss_d_real_running += ldr_\n",
        "            loss_d_fake_running += ldf_\n",
        "        loss_g.append(loss_g_running / batches)\n",
        "        loss_d_real.append(loss_d_real_running / batches)\n",
        "        loss_d_fake.append(loss_d_fake_running / batches)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
        "              f\" G={loss_g[-1]:.3f},\"\n",
        "              f\" Dr={loss_d_real[-1]:.3f},\"\n",
        "              f\" Df={loss_d_fake[-1]:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5ReVWzaZeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bdee817-08e5-4357-ee1b-36c0cf57527e"
      },
      "source": [
        "import time\n",
        "\n",
        "torch.manual_seed(1104)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "generator = Generator(20)\n",
        "discriminator = Discriminator(378, [32, 16, 1])  \n",
        "\n",
        "losses = simpleGAN(generator, discriminator, \n",
        "                   batch_size=2059, epochs=40000, print_every = 500)\n",
        "\n",
        "outputs = losses[3]\n",
        "\n",
        "print(\"Program took\", time.time() - start_time, \"to run\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2059])) that is different to the input size (torch.Size([2059, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2059])) that is different to the input size (torch.Size([2059, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Currently at epoch: 0\n",
            "Currently at epoch: 500\n",
            "Currently at epoch: 1000\n",
            "Currently at epoch: 1500\n",
            "Currently at epoch: 2000\n",
            "Currently at epoch: 2500\n",
            "Currently at epoch: 3000\n",
            "Currently at epoch: 3500\n",
            "Currently at epoch: 4000\n",
            "Currently at epoch: 4500\n",
            "Currently at epoch: 5000\n",
            "Currently at epoch: 5500\n",
            "Currently at epoch: 6000\n",
            "Currently at epoch: 6500\n",
            "Currently at epoch: 7000\n",
            "Currently at epoch: 7500\n",
            "Currently at epoch: 8000\n",
            "Currently at epoch: 8500\n",
            "Currently at epoch: 9000\n",
            "Currently at epoch: 9500\n",
            "Currently at epoch: 10000\n",
            "Currently at epoch: 10500\n",
            "Currently at epoch: 11000\n",
            "Currently at epoch: 11500\n",
            "Currently at epoch: 12000\n",
            "Currently at epoch: 12500\n",
            "Currently at epoch: 13000\n",
            "Currently at epoch: 13500\n",
            "Currently at epoch: 14000\n",
            "Currently at epoch: 14500\n",
            "Currently at epoch: 15000\n",
            "Currently at epoch: 15500\n",
            "Currently at epoch: 16000\n",
            "Currently at epoch: 16500\n",
            "Currently at epoch: 17000\n",
            "Currently at epoch: 17500\n",
            "Currently at epoch: 18000\n",
            "Currently at epoch: 18500\n",
            "Currently at epoch: 19000\n",
            "Currently at epoch: 19500\n",
            "Currently at epoch: 20000\n",
            "Currently at epoch: 20500\n",
            "Currently at epoch: 21000\n",
            "Currently at epoch: 21500\n",
            "Currently at epoch: 22000\n",
            "Currently at epoch: 22500\n",
            "Currently at epoch: 23000\n",
            "Currently at epoch: 23500\n",
            "Currently at epoch: 24000\n",
            "Currently at epoch: 24500\n",
            "Currently at epoch: 25000\n",
            "Currently at epoch: 25500\n",
            "Currently at epoch: 26000\n",
            "Currently at epoch: 26500\n",
            "Currently at epoch: 27000\n",
            "Currently at epoch: 27500\n",
            "Currently at epoch: 28000\n",
            "Currently at epoch: 28500\n",
            "Currently at epoch: 29000\n",
            "Currently at epoch: 29500\n",
            "Currently at epoch: 30000\n",
            "Currently at epoch: 30500\n",
            "Currently at epoch: 31000\n",
            "Currently at epoch: 31500\n",
            "Currently at epoch: 32000\n",
            "Currently at epoch: 32500\n",
            "Currently at epoch: 33000\n",
            "Currently at epoch: 33500\n",
            "Currently at epoch: 34000\n",
            "Currently at epoch: 34500\n",
            "Currently at epoch: 35000\n",
            "Currently at epoch: 35500\n",
            "Currently at epoch: 36000\n",
            "Currently at epoch: 36500\n",
            "Currently at epoch: 37000\n",
            "Currently at epoch: 37500\n",
            "Currently at epoch: 38000\n",
            "Currently at epoch: 38500\n",
            "Currently at epoch: 39000\n",
            "Currently at epoch: 39500\n",
            "Program took 9335.723873376846 to run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drm_MN96pPUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "433b5667-0ac0-4733-f22f-23ac946d630b"
      },
      "source": [
        "loss_g = losses[0]\n",
        "loss_dreal = losses[1]\n",
        "loss_dfake = losses[2]\n",
        "#plt.plot(loss_g, label = 'generator')\n",
        "#plt.plot(loss_dreal, label ='discriminator_Real')\n",
        "plt.plot(loss_dfake, label = 'discrimininator_Fake')\n",
        "plt.xlabel('Update')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff85fe71d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeY0lEQVR4nO3de3QV9b338fd3J5BwDeGi5aZg66li1QoB7cLHGz6CwAFU2trSipez6MVKPX36KPa4qnQtXerjDVsfPDyVgucg1tqC0apHpbTWY6smEhHBFuRiAxEQuQoISb7PHzMZN5CEZMPeE2Z/XmtlZW57z3fPTvZnz29mfmPujoiICEAq7gJERKTtUCiIiEhEoSAiIhGFgoiIRBQKIiISKYy7gCPRs2dPHzBgQNxliIgcUyorKz9y916NzTumQ2HAgAFUVFTEXYaIyDHFzNY1NU/NRyIiElEoiIhIRKEgIiKRY/qYQmP2799PdXU1e/fujbsUacOKi4vp168f7dq1i7sUkTYlcaFQXV1Nly5dGDBgAGYWdznSBrk7W7Zsobq6moEDB8ZdjkibkrXmIzObbWabzGxZ2rTuZvaSma0Mf5eG083MHjKzVWa21MwGZ7revXv30qNHDwWCNMnM6NGjh/YmRRqRzWMKc4BRB02bBixy95OBReE4wKXAyeHPFGDmkaxYgSCHo78RkcZlrfnI3V8xswEHTR4PXBAOzwX+CNwcTn/Mg368/2pm3cyst7vXZKs+kSPx/Ds1rKjZEXcZksdGnHo8Z/bvdtSfN9fHFI5P+6D/EDg+HO4L/CNtuepw2iGhYGZTCPYmOOGEE7JXqUgzblnwDtt270c7HBKX47oWJyIUIu7uZtbqO/y4+yxgFkBZWdkxcYeg22+/nc6dO7Njxw7OO+88Lr744iN6vtGjR/P444/TrVvL/iDKy8tZvnw506ZNa3KZDRs2MHXqVJ566qmManrwwQeZMmUKHTt2zOjx6S644AJqamro0KEDALfeeisTJ05sdNmGq9p79ux5xOttjbp655rhA7jtn0/L6XpFsi3XobCxoVnIzHoDm8Lp64H+acv1C6clys9+9rMjery74+4899xzrXrcuHHjGDduXLPL9OnTJ+NAgCAUvvWtb7UqFOrq6igoKGh03rx58ygrK8u4HhHJTK5DoRyYDNwV/n46bfoPzOwJ4Gxg+9E4njD9mXdZvuHotvsO6tO1Rd8O77jjDubOnctxxx1H//79GTJkCFdffTVjx45l4sSJTJs2jfLycgoLC7nkkku499572bhxI9/97ndZvXo1ADNnzqRPnz6MHDmSs88+m8rKSp577jnOP/98Kioq2LVrF6NGjeKcc87htddeY+jQoVxzzTXcdtttbNq0iXnz5jFs2DDmzJlDRUUFv/jFL7j66qvp2rUrFRUVfPjhh9xzzz1MnDiRtWvXMnbsWJYtW8acOXMoLy9n9+7dvP/++1x22WXcc889AHzve9/jzTffZM+ePUycOJHp06fz0EMPsWHDBi688EJ69uzJ4sWLmT9/PnfeeSfuzpgxY7j77rsB6Ny5M9/5znd4+eWXefjhhzn33HNbtN0bW2+6PXv2cPnll3P55ZfzzW9+kxtuuIFly5axf/9+br/9dsaPH9+at1kkb2UtFMxsPsFB5Z5mVg3cRhAGT5rZdcA64Gvh4s8Bo4FVwG7gmmzVlQuVlZU88cQTVFVVUVtby+DBgxkyZEg0f8uWLSxYsID33nsPM2Pbtm0ATJ06lfPPP58FCxZQV1fHrl272Lp1KytXrmTu3Lmcc845h6xr1apV/OY3v2H27NkMHTqUxx9/nFdffZXy8nLuvPNOFi5ceMhjampqePXVV3nvvfcYN25co00zVVVVLFmyhKKiIr74xS9yww030L9/f+644w66d+9OXV0dI0aMYOnSpUydOpX777+fxYsX07NnTzZs2MDNN99MZWUlpaWlXHLJJSxcuJAJEybwySefcPbZZ3Pfffc1uw0nTZoUNR8tWrSo0fWeccYZAOzatYsrr7ySq666iquuuoqf/OQnXHTRRcyePZtt27YxbNgwLr74Yjp16tTyN1EkT2Xz7KNvNDFrRCPLOnD90a4hrvbeP//5z1x22WVRU8rBTTclJSUUFxdz3XXXMXbsWMaOHQvAH/7wBx577DEACgoKKCkpYevWrZx44omNBgLAwIEDOf300wE47bTTGDFiBGbG6aefztq1axt9zIQJE0ilUgwaNIiNGzc2usyIESMoKSkBYNCgQaxbt47+/fvz5JNPMmvWLGpra6mpqWH58uXRh3ODN998kwsuuIBevYKeeSdNmsQrr7zChAkTKCgo4IorrjjcJjyk+eiRRx5pcr3jx4/npptuYtKkSQC8+OKLlJeXc++99wLBtSsffPABp5566mHXK5Lv1PdRDAoLC3njjTeYOHEizz77LKNGHXw5x4Ga+4ZbVFQUDadSqWg8lUpRW1t72McEedz8MgUFBdTW1rJmzRruvfdeFi1axNKlSxkzZkyrLwArLi5u8jhCUw633uHDh/PCCy9Er8Xd+e1vf0tVVRVVVVXZCYRj4hQHkdZTKGTBeeedx8KFC9mzZw87d+7kmWeeOWD+rl272L59O6NHj+aBBx7g7bffBoJv5zNnBtft1dXVsX379pzX3pwdO3bQqVMnSkpK2LhxI88//3w0r0uXLuzcuROAYcOG8ac//YmPPvqIuro65s+fz/nnn5+V9UJwAL+0tJTrrw92NkeOHMnPf/7zKCSWLFmS8bqbY+h8VEmexPV91BYMHjyYr3/965x55pkcd9xxDB069ID5O3fuZPz48ezduxd35/777wdgxowZTJkyhUcffZSCggJmzpxJ796943gJjTrzzDM566yzOOWUU+jfvz/Dhw+P5k2ZMoVRo0bRp08fFi9ezF133cWFF14YHWg+kgO9za23wYwZM7j22mu56aabmD59OjfeeCNnnHEG9fX1DBw4kGeffTbj9YvkE2uq+eBYUFZW5gffeW3FihVqO5YWOZK/ldNv+y++Wtafn/7zoKNclUj2mVmluzd6zreaj0REJKLmI4nNZZddxpo1aw6YdvfddzNy5MiYKmq5Y3f/WqR5iQwFd1cvmMeABQsWxLbuY7nZVCSbEtd8VFxczJYtW/RPL01quMlOcXHxET2PvndIEiVuT6Ffv35UV1ezefPmuEuRNqzhdpwicqDEhUK7du10i0URkQwlrvlIREQyp1AQyYCOWUlSKRRERCSiUBDJkE4+kiRSKIiISEShICIiEYWCiIhEFAoiGdC5R5JUCgUREYkoFEQypL6PJIkUCiIiElEoiIhIRKEgIiIRhYJIBtT1kSSVQkFERCIKBZEM6ZavkkQKBRERiSgUREQkolAQEZGIQkEkA67ejyShFAoiGdJhZkmiWELBzP7VzN41s2VmNt/Mis1soJm9bmarzOzXZtY+jtpERPJZzkPBzPoCU4Eyd/8SUABcCdwNPODuXwC2AtflujYRkXwXV/NRIdDBzAqBjkANcBHwVDh/LjAhptpERPJWzkPB3dcD9wIfEITBdqAS2ObuteFi1UDfxh5vZlPMrMLMKjZv3pyLkkVE8kYczUelwHhgINAH6ASMaunj3X2Wu5e5e1mvXr2yVKVI89T3kSRVHM1HFwNr3H2zu+8HfgcMB7qFzUkA/YD1MdQm0nI6/UgSKI5Q+AA4x8w6WtB5zAhgObAYmBguMxl4OobaRETyWhzHFF4nOKD8FvBOWMMs4GbgR2a2CugBPJrr2kRE8l3h4Rc5+tz9NuC2gyavBobFUI6IiIR0RbNIBnScWZJKoSAiIhGFgkiGTKcfSQIpFEREJKJQEBGRiEJBREQiCgWRTOj0I0kohYKIiEQUCiIZMp18JAmkUBARkYhCQUREIgoFERGJKBREMuA6/UgSSqEgIiIRhYJIhnTykSSRQkFERCIKBRERiSgUREQkolAQyYDr5CNJKIWCSIbUzYUkkUJBREQiCgUREYkoFEREJKJQEBGRiEJBJAM6+UiSSqEgkiFTRxeSQAoFERGJKBRERCSiUBARkYhCQUREIrGEgpl1M7OnzOw9M1thZl8xs+5m9pKZrQx/l8ZRm0hLuDo/koSKa09hBvCCu58CnAmsAKYBi9z9ZGBROC7SZqnvI0minIeCmZUA5wGPArj7PnffBowH5oaLzQUm5Lo2EZF8F8eewkBgM/ArM1tiZr80s07A8e5eEy7zIXB8Yw82sylmVmFmFZs3b85RySIi+SGOUCgEBgMz3f0s4BMOairyoMG20UZbd5/l7mXuXtarV6+sFysikk/iCIVqoNrdXw/HnyIIiY1m1hsg/L0phtpEWkSHmSWpch4K7v4h8A8z+2I4aQSwHCgHJofTJgNP57o2EZF8VxjTem8A5plZe2A1cA1BQD1pZtcB64CvxVSbSIvo5CNJolhCwd2rgLJGZo3IdS0iIvIZXdEsIiIRhYKIiEQUCiIZUC8XklQKBRERiSgURDKlzo8kgRQKIiISaVEomFknM0uFw/9kZuPMrF12SxMRkVxr6Z7CK0CxmfUFXgS+DczJVlEiIhKPloaCuftu4HLg/7r7V4HTsleWiIjEocWhYGZfASYBvw+nFWSnJJFjgw4zSxK1NBRuBG4BFrj7u2Z2ErA4e2WJiEgcWtT3kbv/CfgTQHjA+SN3n5rNwkREJPdaevbR42bWNbxD2jJguZn97+yWJiIiudbS5qNB7r6D4L7JzxPcUvPbWatKRERi0dJQaBdelzABKHf3/ejmU5KnXB0fSYK1NBT+HVgLdAJeMbMTgR3ZKkrkWKBeLiSJWnqg+SHgobRJ68zswuyUJCIicWnpgeYSM7vfzCrCn/sI9hpERCRBWtp8NBvYSXDf5K8RNB39KltFiYhIPFp6j+bPu/sVaePTzawqGwWJiEh8WrqnsMfMzm0YMbPhwJ7slCTStunkI0mylu4pfBd4zMxKwvGtwOTslCRybDD1fiQJ1NKzj94GzjSzruH4DjO7EViazeJERCS3WnXnNXffEV7ZDPCjLNQjIiIxOpLbcWrfWUQkYY4kFHS4TUQkYZo9pmBmO2n8w9+ADlmpSKSN07chSbJmQ8Hdu+SqEJFjjfo+kiQ6kuYjERFJGIWCiIhEFAoiIhKJLRTMrMDMlpjZs+H4QDN73cxWmdmvzax9XLWJNEc32ZEki3NP4YfAirTxu4EH3P0LBN1oXBdLVSIieSyWUDCzfsAY4JfhuAEXAU+Fi8wluPWnSJulk48kieLaU3gQuAmoD8d7ANvcvTYcrwb6NvZAM5vScLOfzZs3Z79SEZE8kvNQMLOxwCZ3r8zk8e4+y93L3L2sV69eR7k6EZH81tKus4+m4cA4MxsNFANdgRlANzMrDPcW+gHrY6hNRCSv5XxPwd1vcfd+7j4AuBL4g7tPAhYDE8PFJgNP57o2kZbQuUeSZG3pOoWbgR+Z2SqCYwyPxlyPSLPUzYUkURzNRxF3/yPwx3B4NTAsznpERPJdW9pTEBGRmCkUREQkolAQEZGIQkGkldT1kSSZQkEkQ6bTjySBFAoiIhJRKIiISEShICIiEYWCiIhEFAoireTq/UgSTKEgIiIRhYKIiEQUCiIiElEoiIhIRKEgIiIRhYJIK6nvI0kyhYJIhtT1kSSRQkFERCIKBRERiSgUREQkolAQEZGIQkFERCIKBZEMGTr9SJJHoSAiIhGFgoiIRBQKIiISUSiItJK6uZAkUyiIZEjdXEgSKRRERCSiUBARkUjOQ8HM+pvZYjNbbmbvmtkPw+ndzewlM1sZ/i7NdW0iIvkujj2FWuB/ufsg4BzgejMbBEwDFrn7ycCicFxERHIo56Hg7jXu/lY4vBNYAfQFxgNzw8XmAhNyXZtISzg6/UiSK9ZjCmY2ADgLeB043t1rwlkfAsc38ZgpZlZhZhWbN2/OSZ0ijdHJR5JEsYWCmXUGfgvc6O470ue5u0PjX8fcfZa7l7l7Wa9evXJQqYhI/oglFMysHUEgzHP334WTN5pZ73B+b2BTHLWJiOSzOM4+MuBRYIW73582qxyYHA5PBp7OdW0iIvmuMIZ1Dge+DbxjZlXhtJ8AdwFPmtl1wDrgazHUJiKS13IeCu7+Kk0foxuRy1pEMqG+jyTJdEWzSIbU95EkkUJBREQiCgUREYkoFEREJKJQEBGRiEJBpJV08pEkmUJBJEOm3o8kgRQKIiISUSiIiEhEoSDSSq5LmiXBFAoiGdIVzZJECgUREYkoFERaSY1HkmQKBRERiSgUREQkolAQEZGIQkGklXRGqiSZQkEkQ6ZzUiWBFAp5asbLK3lh2YdxlyEibUzO79EsbcMDL/8dgLV3jYm5EhFpS7SnINJaOqYgCaZQEMmQjihIEuVlKHxaW0fluo/jLkOyaM++OvbV1sddhsgxJy9DYfozy7li5l94f/OuuEuRLDn1py/wT7c+n5XndrUfSYLlZSisqNkBwLbd+2OuRI5lOiNVkigvQ0EXH4mINC4vQ6GBvuklU329Ul8kU3kdCpJMdVneFdSepiSZQkESpy5Hewra0ZQkystQ0Be9ZKvNcig0PLv6PpIkystQkNbZvPNTnq5a3+S8/XVt63qAurpsNx/pa4UkV5vq+8jMRgEzgALgl+5+VzbXN/e1tdE/+MIlG+hUVMjFpx6HmZEyKEgZKTNq6526eg/HwTDq3UmZYRYcsC5Mpdi+Zz9FhaloevCaguUhuGgOgmUb5qd/vqSf/37g9LThcMbufXW0L0xRXFjAhu176F1STOqgb67F7Qro0L6AD7fvoTCVYn9dPfvrnNr6zz7EvzHrr/xl9Ra6dWzHRaccx+/eCj78j+9aRGnH9lxy2ud4aNFKAJ6qrKZ3STHrt+3h8706U1fvzHv9AwAe/uZgunVsR+eiQlJm7K2to31BitKO7dm0cy9mUJBK0bmogM5F7Vi1aRdmQY3BNgq+edtB2yz9Jc3+7zWcdUIpp36uC+0LUxSmUhSkLPopDH+/9cHW6DHPv1ND5+JCOrYvAIL3JXXQehrWUe/Ojj21lHZqh2F8/Mk+OhYV8Maaj+nbrQN9uhVTkEqxa2/tIbWJJIW1lW89ZlYA/B34n0A18CbwDXdf3tRjysrKvKKiotXrGjDt95mWKRK576tncsWQfnGXIdJqZlbp7mWNzWtLewrDgFXuvhrAzJ4AxgNNhkKmzv1CT15d9REAj3xrCB3aFzB59hsAPHbtMJzgtMa6+uBbdfvCoJWtYQ+h3iFl0NB0Xe9ObZ3TvjAVTXcPvvcHmeu4Q2FBisKUHdLmnf6FM/3b5wHDHDDC+5t20bmokE5Fhby4fCNjTu9NQcqiPYl6D/ZMPvm0jk9r6zihe0cKC1K0KzDaFaT46iN/yWjbNby+0o7t6NWliL9vDK4Kf27q/2DH3v188mktr6/5mH6lHejbrQMbd3zK/Dc+4BvDTqB3STE7P61lx5793LpwGQC/umZo8MQe7Cm5f7aXFGw/j4a/8x+VQPAe7autj/bg6typq6+nrh7q6uspf3sD/71qCwCXfulzXHvuQHbvq4veEzx4z9wPXMe23fv54OPdfKlvCX95/yPm/mVd9H5NOe8kzv1Cz/BvwulcVEjZgNKMtqFIW9aW9hQmAqPc/V/C8W8DZ7v7Dw5abgowBeCEE04Ysm7dulava9vufcx5bS03XHQyBangw7bqH9so6dCOgT07HeErOTY88/YGVm7cydXDB9K+MMW+2nqWVm9j8ImldG5fSCp1+LaRvfvruL38XaZdegrdOrZv1fq379mPu7f6cS01YNrv+c75J3HLpadm/Bwff7KP7p2yU59InJrbUzjmQiFdps1HIiL5rLlQaEtnH60H+qeN9wuniYhIjrSlUHgTONnMBppZe+BKoDzmmkRE8kqbOdDs7rVm9gPgvwhOSZ3t7u/GXJaISF5pM6EA4O7PAc/FXYeISL5qS81HIiISM4WCiIhEFAoiIhJRKIiISKTNXLyWCTPbDLT+kuZAT+Cjo1jO0aK6Wkd1tV5brU11tc6R1HWiu/dqbMYxHQpHwswqmrqiL06qq3VUV+u11dpUV+tkqy41H4mISEShICIikXwOhVlxF9AE1dU6qqv12mptqqt1slJX3h5TEBGRQ+XznoKIiBxEoSAiIpG8DAUzG2VmfzOzVWY2LUfrXGtm75hZlZlVhNO6m9lLZrYy/F0aTjczeyisb6mZDU57nsnh8ivNbHIGdcw2s01mtixt2lGrw8yGhK9zVfjYFt3evom6bjez9eE2qzKz0WnzbgnX8TczG5k2vdH3NuyS/fVw+q/D7tlbUld/M1tsZsvN7F0z+2Fb2GbN1BXrNjOzYjN7w8zeDuua3txzmVlROL4qnD8g03ozrGuOma1J215fDqfn8m+/wMyWmNmzbWFbBfetzaMfgm653wdOAtoDbwODcrDetUDPg6bdA0wLh6cBd4fDo4HnCW7ffA7weji9O7A6/F0aDpe2so7zgMHAsmzUAbwRLmvhYy89grpuB37cyLKDwvetCBgYvp8Fzb23wJPAleHwI8D3WlhXb2BwONwF+Hu4/li3WTN1xbrNwtfQORxuB7wevrZGnwv4PvBIOHwl8OtM682wrjnAxEaWz+Xf/o+Ax4Fnm9vuudpW+binMAxY5e6r3X0f8AQwPqZaxgNzw+G5wIS06Y954K9ANzPrDYwEXnL3j919K/ASMKo1K3T3V4CPs1FHOK+ru//Vg7/Wx9KeK5O6mjIeeMLdP3X3NcAqgve10fc2/MZ2EfBUI6/xcHXVuPtb4fBOYAXQl5i3WTN1NSUn2yx83bvC0XbhjzfzXOnb8SlgRLjuVtV7BHU1JSfvo5n1A8YAvwzHm9vuOdlW+RgKfYF/pI1X0/w/09HiwItmVmlmU8Jpx7t7TTj8IXD8YWrMVu1Hq46+4fDRrO8H4e77bAubaDKoqwewzd1rj6SucHf9LIJvmW1mmx1UF8S8zcLmkCpgE8GH5vvNPFe0/nD+9nDdR/1/4OC63L1he90Rbq8HzKzo4LpauP5M38cHgZuA+nC8ue2ek22Vj6EQl3PdfTBwKXC9mZ2XPjP8dhH7+cFtpY7QTODzwJeBGuC+uAoxs87Ab4Eb3X1H+rw4t1kjdcW+zdy9zt2/THCf9WHAKbmuoTEH12VmXwJuIahvKEGT0M25qsfMxgKb3L0yV+tsiXwMhfVA/7TxfuG0rHL39eHvTcACgn+WjeFuJ+HvTYepMVu1H6061ofDR6U+d98Y/iPXA/+PYJtlUtcWgt3/woOmt4iZtSP44J3n7r8LJ8e+zRqrq61ss7CWbcBi4CvNPFe0/nB+SbjurP0PpNU1KmyGc3f/FPgVmW+vTN7H4cA4M1tL0LRzETCDuLfV4Q46JO2H4BakqwkOyDQcfDkty+vsBHRJG36N4FjA/+HAg5X3hMNjOPAg1xv+2UGuNQQHuErD4e4Z1DOAAw/oHrU6OPRg2+gjqKt32vC/ErSbApzGgQfWVhMcVGvyvQV+w4EH777fwpqMoH34wYOmx7rNmqkr1m0G9AK6hcMdgD8DY5t6LuB6Djx4+mSm9WZYV++07fkgcFdMf/sX8NmB5ni3VWs/UJLwQ3Bmwd8J2jr/LQfrOyl8Q94G3m1YJ0F74CJgJfBy2h+XAQ+H9b0DlKU917UEB5JWAddkUMt8gmaF/QRtjNcdzTqAMmBZ+JhfEF41n2Fd/xGudylQzoEfeP8WruNvpJ3l0dR7G74Hb4T1/gYoamFd5xI0DS0FqsKf0XFvs2bqinWbAWcAS8L1LwN+2txzAcXh+Kpw/kmZ1pthXX8It9cy4D/57AylnP3th4+9gM9CIdZtpW4uREQkko/HFEREpAkKBRERiSgUREQkolAQEZGIQkFERCIKBZGDmNkAS+utNZx2u5n9uBXPsdbMeh5mmZ9kWqNItigUROKjUJA2R6Eg0gpm9kczmxH2vb/MzIaF03uY2YthX/2/JLj4qeExC8OOEN9t6AzRzO4COoTPMy+c9q2wz/8qM/t3MyuI4zVKflMoiLReRw86Vvs+MDucdhvwqrufRtC31Qlpy1/r7kMIrnidamY93H0asMfdv+zuk8zsVODrwPDwueuASbl6QSINCg+/iEjeaeoy/4bp8yG4B4SZdTWzbgQ3Cbo8nP57M9ua9ripZnZZONwfOJmgI7N0I4AhwJvhDbs68FkneyI5o1AQOdQWgs7O0jV0hAaHhkaTfcWY2QXAxcBX3H23mf2RoA+bQxYF5rr7LZkULHK0qPlI5CAe3KGrxswuguB+zAS92r4aLvL1cPq5wHZ33w68AnwznH4pn4VKCbA1DIRTCHrRbLA/7P4ags71JprZcQ3rNLMTs/UaRZqiPQWRxl0FPGxm94fj0939/bBpZ6+ZLSG4peO1DfOB+Wb2LkHX6B+E018AvmtmKwh6sPxr2jpmAUvN7K3wuMKtBHfnSxH0Fns9sC57L1HkUOolVaQVwuafH7t7Rdy1iGSDmo9ERCSiPQUREYloT0FERCIKBRERiSgUREQkolAQEZGIQkFERCL/HyqYPU10Rkm2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEjvvIz6kBNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "389c09c9-5b6a-4c19-be88-848033f67a7d"
      },
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c5b7e396cfc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlsIG8LP8rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('outputs.txt', 'w') as f:\n",
        "    for item in outputs:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmh3YVl-dUdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f770d220-dbc5-46c1-dbb5-45f9b4abdd68"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('outputs.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eef95cb7-9641-437f-9485-af1408629683\", \"outputs.txt\", 7089768)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7BrMAa4zcC7",
        "colab_type": "text"
      },
      "source": [
        "##NEW CODE - SeqGAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWs2msGddLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All dependencies \n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pdb\n",
        "import math\n",
        "import torch.nn.init as init\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grt5UNBW6Chv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download and extract Antiviral Sequences from AVPdb\n",
        "\n",
        "MAX_SEQ_LEN = 18 #2000 kDa / 110 kDa = 18\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/preetham-v/antiviralGAN/master/AVPdb_data.csv'\n",
        "\n",
        "data = pd.read_csv(url, skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])\n",
        "\n",
        "all_sequences = np.asarray(data['seq'])\n",
        "\n",
        "#Two way dictionary of 20 canonical amino acids (not efficient, but helpful!)\n",
        "CHARACTER_DICT = {\n",
        "    'A': 1, 'C': 2, 'E': 3, 'D': 4, 'F': 5, 'I': 6, 'H': 7, \n",
        "    'K': 8, 'M': 9, 'L': 10, 'N': 11, 'Q': 12, 'P': 13, 'S': 14, \n",
        "    'R': 15, 'T': 16, 'W': 17, 'V': 18, 'Y': 19, 'G': 20}\n",
        "\n",
        "INDEX_DICT = {\n",
        "    1: 'A', 2: 'C', 3: 'E', 4: 'D', 5: 'F', 6: 'I', 7: 'H',\n",
        "    8: 'K', 9: 'M', 10: 'L', 11: 'N', 12: 'Q', 13: 'P', 14: 'S',\n",
        "    15: 'R', 16: 'T', 17: 'W', 18: 'V', 19: 'Y', 20: 'G', 21: 'X'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-ITgncO-SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "b5dcb34b-1e10-46bd-f4e5-04f3aec98797"
      },
      "source": [
        "#Checks for the distribution of sequences in Dataset\n",
        "\n",
        "count = np.zeros(100)\n",
        "\n",
        "for seq in all_sequences:\n",
        "  count[len(seq)] += 1\n",
        "\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Number of residues')\n",
        "plt.ylabel('Number of sequences')\n",
        "plt.bar(np.arange(100), count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 100 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe9ElEQVR4nO3debgcVbnv8e+PSQaBAIkRkkAAUQ84BIyAooh4VAYP4XAF4aogIoF7iaKi14AiOKBxAJULB2WSoAhykSEaZJDDIHoZkoDMHDAGCIQkCJgAMoS854+1dlHZ7KH2Tlf33t2/z/Psp7tWVa96qyvpt2rVqlWKCMzMzABWaXUAZmY2dDgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzFpA0i6S5je4zmMkndnA+p6RtEV+f46kbzew7p9KOrZR9VnjOCl0KEnvkfRnSf+Q9KSkP0l6Z6vjagZJIekNw2mdkq6T9LykpZKWSJotaaqk13QtExHfiYjPVKyr3+Ui4rURMXewMZfW9ylJN3ar+/CI+NbK1m2N56TQgSStB/wO+L/AhsAY4BvAC62My/o1JSLWBTYGjgL2By6XpEauRNJqjazPhhcnhc70RoCIOD8iXo6If0bEVRFxR9cCkj4t6V5JT0m6UtJmpXkflHRfPss4RdL1XUeeko6X9MvSsuPzUfJqeXp9SWdJWiDpUUnflrRqnvcpSTdK+mFe798k7V6qa0NJP5f0WJ5/aWneRyTdLunpfAb0toF+KZJek9f9sKSFuYljrTxvF0nzJR0laVGO/+DSZzeS9Nt8FH9r3q4b87wb8mJ/yU0yHyt9rsf6+hIRz0bEdcBewLuAPXNdxXcvaU1Jv5T09/yd3CpptKQTgPcCp+RYTsnLh6QjJD0APFAqK5/djJR0dT5bub7r30T3fZzLrpP0GUn/AvwUeFde39N5/grNUZIOlfRgPmudIWmT0ryQdLikB/K2nNroRGivcFLoTP8FvCxpuqTdJW1QnilpEnAMsA8wCvgjcH6eNxK4GPgaMBL4K7DTANZ9DrAMeAOwLfAhoNyUsQNwf677+8BZpR+AXwBrA9sArwN+lGPaFjgbOAzYCPgZMKPctFLRNFLCnJDjGwN8vTT/9cD6ufwQ4NTSd3cq8Gxe5qD8B0BE7Jzfvj03yfy6Qn39ioiHgVmkH/nuDsp1jyN9J4cD/4yIr5L255Qcy5TSZ/Ymff9b97LKjwPfIu2b24HzKsR4b173/8/rG9F9GUm7At8F9iOdBT0EXNBtsY8A7wTelpf7cH/rtsFxUuhAEbEEeA8QwBnA4nx0Njovcjjw3Yi4NyKWAd8BJuQjwz2AuyPiooh4Cfgx8HiV9eb69wA+n492F5F+2PcvLfZQRJwRES8D00k/EqMlbQzsDhweEU9FxEsRcX3+zGTgZxFxcz7zmU5qCtux6neSE89k4AsR8WRELM3bXY7tJeCbed2XA88Ab8pnOv8DOC4inouIe3Ls/emxvqoxZ4+RmgB7qnsj4A35O5md93tfvpu3/Z+9zJ8ZETdExAvAV0lH/+MGGG9PPg6cHRFzct1H57rHl5aZFhFP50R4LSlxWw2cFDpU/sH/VESMBd4CbEL6gQfYDPhJPlV/GngSEOmIdhPgkVI9UZ7ux2bA6sCCUt0/Ix31dykSTEQ8l9++lnTE+2REPNVLvUd11ZnrHZdjrWoU6SxkdqmOK3J5l7/nJNnluRzbKGA1VvweqnwnvdU3EGNI+6e7XwBXAhfk5rbvS1q9n7r6i7m835/J6x3Id9ybTUhnB+W6/07ati7lA4/BfE9WkZOCERH3kZp13pKLHgEOi4gRpb+1IuLPwALSDy5QHGGXjxafJf24dnl96f0jpCP4kaV614uIbSqE+QiwoaRXNT/keSd0i3ftiDi/Qr1dngD+CWxTqmP9iKjy47OY1CQ2tlTWiCPoPuWj9HeQmoNWkM8+vhERWwPvJjW/HNg1u5cq+xsyubzfX0s6Q3mMtM+h9/3eX72PkRJ7V93rkM5yHu3nc1YDJ4UOJOnN+QLn2Dw9DjgAuCkv8lPgaEnb5PnrS9o3z5sJbCNpn3xh8XOs+ANwO7CzpE0lrU9qCgAgIhYAVwEnSlpP0iqStpT0vv5izp/9PfAfkjaQtLqkrrb6M4DDJe2gZB1Je0pat48q18gXY9eUtCbpTOgM4EeSXpe3e4ykftuuc1PXxcDxktaW9GZe+QHushDYor+6qsjreB9wGXALcHkPy7xf0ltz09YSUnPS8pWMZQ+lrsxrkK4t3BQRj0TEYtIP+CckrSrp08CWpc8tBMbmz/XkfOBgSRPydaDvADdHxLxBxGgryUmhMy0lXVC8WdKzpGRwF6mbIxFxCfA9UtPDkjxv9zzvCWBf0kXZvwNbAX/qqjgirgZ+DdwBzCZ1fS07EFgDuAd4CriIdN2gik+SftzuAxYBn8/rnAUcCpyS63wQ+FQ/dd1NOjPo+jsY+Er+7E15u/9A9Tb+KaQLu4+Tmm7OZ8UuvscD03PT1H4V6+zuFElLST+yPwZ+A+wWEct7WPb1pO92CXAvcH2OC+AnwEeVenCdPID1/wo4jtRs9A7gE6V5hwJfJv2b2Ab4c2nef5K+78clPdG90oj4A3Bs3p4FpISyf/flrDnkh+zYypJ0HfDLiGjY3bTDnaTvAa+PiIP6XdhsCPGZglkD5Ca5t+Xmq+1JXUwvaXVcZgPlOxfNGmNdUpPRJqTmnRNJbf5mw4qbj8zMrODmIzMzKwzr5qORI0fG+PHjWx2GmdmwMnv27CciYlRP84Z1Uhg/fjyzZs1qdRhmZsOKpId6m1db85GkcZKulXSPpLslHZnLj1caHfP2/LdH6TNHK42UeH+Vm4bMzKyx6jxTWAYcFRFz8p2lsyVdnef9KCJ+WF5Y0takG1a2IfXg+IOkN+a7Rc3MrAlqO1OIiAURMSe/X0q6q3JMHx+ZBFwQES9ExN9Id5ZuX1d8Zmb2ak3pfZSHwN0WuDkXTZF0h6SzS+PHj2HFURrn03cSMTOzBqs9KeTRFH9DGkN/CXAaaWyTCaRxTk4cYH2TJc2SNGvx4sUNj9fMrJPVmhTy+O2/Ac6LiIsBImJhfujHctKolF1NRI+y4nDDY+lh6NyIOD0iJkbExFGjeuxRZWZmg1Rn7yMBZwH3RsRJpfLyiJj/ThqBE2AGsL/Sc3I3J42+eUtd8ZmZ2avV2ftoJ9JQx3dKuj2XHQMcIGkC6cEb80jP1SUi7pZ0IWlI5WXAEe55ZGbWXLUlhYi4kfTgku5e9UCQ0mdOAE6oKyYzM+ubxz5qkvFTZzJ+6sxWh2Fm1icnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCnWOkmo1KI+fNG/ani2MxMzakc8UzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKtSUFSeMkXSvpHkl3Szoyl28o6WpJD+TXDXK5JJ0s6UFJd0jarq7YzMysZ3WeKSwDjoqIrYEdgSMkbQ1MBa6JiK2Aa/I0wO7AVvlvMnBajbGZmVkPaksKEbEgIubk90uBe4ExwCRgel5sOrB3fj8JODeSm4ARkjauKz4zM3u1plxTkDQe2Ba4GRgdEQvyrMeB0fn9GOCR0sfm57LudU2WNEvSrMWLF9cWs5lZJ6o9KUh6LfAb4PMRsaQ8LyICiIHUFxGnR8TEiJg4atSoBkZqZma1JgVJq5MSwnkRcXEuXtjVLJRfF+XyR4FxpY+PzWVmZtYkdfY+EnAWcG9EnFSaNQM4KL8/CLisVH5g7oW0I/CPUjOTmZk1wWo11r0T8EngTkm357JjgGnAhZIOAR4C9svzLgf2AB4EngMOrjE2MzPrQW1JISJuBNTL7A/0sHwAR9QVj5mZ9c93NJuZWcFJwczMCk4KZmZW6DcpSDpS0nq5V9BZkuZI+lAzgjMzs+aqcqbw6XzT2YeADUg9iqbVGpWZmbVElaTQ1YNoD+AXEXE3vfcqMjOzYaxKUpgt6SpSUrhS0rrA8nrDMjOzVqhyn8IhwARgbkQ8J2kjfGOZmVlbqnKmEMDWwOfy9DrAmrVFZGZmLVMlKfwH8C7ggDy9FDi1tojMzKxlqjQf7RAR20m6DSAinpK0Rs1xmZlZC1Q5U3hJ0qrk5x5IGoUvNJuZtaUqSeFk4BLgdZJOAG4EvlNrVGZm1hL9Nh9FxHmSZpNGNhWwd0TcW3tkZtn4qTOL9/Om7dnCSMzaX79JIT/w5u6IODVPrydph4i4ufbozMysqao0H50GPFOafiaXmZlZm6k0zEV+AA4AEbGcep/YZmZmLVIlKcyV9DlJq+e/I4G5dQdmZmbNVyUpHA68G3gUmA/sAEyuMygzM2uNKr2PFgH7NyEWMzNrsSq9j0YBhwLjy8tHxKfrC8vcDdPMWqHKBePLgD8CfwBerjccMzNrpSpJYe2I+ErtkZiZWctVudD8O0l71B6JmZm1XJWkcCQpMTwvaYmkpZKW1B2YmZk1X5XeR+s2IxAzM2u9fs8UlHxC0rF5epyk7esPzczMmm0gT177n3n6GfzkNTOztuQnr5mZWcFPXjMzs4KfvGZmZgU/ec3MzApVxj7aFHgO+G25LCIerjMwMzNrvirNRzOB3+XXa0jPUvh9fx+SdLakRZLuKpUdL+lRSbfnvz1K846W9KCk+yV9eOCbYmZmK6tK89Fby9OStgP+d4W6zwFOAc7tVv6jiPhhtzq3Jg3PvQ2wCfAHSW+MCA/AZ2bWRFXOFFYQEXNID9rpb7kbgCcrVjsJuCAiXoiIvwEPAr5BzsysyapcU/hiaXIVYDvgsZVY5xRJBwKzgKMi4ilgDHBTaZn5uayneCaTn/y26aabrkQYZmbWXZUzhXVLf68hXVuYNMj1nQZsCUwAFgAnDrSCiDg9IiZGxMRRo0YNMgwzM+tJlWsK32jUyiJiYdd7SWeQLmBDev7zuNKiY3OZmZk1UZXmo9+S72buSUTsVXVlkjaOiAV58t+Brp5JM4BfSTqJdKF5K+CWqvWamVljVBn7aC7weuCXefoAYCFwaV8fknQ+sAswUtJ84DhgF0kTSElmHnAYQETcLelC4B5gGXCEex6ZmTVflaSwU0RMLE3/VtKsiPhCXx+KiAN6KD6rj+VPAE6oEI+ZmdWkyoXmdSRt0TUhaXNgnfpCMjOzVqlypvAF4DpJc0ljH21GbvYxM7P2UqX30RWStgLenIvui4gX6g3LzMxaocrjONcGvgxMiYi/AJtK+kjtkZmZWdNVuabwc+BF0iM5Id0/8O3aIjIzs5apck1hy4j4mKQDACLiOUmqOS5bCeOnzizez5u2ZwsjMbPhpkpSeFHSWrzyOM4tAV9TGIacLMysP1WSwnHAFcA4SecBOwGfqjMoMzNrjSq9j66WNAfYkdQl9ciIeKL2yMzMrOmq9D7aCXg+ImYCI4BjJG1We2RmZtZ0VXofnQY8J+ntwBeBv/Lqp6mZmVkbqJIUlkVEkJ6hcGpEnEp6toKZmbWZKheal0o6GvgEsLOkVYDV6w3LzMxaocqZwsdIXVAPiYjHSQ/A+UGtUZmZWUtU6X30OHBSafphfE3BzKwtVTlTMDOzDuGkYGZmhV6TgqRr8uv3mheOmZm1Ul/XFDaW9G5gL0kXkO5mLkTEnFojMzOzpusrKXwdOJbU2+ikbvMC2LWuoMzMrDV6TQoRcRFwkaRjI+JbTYzJzMxapEqX1G9J2gvYORddFxG/qzcsMzNrhSoD4n0XOBK4J/8dKek7dQdmZmbNV2WYiz2BCRGxHEDSdOA24Jg6AzMzs+arep/CiNL79esIxMzMWq/KmcJ3gdskXUvqlrozMLXWqMzMrCWqXGg+X9J1wDtz0VfyeEhmZtZmqpwpEBELgBk1x2JmZi3msY/MzKzgpGBmZoU+k4KkVSXd16xgzMystfpMChHxMnC/pE2bFI+ZmbVQlQvNGwB3S7oFeLarMCL2qi0qMzNriSpJ4djaozAzsyGh3wvNEXE9MA9YPb+/Fej3WQqSzpa0SNJdpbINJV0t6YH8ukEul6STJT0o6Q5J2w16i8zMbNCqDIh3KHAR8LNcNAa4tELd5wC7dSubClwTEVsB1/DKndG7A1vlv8nAaRXqNzOzBqvSJfUIYCdgCUBEPAC8rr8PRcQNwJPdiicB0/P76cDepfJzI7kJGCFp4wqxmZlZA1VJCi9ExItdE5JWIz15bTBG57ujAR4HRuf3Y4BHSsvNz2WvImmypFmSZi1evHiQYZiZWU+qJIXrJR0DrCXpg8D/A367siuOiGAQySUiTo+IiRExcdSoUSsbhpmZlVRJClOBxcCdwGHA5cDXBrm+hV3NQvl1US5/FBhXWm5sLjMzsyaqMkrq8vxgnZtJR/b356P8wZgBHARMy6+XlcqnSLoA2AH4R6mZyczMmqTfpCBpT+CnwF9Jz1PYXNJhEfH7fj53PrALMFLSfOA4UjK4UNIhwEPAfnnxy4E9gAeB54CDB7U1Zma2UqrcvHYi8P6IeBBA0pbATKDPpBARB/Qy6wM9LBukXk5mZtZCVa4pLO1KCNlcYGlN8ZiZWQv1eqYgaZ/8dpaky4ELSdcU9iXd1WxmZm2mr+ajfyu9Xwi8L79fDKxVW0RmZtYyvSaFiPDFXjOzDlOl99HmwGeB8eXlPXS2mVn7qdL76FLgLNJdzMvrDcfMzFqpSlJ4PiJOrj0SMzNruSpJ4SeSjgOuAl7oKoyIfp+pYGZmw0uVpPBW4JPArrzSfBR52hpo/NSZAMybtmeLIzGzTlUlKewLbFEePtvMzNpTlTua7wJG1B2ImZm1XpUzhRHAfZJuZcVrCu6SambWZqokheNqj8LMzIaEKs9TuL4ZgZiZWetVuaN5Ka88NnMNYHXg2YhYr87AzMys+aqcKazb9V6SgEnAjnUGZWZmrVGl91EhkkuBD9cUj5mZtVCV5qN9SpOrABOB52uLyMzMWqZK76PycxWWAfNITUhmZtZmqlxT8HMVWqxr+Aszs7r19TjOr/fxuYiIb9UQj5mZtVBfZwrP9lC2DnAIsBHgpGBm1mb6ehzniV3vJa0LHAkcDFwAnNjb58zMbPjq85qCpA2BLwIfB6YD20XEU80IzMzMmq+vawo/APYBTgfeGhHPNC0qMzNrib5uXjsK2AT4GvCYpCX5b6mkJc0Jz8zMmqmvawoDutvZzMyGP//wm5lZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFaqMkmrDgAfNM7NGaElSkDQPWAq8DCyLiIn57ulfA+NJw3PvN9zvnu76oZ43bc8WR2JmVk0rm4/eHxETImJinp4KXBMRWwHX5GkzM2uioXRNYRJpfCXy694tjMXMrCO1KikEcJWk2ZIm57LREbEgv38cGN3TByVNljRL0qzFixc3I1Yzs47RqgvN74mIRyW9Drha0n3lmRERkqKnD0bE6aRB+pg4cWKPy5iZ2eC05EwhIh7Nr4uAS4DtgYWSNgbIr4taEZuZWSdr+pmCpHWAVSJiaX7/IeCbwAzgIGBafr2s2bG1I3dVNbOBaEXz0WjgEkld6/9VRFwh6VbgQkmHAA8B+7UgNjOzjtb0pBARc4G391D+d+ADzY6nFXz/gpkNVb6j2YYkN3uZtcZQuk/BOtz4qTOdDMxazEnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRglQ20d5B7E5kNP04KZmZWcFKwhvLZgdnw5qRgZmYFJwUzMys4KTSAm0zMrF04KbSYE4qZDSUeJXUYG07JpFHDhZe32UOPmzWezxTMzKzgpDBIvTX7uDnIzIYzJwV7FSc2s87lawodym3zZtYTnymYmVnBScEANxmZWeKkYGZmBScFMzMrOCmYmVnBvY9s2HIPKrPG85mCDUo7XJhuh20wazQnBTMzKzgpWJ+qHE2vzBG3j9bNhhZfU7C242sNZoPnMwUzMyv4TGEAGvVMAGsenzWYDYyTgnUMJwiz/rn5yMzMCj5T6IGbieo1FL/f3mIql1dZZijwGZGtjCGXFCTtBvwEWBU4MyKmtTKeofYf3nrWzt1a/SNvzTSkkoKkVYFTgQ8C84FbJc2IiHtaG5l1kioHAu1w1jDUYl0ZTpyNM6SSArA98GBEzAWQdAEwCXBSsGFnuCQOGHjz2VDchuFuqCQ2RUTLVt6dpI8Cu0XEZ/L0J4EdImJKaZnJwOQ8+Sbg/pVc7UjgiZWsY7jxNncGb3NnGMw2bxYRo3qaMdTOFPoVEacDpzeqPkmzImJio+obDrzNncHb3Bkavc1DrUvqo8C40vTYXGZmZk0w1JLCrcBWkjaXtAawPzCjxTGZmXWMIdV8FBHLJE0BriR1ST07Iu6uebUNa4oaRrzNncHb3Bkaus1D6kKzmZm11lBrPjIzsxZyUjAzs0LHJgVJu0m6X9KDkqa2Op46SBon6VpJ90i6W9KRuXxDSVdLeiC/btDqWBtN0qqSbpP0uzy9uaSb8/7+de7I0DYkjZB0kaT7JN0r6V3tvp8lfSH/u75L0vmS1my3/SzpbEmLJN1VKutxvyo5OW/7HZK2G8w6OzIplIbT2B3YGjhA0tatjaoWy4CjImJrYEfgiLydU4FrImIr4Jo83W6OBO4tTX8P+FFEvAF4CjikJVHV5yfAFRHxZuDtpG1v2/0saQzwOWBiRLyF1DFlf9pvP58D7NatrLf9ujuwVf6bDJw2mBV2ZFKgNJxGRLwIdA2n0VYiYkFEzMnvl5J+KMaQtnV6Xmw6sHdrIqyHpLHAnsCZeVrArsBFeZG22mZJ6wM7A2cBRMSLEfE0bb6fSb0n15K0GrA2sIA2288RcQPwZLfi3vbrJODcSG4CRkjaeKDr7NSkMAZ4pDQ9P5e1LUnjgW2Bm4HREbEgz3ocGN2isOryY+D/AMvz9EbA0xGxLE+32/7eHFgM/Dw3mZ0paR3aeD9HxKPAD4GHScngH8Bs2ns/d+ltvzbkd61Tk0JHkfRa4DfA5yNiSXlepD7JbdMvWdJHgEURMbvVsTTRasB2wGkRsS3wLN2aitpwP29AOjLeHNgEWIdXN7O0vTr2a6cmhY4ZTkPS6qSEcF5EXJyLF3adVubXRa2KrwY7AXtJmkdqFtyV1N4+IjczQPvt7/nA/Ii4OU9fREoS7byf/xX4W0QsjoiXgItJ+76d93OX3vZrQ37XOjUpdMRwGrkt/Szg3og4qTRrBnBQfn8QcFmzY6tLRBwdEWMjYjxpv/5nRHwcuBb4aF6s3bb5ceARSW/KRR8gDTfftvuZ1Gy0o6S187/zrm1u2/1c0tt+nQEcmHsh7Qj8o9TMVFnH3tEsaQ9S23PXcBontDikhpP0HuCPwJ280r5+DOm6woXApsBDwH4R0f1i1rAnaRfgSxHxEUlbkM4cNgRuAz4RES+0Mr5GkjSBdGF9DWAucDDpoK9t97OkbwAfI/Wyuw34DKkNvW32s6TzgV1Iw2MvBI4DLqWH/ZqT4ymkZrTngIMjYtaA19mpScHMzF6tU5uPzMysB04KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYEOWpJB0Ymn6S5KOb1Dd50j6aP9LrvR69s2jll7b4HovlzSih/LjJX2pkeuyzuKkYEPZC8A+kka2OpCy0h2zVRwCHBoR729QfQBExB550DuzhnJSsKFsGen5s1/oPqP7kb6kZ/LrLpKul3SZpLmSpkn6uKRbJN0pactSNf8qaZak/8pjJnU9h+EHkm7NY9IfVqr3j5JmkO6c7R7PAbn+uyR9L5d9HXgPcJakH3RbfoX6+ljvxpJukHR7rvu9uXxeV7KU9NW8DTcCbyqt4zpJE/P7kXnoj762scd1WWcZ8BGKWZOdCtwh6fsD+MzbgX8hDTk8FzgzIrZXesjQZ4HP5+XGk4ZR3xK4VtIbgANJwwO8U9JrgD9Juiovvx3wloj4W3llkjYhjeP/DtIY/ldJ2jsivilpV9Jd1T3dWVrUJ2lyL+vdB7gyIk5Qeg7I2t3W/Q7ScB4TSP+f55BGC+3LIYNZl3UGJwUb0iJiiaRzSQ9U+WfFj93aNeaLpL8CXT/qdwLlZpwLI2I58ICkucCbgQ8BbyudhaxPemjJi8At3RNC9k7guohYnNd5Hun5Bpf2E2e5vt7WeytwttLAhpdGxO3d6ngvcElEPJfXXWUMr8GuyzqAk4INBz8mHQH/vFS2jNz8KWkV0pg/Xcpj3SwvTS9nxX/z3cd4CUDAZyPiyvKMPI7Ss4MLv1fl+npcb173zqSHBp0j6aSIOLdi/cV3BKxZ87qsTfiagg15eRC3C1nx0YrzSM01AHsBqw+i6n0lrZKvM2wB3A9cCfyvfLSMpDcqPbCmL7cA78vt9qsCBwDXDzCWHtcraTNgYUScQRrwrvtzd28A9pa0lqR1gX8rzZvHK99RuafVYNdlHcBnCjZcnAhMKU2fAVwm6S/AFQzuKP5h0g/6esDhEfG8pDNJ1xrmSBLpiWZ9PtIxIhZImkoatlnAzIgY6JDNva13F+DLkl4CniFd8yive46kXwN/IY2rf2tp9g+BC/P1ipkruy7rDB4l1czMCm4+MjOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwK/w3pmC8T22oLsAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EptLKy7U6V3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenises Sequence using dictionary above\n",
        "def sequence_to_vector(sequence):\n",
        "\n",
        "    default = np.asarray([21]*(MAX_SEQ_LEN))\n",
        "    for i, character in enumerate(sequence[:MAX_SEQ_LEN]):\n",
        "        default[i] = CHARACTER_DICT[character]\n",
        "    return default.astype(int)\n",
        "\n",
        "# Converts token vector to readable sequence\n",
        "# Only token outside of INDEX_DICT possible is 0,\n",
        "# and thus it will show up as 0 in sequence making it invalid\n",
        "\n",
        "def vector_to_sequence(vector):\n",
        "  \n",
        "    return ''.join([INDEX_DICT.get(item, '0')  for item in vector])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuUZiwOlAxIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7db12a60-ddf5-4cb1-f75b-51d382781a0f"
      },
      "source": [
        "all_sequences = np.asarray(data['seq'])\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for i in range(len(all_sequences)):\n",
        "\n",
        "  all_data.append(sequence_to_vector(all_sequences[i]))\n",
        "\n",
        "#Sanity check whether the data has been featurized properly\n",
        "some_random_number = np.random.randint(len(all_sequences))\n",
        "print(all_sequences[some_random_number])\n",
        "print(all_data[some_random_number])\n",
        "print(vector_to_sequence(all_data[some_random_number]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KKKKVVAAFYVFV\n",
            "[ 8  8  8  8 18 18  1  1  5 19 18  5 18 21 21 21 21 21]\n",
            "KKKKVVAAFYVFVXXXXX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvGgl0eizU6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, gpu=False, oracle_init=False):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.gru2out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # initialise oracle network with N(0,1)\n",
        "        # otherwise variance of initialisation is very small => high NLL for data sampled from the same model\n",
        "        if oracle_init:\n",
        "            for p in self.parameters():\n",
        "                init.normal(p, 0, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        h = autograd.Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
        "\n",
        "        if self.gpu:\n",
        "            return h.cuda()\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        \"\"\"\n",
        "        Embeds input and applies GRU one token at a time (seq_len = 1)\n",
        "        \"\"\"\n",
        "        # input dim                                             # batch_size\n",
        "        emb = self.embeddings(inp)                              # batch_size x embedding_dim\n",
        "        emb = emb.view(1, -1, self.embedding_dim)               # 1 x batch_size x embedding_dim\n",
        "        out, hidden = self.gru(emb, hidden)                     # 1 x batch_size x hidden_dim (out)\n",
        "        out = self.gru2out(out.view(-1, self.hidden_dim))       # batch_size x vocab_size\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "        return out, hidden\n",
        "\n",
        "    def sample(self, num_samples, start_letter=0):\n",
        "        \"\"\"\n",
        "        Samples the network and returns num_samples samples of length max_seq_len.\n",
        "        Outputs: samples, hidden\n",
        "            - samples: num_samples x max_seq_length (a sampled sequence in each row)\n",
        "        \"\"\"\n",
        "\n",
        "        samples = torch.zeros(num_samples, self.max_seq_len).type(torch.LongTensor)\n",
        "\n",
        "        h = self.init_hidden(num_samples)\n",
        "        inp = autograd.Variable(torch.LongTensor([start_letter]*num_samples))\n",
        "\n",
        "        if self.gpu:\n",
        "            samples = samples.cuda()\n",
        "            inp = inp.cuda()\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            out, h = self.forward(inp, h)               # out: num_samples x vocab_size\n",
        "            out = torch.multinomial(torch.exp(out), 1)  # num_samples x 1 (sampling from each row)\n",
        "            samples[:, i] = out.view(-1).data\n",
        "\n",
        "            inp = out.view(-1)\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def batchNLLLoss(self, inp, target):\n",
        "        \"\"\"\n",
        "        Returns the NLL Loss for predicting target sequence.\n",
        "        Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size x seq_len\n",
        "            inp should be target with <s> (start letter) prepended\n",
        "        \"\"\"\n",
        "\n",
        "        loss_fn = nn.NLLLoss()\n",
        "        batch_size, seq_len = inp.size()\n",
        "        inp = inp.permute(1, 0)           # seq_len x batch_size\n",
        "        target = target.permute(1, 0)     # seq_len x batch_size\n",
        "        h = self.init_hidden(batch_size)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            out, h = self.forward(inp[i], h)\n",
        "            loss += loss_fn(out, target[i])\n",
        "\n",
        "        return loss     # per batch\n",
        "\n",
        "    def batchPGLoss(self, inp, target, reward):\n",
        "        \"\"\"\n",
        "        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).\n",
        "        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/\n",
        "        Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size x seq_len\n",
        "            - reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding\n",
        "                      sentence)\n",
        "            inp should be target with <s> (start letter) prepended\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inp.size()\n",
        "        inp = inp.permute(1, 0)          # seq_len x batch_size\n",
        "        target = target.permute(1, 0)    # seq_len x batch_size\n",
        "        h = self.init_hidden(batch_size)\n",
        "\n",
        "        loss = 0\n",
        "        for i in range(seq_len):\n",
        "            out, h = self.forward(inp[i], h)\n",
        "            # TODO: should h be detached from graph (.detach())?\n",
        "            for j in range(batch_size):\n",
        "                loss += -out[j][target.data[i][j]]*reward[j]     # log(P(y_t|Y_1:Y_{t-1})) * Q\n",
        "\n",
        "        return loss/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3pIMHP1Rlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, gpu=False, dropout=0.2):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=dropout)\n",
        "        self.gru2hidden = nn.Linear(2*2*hidden_dim, hidden_dim)\n",
        "        self.dropout_linear = nn.Dropout(p=dropout)\n",
        "        self.hidden2out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = autograd.Variable(torch.zeros(2*2*1, batch_size, self.hidden_dim))\n",
        "\n",
        "        if self.gpu:\n",
        "            return h.cuda()\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input dim                                                # batch_size x seq_len\n",
        "        emb = self.embeddings(input)                               # batch_size x seq_len x embedding_dim\n",
        "        emb = emb.permute(1, 0, 2)                                 # seq_len x batch_size x embedding_dim\n",
        "        _, hidden = self.gru(emb, hidden)                          # 4 x batch_size x hidden_dim\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()              # batch_size x 4 x hidden_dim\n",
        "        out = self.gru2hidden(hidden.view(-1, 4*self.hidden_dim))  # batch_size x 4*hidden_dim\n",
        "        out = torch.tanh(out)\n",
        "        out = self.dropout_linear(out)\n",
        "        out = self.hidden2out(out)                                 # batch_size x 1\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "    def batchClassify(self, inp):\n",
        "        \"\"\"\n",
        "        Classifies a batch of sequences.\n",
        "        Inputs: inp\n",
        "            - inp: batch_size x seq_len\n",
        "        Returns: out\n",
        "            - out: batch_size ([0,1] score)\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.init_hidden(inp.size()[0])\n",
        "        out = self.forward(inp, h)\n",
        "        return out.view(-1)\n",
        "\n",
        "    def batchBCELoss(self, inp, target):\n",
        "        \"\"\"\n",
        "        Returns Binary Cross Entropy Loss for discriminator.\n",
        "         Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size (binary 1/0)\n",
        "        \"\"\"\n",
        "\n",
        "        loss_fn = nn.BCELoss()\n",
        "        h = self.init_hidden(inp.size()[0])\n",
        "        out = self.forward(inp, h)\n",
        "        return loss_fn(out, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI9bydIE1e0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_generator_batch(samples, start_letter=0, gpu=False):\n",
        "    \"\"\"\n",
        "    Takes samples (a batch) and returns\n",
        "    Inputs: samples, start_letter, cuda\n",
        "        - samples: batch_size x seq_len (Tensor with a sample in each row)\n",
        "    Returns: inp, target\n",
        "        - inp: batch_size x seq_len (same as target, but with start_letter prepended)\n",
        "        - target: batch_size x seq_len (Variable same as samples)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, seq_len = samples.size()\n",
        "\n",
        "    inp = torch.zeros(batch_size, seq_len)\n",
        "    target = samples\n",
        "    inp[:, 0] = start_letter\n",
        "    inp[:, 1:] = target[:, :seq_len-1]\n",
        "\n",
        "    inp = inp.type(torch.LongTensor)\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    if gpu:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "def prepare_discriminator_data(pos_samples, neg_samples, gpu=False):\n",
        "    \"\"\"\n",
        "    Takes positive (target) samples, negative (generator) samples and \n",
        "    prepares inp and target data for discriminator.\n",
        "    Inputs: pos_samples, neg_samples\n",
        "        - pos_samples: pos_size x seq_len\n",
        "        - neg_samples: neg_size x seq_len\n",
        "    Returns: inp, target\n",
        "        - inp: (pos_size + neg_size) x seq_len\n",
        "        - target: pos_size + neg_size (boolean 1/0)\n",
        "    \"\"\"\n",
        "\n",
        "    inp = torch.cat((pos_samples, neg_samples), 0).type(torch.LongTensor)\n",
        "    target = torch.ones(pos_samples.size()[0] + neg_samples.size()[0])\n",
        "    target[pos_samples.size()[0]:] = 0\n",
        "\n",
        "    # shuffle\n",
        "    perm = torch.randperm(target.size()[0])\n",
        "    target = target[perm]\n",
        "    inp = inp[perm]\n",
        "\n",
        "#    inp = Variable(inp)\n",
        "#    target = Variable(target)\n",
        "\n",
        "    if gpu:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "def batchwise_sample(gen, num_samples, batch_size):\n",
        "    \"\"\"\n",
        "    Sample num_samples samples batch_size samples at a time from gen.\n",
        "    Does not require gpu since gen.sample() takes care of that.\n",
        "    \"\"\"\n",
        "\n",
        "    samples = []\n",
        "    for i in range(int(ceil(num_samples/float(batch_size)))):\n",
        "        samples.append(gen.sample(batch_size))\n",
        "\n",
        "    return torch.cat(samples, 0)[:num_samples]\n",
        "\n",
        "def batchwise_oracle_nll(gen, oracle, num_samples, batch_size, max_seq_len, start_letter=0, gpu=False):\n",
        "    s = batchwise_sample(gen, num_samples, batch_size)\n",
        "    oracle_nll = 0\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        inp, target = prepare_generator_batch(s[i:i+batch_size], start_letter, gpu)\n",
        "        oracle_loss = oracle.batchNLLLoss(inp, target) / max_seq_len\n",
        "        oracle_nll += oracle_loss.data.item()\n",
        "\n",
        "    return oracle_nll/(num_samples/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJi2eS9t2BeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator_MLE(gen, gen_opt, oracle, real_data_samples, epochs):\n",
        "    \"\"\"\n",
        "    Max Likelihood Pretraining for the generator\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch %d : ' % (epoch + 1), end='')\n",
        "        sys.stdout.flush()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
        "            inp, target = prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
        "                                                          gpu=CUDA)\n",
        "            gen_opt.zero_grad()\n",
        "            loss = gen.batchNLLLoss(inp, target)\n",
        "            loss.backward()\n",
        "            gen_opt.step()\n",
        "\n",
        "            total_loss += loss.data.item()\n",
        "\n",
        "            if (i / BATCH_SIZE) % ceil(\n",
        "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
        "                print('.', end='')\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        # each loss in a batch is loss per sample\n",
        "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
        "\n",
        "        # sample from generator and compute oracle NLL\n",
        "        oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
        "        \n",
        "        loss_g.append(oracle_loss)\n",
        "\n",
        "        print(' average_train_NLL = %.4f, oracle_sample_NLL = %.4f' % (total_loss, oracle_loss))\n",
        "\n",
        "\n",
        "def train_generator_PG(gen, gen_opt, oracle, dis, num_batches):\n",
        "    \"\"\"\n",
        "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
        "    Training is done for num_batches batches.\n",
        "    \"\"\"\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
        "        inp, target = prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
        "        rewards = dis.batchClassify(target)\n",
        "\n",
        "        gen_opt.zero_grad()\n",
        "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
        "        pg_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "    # sample from generator and compute oracle NLL\n",
        "    oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
        "\n",
        "    loss_g.append(oracle_loss)\n",
        "    \n",
        "    print(' oracle_sample_NLL = %.4f' % oracle_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nFTuxWK2fn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs):\n",
        "    \"\"\"\n",
        "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
        "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # generating a small validation set before training (using oracle and generator)\n",
        "    pos_val = oracle.sample(100)\n",
        "    neg_val = generator.sample(100)\n",
        "    val_inp, val_target = prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
        "\n",
        "    for d_step in range(d_steps):\n",
        "        s = batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
        "#        print(s.shape, real_data_samples.shape)\n",
        "        dis_inp, dis_target = prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
        "        for epoch in range(epochs):\n",
        "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
        "            sys.stdout.flush()\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "\n",
        "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE): #2 * POS_NEG_SAMPLES because both pos \n",
        "                                                                #and neg samples included in dis_inp\n",
        "                inp, target = dis_inp[i:i + BATCH_SIZE], dis_target[i:i + BATCH_SIZE]\n",
        "                dis_opt.zero_grad()\n",
        "                out = discriminator.batchClassify(inp)\n",
        "                loss_fn = nn.BCELoss()\n",
        "                loss = loss_fn(out, target)\n",
        "                loss.backward()\n",
        "                dis_opt.step()\n",
        "\n",
        "                total_loss += loss.data.item()\n",
        "                total_acc += torch.sum((out>0.5)==(target>0.5)).data.item()\n",
        "\n",
        "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
        "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
        "                    print('.', end='')\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
        "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
        "\n",
        "            val_pred = discriminator.batchClassify(val_inp)\n",
        "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
        "                total_loss, total_acc, torch.sum((val_pred>0.5)==(val_target>0.5)).data.item()/200.))\n",
        "            \n",
        "            loss_d.append(total_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6kKYPcBhm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA = False\n",
        "\n",
        "#Fixed Params\n",
        "VOCAB_SIZE = 22 #Starting Letter + 20 AA + Padding\n",
        "MAX_SEQ_LEN = 18 #2000 kDa / 110 kDa = 18\n",
        "START_LETTER = 0 \n",
        "POS_NEG_SAMPLES = len(all_data) #Size of AVPDb dataset\n",
        "torch.manual_seed(11)\n",
        "\n",
        "#Variables\n",
        "BATCH_SIZE = 64 \n",
        "ADV_TRAIN_EPOCHS = 50 \n",
        "\n",
        "#Generator Parameters\n",
        "MLE_TRAIN_EPOCHS = 75\n",
        "GEN_EMBEDDING_DIM = 2\n",
        "GEN_HIDDEN_DIM = 24\n",
        "NUM_PG_BATCHES = 1\n",
        "GEN_lr = 3e-2\n",
        "\n",
        "#Discriminator Parameters\n",
        "DIS_EMBEDDING_DIM = 2            \n",
        "DIS_HIDDEN_DIM = 24\n",
        "D_STEPS = 50\n",
        "D_EPOCHS = 2\n",
        "ADV_D_EPOCHS = 5\n",
        "ADV_D_STEPS = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y8IoAPo2ktX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a804f82a-0cea-4262-e845-c5408178e89f"
      },
      "source": [
        "#Used to store output of cell\n",
        "\n",
        "# MAIN\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # a new oracle can be generated by passing oracle_init=True in the generator constructor\n",
        "    oracle = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA, oracle_init=True)\n",
        "\n",
        "    #Initialize Generator and Discriminator\n",
        "    gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
        "    dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
        "\n",
        "    loss_g = []\n",
        "    loss_d = []\n",
        "    \n",
        "\n",
        "    if CUDA:\n",
        "      oracle = oracle.cuda()\n",
        "      gen = gen.cuda()\n",
        "      dis = dis.cuda()\n",
        "      \n",
        "      #Makes a dataset which follows oracle's distribution\n",
        "      oracle_samples = oracle.sample(POS_NEG_SAMPLES)\n",
        "      oracle_samples = oracle_samples.cuda()\n",
        "    \n",
        "    else:\n",
        "      oracle_samples = oracle.sample(POS_NEG_SAMPLES)\n",
        "\n",
        "    # GENERATOR MLE TRAINING\n",
        "    print('Starting Generator MLE Training...')\n",
        "    gen_optimizer = optim.Adam(gen.parameters(), lr = GEN_lr)\n",
        "    train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
        "\n",
        "    # torch.save(gen.state_dict(), pretrained_gen_path)\n",
        "    # gen.load_state_dict(torch.load(pretrained_gen_path))\n",
        "\n",
        "    # PRETRAIN DISCRIMINATOR\n",
        "    print('\\nStarting Discriminator Training...')\n",
        "    dis_optimizer = optim.Adagrad(dis.parameters())\n",
        "    train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, D_STEPS, D_EPOCHS)\n",
        "\n",
        "    # ADVERSARIAL TRAINING\n",
        "    print('\\nStarting Adversarial Training...')\n",
        "    oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                               start_letter=START_LETTER, gpu=CUDA)\n",
        "    print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
        "\n",
        "    for epoch in range(ADV_TRAIN_EPOCHS):\n",
        "        print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
        "        # TRAIN GENERATOR\n",
        "        print('\\nAdversarial Training Generator : ', end='')\n",
        "        sys.stdout.flush()\n",
        "        train_generator_PG(gen, gen_optimizer, oracle, dis, NUM_PG_BATCHES)\n",
        "\n",
        "        # TRAIN DISCRIMINATOR\n",
        "        print('\\nAdversarial Training Discriminator : ')\n",
        "        train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, ADV_D_STEPS, ADV_D_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Generator MLE Training...\n",
            "epoch 1 : .."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "....... average_train_NLL = 2.0570, oracle_sample_NLL = 3.0797\n",
            "epoch 2 : ......... average_train_NLL = 1.8306, oracle_sample_NLL = 2.9637\n",
            "epoch 3 : ......... average_train_NLL = 1.7684, oracle_sample_NLL = 2.7027\n",
            "epoch 4 : ......... average_train_NLL = 1.7194, oracle_sample_NLL = 2.6833\n",
            "epoch 5 : ......... average_train_NLL = 1.6915, oracle_sample_NLL = 2.5729\n",
            "epoch 6 : ......... average_train_NLL = 1.6684, oracle_sample_NLL = 2.4939\n",
            "epoch 7 : ......... average_train_NLL = 1.6502, oracle_sample_NLL = 2.4554\n",
            "epoch 8 : ......... average_train_NLL = 1.6381, oracle_sample_NLL = 2.3891\n",
            "epoch 9 : ......... average_train_NLL = 1.6291, oracle_sample_NLL = 2.3684\n",
            "epoch 10 : ......... average_train_NLL = 1.6172, oracle_sample_NLL = 2.3743\n",
            "epoch 11 : ......... average_train_NLL = 1.6130, oracle_sample_NLL = 2.3112\n",
            "epoch 12 : ......... average_train_NLL = 1.6037, oracle_sample_NLL = 2.2337\n",
            "epoch 13 : ......... average_train_NLL = 1.5982, oracle_sample_NLL = 2.2585\n",
            "epoch 14 : ......... average_train_NLL = 1.5970, oracle_sample_NLL = 2.2160\n",
            "epoch 15 : ......... average_train_NLL = 1.6002, oracle_sample_NLL = 2.1444\n",
            "epoch 16 : ......... average_train_NLL = 1.5976, oracle_sample_NLL = 2.1314\n",
            "epoch 17 : ......... average_train_NLL = 1.5855, oracle_sample_NLL = 2.1747\n",
            "epoch 18 : ......... average_train_NLL = 1.5830, oracle_sample_NLL = 2.1505\n",
            "epoch 19 : ......... average_train_NLL = 1.5800, oracle_sample_NLL = 2.1103\n",
            "epoch 20 : ......... average_train_NLL = 1.5772, oracle_sample_NLL = 2.1558\n",
            "epoch 21 : ......... average_train_NLL = 1.5725, oracle_sample_NLL = 2.1671\n",
            "epoch 22 : ......... average_train_NLL = 1.5685, oracle_sample_NLL = 2.1730\n",
            "epoch 23 : ......... average_train_NLL = 1.5675, oracle_sample_NLL = 2.0619\n",
            "epoch 24 : ......... average_train_NLL = 1.5708, oracle_sample_NLL = 2.1042\n",
            "epoch 25 : ......... average_train_NLL = 1.5665, oracle_sample_NLL = 2.1148\n",
            "epoch 26 : ......... average_train_NLL = 1.5639, oracle_sample_NLL = 2.1001\n",
            "epoch 27 : ......... average_train_NLL = 1.5654, oracle_sample_NLL = 2.1503\n",
            "epoch 28 : ......... average_train_NLL = 1.5616, oracle_sample_NLL = 2.1052\n",
            "epoch 29 : ......... average_train_NLL = 1.5581, oracle_sample_NLL = 2.1450\n",
            "epoch 30 : ......... average_train_NLL = 1.5599, oracle_sample_NLL = 2.1351\n",
            "epoch 31 : ......... average_train_NLL = 1.5579, oracle_sample_NLL = 2.1094\n",
            "epoch 32 : ......... average_train_NLL = 1.5586, oracle_sample_NLL = 2.0678\n",
            "epoch 33 : ......... average_train_NLL = 1.5579, oracle_sample_NLL = 2.1016\n",
            "epoch 34 : ......... average_train_NLL = 1.5584, oracle_sample_NLL = 2.0826\n",
            "epoch 35 : ......... average_train_NLL = 1.5636, oracle_sample_NLL = 2.1581\n",
            "epoch 36 : ......... average_train_NLL = 1.5578, oracle_sample_NLL = 2.0598\n",
            "epoch 37 : ......... average_train_NLL = 1.5578, oracle_sample_NLL = 2.1017\n",
            "epoch 38 : ......... average_train_NLL = 1.5531, oracle_sample_NLL = 2.0676\n",
            "epoch 39 : ......... average_train_NLL = 1.5540, oracle_sample_NLL = 2.0792\n",
            "epoch 40 : ......... average_train_NLL = 1.5500, oracle_sample_NLL = 2.1385\n",
            "epoch 41 : ......... average_train_NLL = 1.5520, oracle_sample_NLL = 2.1399\n",
            "epoch 42 : ......... average_train_NLL = 1.5520, oracle_sample_NLL = 2.1698\n",
            "epoch 43 : ......... average_train_NLL = 1.5440, oracle_sample_NLL = 2.1195\n",
            "epoch 44 : ......... average_train_NLL = 1.5464, oracle_sample_NLL = 2.0978\n",
            "epoch 45 : ......... average_train_NLL = 1.5482, oracle_sample_NLL = 2.1482\n",
            "epoch 46 : ......... average_train_NLL = 1.5463, oracle_sample_NLL = 2.1249\n",
            "epoch 47 : ......... average_train_NLL = 1.5439, oracle_sample_NLL = 2.0884\n",
            "epoch 48 : ......... average_train_NLL = 1.5455, oracle_sample_NLL = 2.0813\n",
            "epoch 49 : ......... average_train_NLL = 1.5478, oracle_sample_NLL = 2.1088\n",
            "epoch 50 : ......... average_train_NLL = 1.5484, oracle_sample_NLL = 2.0327\n",
            "epoch 51 : ......... average_train_NLL = 1.5517, oracle_sample_NLL = 2.0519\n",
            "epoch 52 : ......... average_train_NLL = 1.5517, oracle_sample_NLL = 2.0948\n",
            "epoch 53 : ......... average_train_NLL = 1.5514, oracle_sample_NLL = 2.0736\n",
            "epoch 54 : ......... average_train_NLL = 1.5572, oracle_sample_NLL = 2.1330\n",
            "epoch 55 : ......... average_train_NLL = 1.5496, oracle_sample_NLL = 2.1066\n",
            "epoch 56 : ......... average_train_NLL = 1.5518, oracle_sample_NLL = 2.0869\n",
            "epoch 57 : ......... average_train_NLL = 1.5603, oracle_sample_NLL = 2.0918\n",
            "epoch 58 : ......... average_train_NLL = 1.5494, oracle_sample_NLL = 2.1123\n",
            "epoch 59 : ......... average_train_NLL = 1.5480, oracle_sample_NLL = 2.1161\n",
            "epoch 60 : ......... average_train_NLL = 1.5525, oracle_sample_NLL = 2.0640\n",
            "epoch 61 : ......... average_train_NLL = 1.5485, oracle_sample_NLL = 2.1394\n",
            "epoch 62 : ......... average_train_NLL = 1.5450, oracle_sample_NLL = 2.1695\n",
            "epoch 63 : ......... average_train_NLL = 1.5471, oracle_sample_NLL = 2.0675\n",
            "epoch 64 : ......... average_train_NLL = 1.5528, oracle_sample_NLL = 2.1161\n",
            "epoch 65 : ......... average_train_NLL = 1.5499, oracle_sample_NLL = 2.0977\n",
            "epoch 66 : ......... average_train_NLL = 1.5482, oracle_sample_NLL = 2.0767\n",
            "epoch 67 : ......... average_train_NLL = 1.5430, oracle_sample_NLL = 2.0979\n",
            "epoch 68 : ......... average_train_NLL = 1.5535, oracle_sample_NLL = 2.1291\n",
            "epoch 69 : ......... average_train_NLL = 1.5460, oracle_sample_NLL = 2.1224\n",
            "epoch 70 : ......... average_train_NLL = 1.5407, oracle_sample_NLL = 2.0618\n",
            "epoch 71 : ......... average_train_NLL = 1.5461, oracle_sample_NLL = 2.0704\n",
            "epoch 72 : ......... average_train_NLL = 1.5465, oracle_sample_NLL = 2.0852\n",
            "epoch 73 : ......... average_train_NLL = 1.5488, oracle_sample_NLL = 2.1072\n",
            "epoch 74 : ......... average_train_NLL = 1.5445, oracle_sample_NLL = 2.1451\n",
            "epoch 75 : ......... average_train_NLL = 1.5432, oracle_sample_NLL = 2.0826\n",
            "\n",
            "Starting Discriminator Training...\n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6856, train_acc = 0.5425, val_acc = 0.5400\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6762, train_acc = 0.5695, val_acc = 0.5800\n",
            "d-step 2 epoch 1 : .......... average_loss = 0.6783, train_acc = 0.5566, val_acc = 0.5400\n",
            "d-step 2 epoch 2 : .......... average_loss = 0.6758, train_acc = 0.5617, val_acc = 0.5750\n",
            "d-step 3 epoch 1 : .......... average_loss = 0.6703, train_acc = 0.5709, val_acc = 0.5800\n",
            "d-step 3 epoch 2 : .......... average_loss = 0.6692, train_acc = 0.5704, val_acc = 0.5900\n",
            "d-step 4 epoch 1 : .......... average_loss = 0.6704, train_acc = 0.5792, val_acc = 0.5400\n",
            "d-step 4 epoch 2 : .......... average_loss = 0.6673, train_acc = 0.5823, val_acc = 0.5450\n",
            "d-step 5 epoch 1 : .......... average_loss = 0.6703, train_acc = 0.5799, val_acc = 0.5500\n",
            "d-step 5 epoch 2 : .......... average_loss = 0.6696, train_acc = 0.5852, val_acc = 0.5550\n",
            "d-step 6 epoch 1 : .......... average_loss = 0.6685, train_acc = 0.5780, val_acc = 0.5400\n",
            "d-step 6 epoch 2 : .......... average_loss = 0.6689, train_acc = 0.5806, val_acc = 0.5050\n",
            "d-step 7 epoch 1 : .......... average_loss = 0.6681, train_acc = 0.5741, val_acc = 0.5450\n",
            "d-step 7 epoch 2 : .......... average_loss = 0.6679, train_acc = 0.5772, val_acc = 0.5450\n",
            "d-step 8 epoch 1 : .......... average_loss = 0.6657, train_acc = 0.5877, val_acc = 0.5350\n",
            "d-step 8 epoch 2 : .......... average_loss = 0.6653, train_acc = 0.5833, val_acc = 0.5350\n",
            "d-step 9 epoch 1 : .......... average_loss = 0.6671, train_acc = 0.5831, val_acc = 0.5550\n",
            "d-step 9 epoch 2 : .......... average_loss = 0.6669, train_acc = 0.5828, val_acc = 0.5650\n",
            "d-step 10 epoch 1 : .......... average_loss = 0.6628, train_acc = 0.5864, val_acc = 0.5500\n",
            "d-step 10 epoch 2 : .......... average_loss = 0.6603, train_acc = 0.5889, val_acc = 0.5400\n",
            "d-step 11 epoch 1 : .......... average_loss = 0.6611, train_acc = 0.5918, val_acc = 0.5250\n",
            "d-step 11 epoch 2 : .......... average_loss = 0.6602, train_acc = 0.5937, val_acc = 0.5300\n",
            "d-step 12 epoch 1 : .......... average_loss = 0.6645, train_acc = 0.5877, val_acc = 0.5450\n",
            "d-step 12 epoch 2 : .......... average_loss = 0.6639, train_acc = 0.5901, val_acc = 0.5450\n",
            "d-step 13 epoch 1 : .......... average_loss = 0.6649, train_acc = 0.5816, val_acc = 0.5550\n",
            "d-step 13 epoch 2 : .......... average_loss = 0.6638, train_acc = 0.5847, val_acc = 0.5600\n",
            "d-step 14 epoch 1 : .......... average_loss = 0.6639, train_acc = 0.5850, val_acc = 0.5550\n",
            "d-step 14 epoch 2 : .......... average_loss = 0.6635, train_acc = 0.5903, val_acc = 0.5400\n",
            "d-step 15 epoch 1 : .......... average_loss = 0.6665, train_acc = 0.5864, val_acc = 0.5550\n",
            "d-step 15 epoch 2 : .......... average_loss = 0.6657, train_acc = 0.5831, val_acc = 0.5450\n",
            "d-step 16 epoch 1 : .......... average_loss = 0.6610, train_acc = 0.5908, val_acc = 0.5450\n",
            "d-step 16 epoch 2 : .......... average_loss = 0.6610, train_acc = 0.5903, val_acc = 0.5350\n",
            "d-step 17 epoch 1 : .......... average_loss = 0.6643, train_acc = 0.5864, val_acc = 0.5450\n",
            "d-step 17 epoch 2 : .......... average_loss = 0.6642, train_acc = 0.5855, val_acc = 0.5500\n",
            "d-step 18 epoch 1 : .......... average_loss = 0.6539, train_acc = 0.6003, val_acc = 0.5550\n",
            "d-step 18 epoch 2 : .......... average_loss = 0.6528, train_acc = 0.6000, val_acc = 0.5650\n",
            "d-step 19 epoch 1 : .......... average_loss = 0.6633, train_acc = 0.5877, val_acc = 0.5550\n",
            "d-step 19 epoch 2 : .......... average_loss = 0.6620, train_acc = 0.5898, val_acc = 0.5400\n",
            "d-step 20 epoch 1 : .......... average_loss = 0.6584, train_acc = 0.5925, val_acc = 0.5350\n",
            "d-step 20 epoch 2 : .......... average_loss = 0.6574, train_acc = 0.5869, val_acc = 0.5400\n",
            "d-step 21 epoch 1 : .......... average_loss = 0.6619, train_acc = 0.5864, val_acc = 0.5450\n",
            "d-step 21 epoch 2 : .......... average_loss = 0.6606, train_acc = 0.5911, val_acc = 0.5400\n",
            "d-step 22 epoch 1 : .......... average_loss = 0.6617, train_acc = 0.5872, val_acc = 0.5550\n",
            "d-step 22 epoch 2 : .......... average_loss = 0.6621, train_acc = 0.5862, val_acc = 0.5500\n",
            "d-step 23 epoch 1 : .......... average_loss = 0.6641, train_acc = 0.5850, val_acc = 0.5450\n",
            "d-step 23 epoch 2 : .......... average_loss = 0.6631, train_acc = 0.5862, val_acc = 0.5500\n",
            "d-step 24 epoch 1 : .......... average_loss = 0.6640, train_acc = 0.5831, val_acc = 0.5550\n",
            "d-step 24 epoch 2 : .......... average_loss = 0.6632, train_acc = 0.5872, val_acc = 0.5450\n",
            "d-step 25 epoch 1 : .......... average_loss = 0.6603, train_acc = 0.5949, val_acc = 0.5600\n",
            "d-step 25 epoch 2 : .......... average_loss = 0.6602, train_acc = 0.5911, val_acc = 0.5300\n",
            "d-step 26 epoch 1 : .......... average_loss = 0.6586, train_acc = 0.5935, val_acc = 0.5250\n",
            "d-step 26 epoch 2 : .......... average_loss = 0.6593, train_acc = 0.5983, val_acc = 0.5250\n",
            "d-step 27 epoch 1 : .......... average_loss = 0.6634, train_acc = 0.5898, val_acc = 0.5600\n",
            "d-step 27 epoch 2 : .......... average_loss = 0.6641, train_acc = 0.5881, val_acc = 0.5100\n",
            "d-step 28 epoch 1 : .......... average_loss = 0.6566, train_acc = 0.5947, val_acc = 0.5450\n",
            "d-step 28 epoch 2 : .......... average_loss = 0.6548, train_acc = 0.5981, val_acc = 0.5300\n",
            "d-step 29 epoch 1 : .......... average_loss = 0.6583, train_acc = 0.5964, val_acc = 0.5500\n",
            "d-step 29 epoch 2 : .......... average_loss = 0.6579, train_acc = 0.5947, val_acc = 0.5500\n",
            "d-step 30 epoch 1 : .......... average_loss = 0.6590, train_acc = 0.5981, val_acc = 0.5250\n",
            "d-step 30 epoch 2 : .......... average_loss = 0.6583, train_acc = 0.6003, val_acc = 0.5300\n",
            "d-step 31 epoch 1 : .......... average_loss = 0.6581, train_acc = 0.6010, val_acc = 0.5300\n",
            "d-step 31 epoch 2 : .......... average_loss = 0.6578, train_acc = 0.6017, val_acc = 0.5450\n",
            "d-step 32 epoch 1 : .......... average_loss = 0.6591, train_acc = 0.5991, val_acc = 0.5300\n",
            "d-step 32 epoch 2 : .......... average_loss = 0.6598, train_acc = 0.6017, val_acc = 0.5350\n",
            "d-step 33 epoch 1 : .......... average_loss = 0.6561, train_acc = 0.6066, val_acc = 0.5400\n",
            "d-step 33 epoch 2 : .......... average_loss = 0.6564, train_acc = 0.6051, val_acc = 0.5350\n",
            "d-step 34 epoch 1 : .......... average_loss = 0.6587, train_acc = 0.6010, val_acc = 0.5300\n",
            "d-step 34 epoch 2 : .......... average_loss = 0.6574, train_acc = 0.6013, val_acc = 0.5350\n",
            "d-step 35 epoch 1 : .......... average_loss = 0.6496, train_acc = 0.6081, val_acc = 0.5400\n",
            "d-step 35 epoch 2 : .......... average_loss = 0.6494, train_acc = 0.6132, val_acc = 0.5300\n",
            "d-step 36 epoch 1 : .......... average_loss = 0.6552, train_acc = 0.6030, val_acc = 0.5400\n",
            "d-step 36 epoch 2 : .......... average_loss = 0.6552, train_acc = 0.6020, val_acc = 0.5300\n",
            "d-step 37 epoch 1 : .......... average_loss = 0.6552, train_acc = 0.6136, val_acc = 0.5350\n",
            "d-step 37 epoch 2 : .......... average_loss = 0.6536, train_acc = 0.6102, val_acc = 0.5250\n",
            "d-step 38 epoch 1 : .......... average_loss = 0.6534, train_acc = 0.6044, val_acc = 0.5550\n",
            "d-step 38 epoch 2 : .......... average_loss = 0.6537, train_acc = 0.6025, val_acc = 0.5600\n",
            "d-step 39 epoch 1 : .......... average_loss = 0.6508, train_acc = 0.6105, val_acc = 0.5300\n",
            "d-step 39 epoch 2 : .......... average_loss = 0.6508, train_acc = 0.6081, val_acc = 0.5150\n",
            "d-step 40 epoch 1 : .......... average_loss = 0.6528, train_acc = 0.6005, val_acc = 0.5600\n",
            "d-step 40 epoch 2 : .......... average_loss = 0.6534, train_acc = 0.6020, val_acc = 0.5450\n",
            "d-step 41 epoch 1 : .......... average_loss = 0.6534, train_acc = 0.6078, val_acc = 0.5500\n",
            "d-step 41 epoch 2 : .......... average_loss = 0.6529, train_acc = 0.6102, val_acc = 0.5400\n",
            "d-step 42 epoch 1 : .......... average_loss = 0.6560, train_acc = 0.6085, val_acc = 0.5550\n",
            "d-step 42 epoch 2 : .......... average_loss = 0.6548, train_acc = 0.6051, val_acc = 0.5400\n",
            "d-step 43 epoch 1 : .......... average_loss = 0.6529, train_acc = 0.6141, val_acc = 0.5500\n",
            "d-step 43 epoch 2 : .......... average_loss = 0.6521, train_acc = 0.6098, val_acc = 0.5600\n",
            "d-step 44 epoch 1 : .......... average_loss = 0.6577, train_acc = 0.6059, val_acc = 0.5350\n",
            "d-step 44 epoch 2 : .......... average_loss = 0.6560, train_acc = 0.6020, val_acc = 0.5700\n",
            "d-step 45 epoch 1 : .......... average_loss = 0.6573, train_acc = 0.5996, val_acc = 0.5250\n",
            "d-step 45 epoch 2 : .......... average_loss = 0.6563, train_acc = 0.6076, val_acc = 0.5350\n",
            "d-step 46 epoch 1 : .......... average_loss = 0.6572, train_acc = 0.6013, val_acc = 0.5400\n",
            "d-step 46 epoch 2 : .......... average_loss = 0.6575, train_acc = 0.6047, val_acc = 0.5200\n",
            "d-step 47 epoch 1 : .......... average_loss = 0.6531, train_acc = 0.6158, val_acc = 0.5750\n",
            "d-step 47 epoch 2 : .......... average_loss = 0.6538, train_acc = 0.6185, val_acc = 0.5600\n",
            "d-step 48 epoch 1 : .......... average_loss = 0.6494, train_acc = 0.6098, val_acc = 0.5400\n",
            "d-step 48 epoch 2 : .......... average_loss = 0.6503, train_acc = 0.6085, val_acc = 0.5600\n",
            "d-step 49 epoch 1 : .......... average_loss = 0.6534, train_acc = 0.6073, val_acc = 0.5400\n",
            "d-step 49 epoch 2 : .......... average_loss = 0.6527, train_acc = 0.6078, val_acc = 0.5650\n",
            "d-step 50 epoch 1 : .......... average_loss = 0.6492, train_acc = 0.6083, val_acc = 0.5750\n",
            "d-step 50 epoch 2 : .......... average_loss = 0.6482, train_acc = 0.6144, val_acc = 0.5500\n",
            "\n",
            "Starting Adversarial Training...\n",
            "\n",
            "Initial Oracle Sample Loss : 2.0735\n",
            "\n",
            "--------\n",
            "EPOCH 1\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0624\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6503, train_acc = 0.6093, val_acc = 0.5400\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6492, train_acc = 0.6149, val_acc = 0.5250\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6468, train_acc = 0.6136, val_acc = 0.5450\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6493, train_acc = 0.6170, val_acc = 0.5550\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6476, train_acc = 0.6173, val_acc = 0.5450\n",
            "\n",
            "--------\n",
            "EPOCH 2\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0570\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6478, train_acc = 0.6163, val_acc = 0.5650\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6480, train_acc = 0.6149, val_acc = 0.5650\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6476, train_acc = 0.6151, val_acc = 0.5650\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6469, train_acc = 0.6173, val_acc = 0.5650\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6470, train_acc = 0.6149, val_acc = 0.5650\n",
            "\n",
            "--------\n",
            "EPOCH 3\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0609\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6447, train_acc = 0.6183, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6423, train_acc = 0.6236, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6413, train_acc = 0.6204, val_acc = 0.5800\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6412, train_acc = 0.6246, val_acc = 0.6100\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6409, train_acc = 0.6263, val_acc = 0.5950\n",
            "\n",
            "--------\n",
            "EPOCH 4\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0381\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6369, train_acc = 0.6260, val_acc = 0.5900\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6356, train_acc = 0.6306, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6363, train_acc = 0.6287, val_acc = 0.5900\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6361, train_acc = 0.6277, val_acc = 0.5950\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6351, train_acc = 0.6319, val_acc = 0.5950\n",
            "\n",
            "--------\n",
            "EPOCH 5\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0579\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6424, train_acc = 0.6180, val_acc = 0.6450\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6428, train_acc = 0.6180, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6400, train_acc = 0.6192, val_acc = 0.6600\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6401, train_acc = 0.6204, val_acc = 0.6550\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6391, train_acc = 0.6197, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 6\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0745\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6358, train_acc = 0.6311, val_acc = 0.6000\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6363, train_acc = 0.6384, val_acc = 0.5950\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6350, train_acc = 0.6355, val_acc = 0.5950\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6330, train_acc = 0.6399, val_acc = 0.5750\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6335, train_acc = 0.6374, val_acc = 0.6200\n",
            "\n",
            "--------\n",
            "EPOCH 7\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1143\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6437, train_acc = 0.6190, val_acc = 0.6050\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6429, train_acc = 0.6246, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6420, train_acc = 0.6260, val_acc = 0.6200\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6411, train_acc = 0.6217, val_acc = 0.6050\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6419, train_acc = 0.6234, val_acc = 0.5750\n",
            "\n",
            "--------\n",
            "EPOCH 8\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0934\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6345, train_acc = 0.6304, val_acc = 0.5900\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6344, train_acc = 0.6299, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6329, train_acc = 0.6309, val_acc = 0.5900\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6319, train_acc = 0.6336, val_acc = 0.5950\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6326, train_acc = 0.6326, val_acc = 0.5800\n",
            "\n",
            "--------\n",
            "EPOCH 9\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1088\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6399, train_acc = 0.6272, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6386, train_acc = 0.6275, val_acc = 0.6450\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6395, train_acc = 0.6297, val_acc = 0.6500\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6376, train_acc = 0.6304, val_acc = 0.6400\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6387, train_acc = 0.6345, val_acc = 0.6400\n",
            "\n",
            "--------\n",
            "EPOCH 10\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1084\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6339, train_acc = 0.6345, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6336, train_acc = 0.6360, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6335, train_acc = 0.6350, val_acc = 0.6450\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6306, train_acc = 0.6406, val_acc = 0.6550\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6304, train_acc = 0.6404, val_acc = 0.6600\n",
            "\n",
            "--------\n",
            "EPOCH 11\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1536\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6421, train_acc = 0.6231, val_acc = 0.6450\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6404, train_acc = 0.6270, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6391, train_acc = 0.6270, val_acc = 0.6400\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6387, train_acc = 0.6289, val_acc = 0.6200\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6375, train_acc = 0.6302, val_acc = 0.6450\n",
            "\n",
            "--------\n",
            "EPOCH 12\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1472\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6428, train_acc = 0.6287, val_acc = 0.6500\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6430, train_acc = 0.6304, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6415, train_acc = 0.6331, val_acc = 0.6600\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6413, train_acc = 0.6311, val_acc = 0.6500\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6404, train_acc = 0.6391, val_acc = 0.6500\n",
            "\n",
            "--------\n",
            "EPOCH 13\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1694\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6382, train_acc = 0.6401, val_acc = 0.6700\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6368, train_acc = 0.6384, val_acc = 0.7000\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6347, train_acc = 0.6387, val_acc = 0.6800\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6334, train_acc = 0.6387, val_acc = 0.6800\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6342, train_acc = 0.6467, val_acc = 0.6750\n",
            "\n",
            "--------\n",
            "EPOCH 14\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1820\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6437, train_acc = 0.6294, val_acc = 0.6450\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6439, train_acc = 0.6311, val_acc = 0.6500\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6440, train_acc = 0.6345, val_acc = 0.6400\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6415, train_acc = 0.6340, val_acc = 0.6450\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6420, train_acc = 0.6370, val_acc = 0.6450\n",
            "\n",
            "--------\n",
            "EPOCH 15\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2190\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6408, train_acc = 0.6350, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6391, train_acc = 0.6396, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6387, train_acc = 0.6379, val_acc = 0.6550\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6371, train_acc = 0.6430, val_acc = 0.6450\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6364, train_acc = 0.6459, val_acc = 0.6400\n",
            "\n",
            "--------\n",
            "EPOCH 16\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2128\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6421, train_acc = 0.6294, val_acc = 0.6200\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6385, train_acc = 0.6340, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6381, train_acc = 0.6357, val_acc = 0.6400\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6376, train_acc = 0.6333, val_acc = 0.6300\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6368, train_acc = 0.6367, val_acc = 0.6050\n",
            "\n",
            "--------\n",
            "EPOCH 17\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2272\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6472, train_acc = 0.6302, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6463, train_acc = 0.6319, val_acc = 0.7050\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6452, train_acc = 0.6326, val_acc = 0.6950\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6440, train_acc = 0.6316, val_acc = 0.6600\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6446, train_acc = 0.6321, val_acc = 0.6850\n",
            "\n",
            "--------\n",
            "EPOCH 18\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2289\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6413, train_acc = 0.6340, val_acc = 0.6150\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6370, train_acc = 0.6333, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6379, train_acc = 0.6319, val_acc = 0.6000\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6358, train_acc = 0.6387, val_acc = 0.5950\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6355, train_acc = 0.6311, val_acc = 0.5950\n",
            "\n",
            "--------\n",
            "EPOCH 19\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2270\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6476, train_acc = 0.6268, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6426, train_acc = 0.6263, val_acc = 0.6250\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6440, train_acc = 0.6207, val_acc = 0.6200\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6433, train_acc = 0.6268, val_acc = 0.6200\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6399, train_acc = 0.6289, val_acc = 0.6150\n",
            "\n",
            "--------\n",
            "EPOCH 20\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2297\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6441, train_acc = 0.6372, val_acc = 0.6050\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6410, train_acc = 0.6365, val_acc = 0.6050\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6409, train_acc = 0.6357, val_acc = 0.6200\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6415, train_acc = 0.6314, val_acc = 0.6250\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6407, train_acc = 0.6377, val_acc = 0.6050\n",
            "\n",
            "--------\n",
            "EPOCH 21\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2139\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6377, train_acc = 0.6348, val_acc = 0.6550\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6362, train_acc = 0.6442, val_acc = 0.6100\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6355, train_acc = 0.6394, val_acc = 0.6450\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6360, train_acc = 0.6365, val_acc = 0.6600\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6337, train_acc = 0.6379, val_acc = 0.6600\n",
            "\n",
            "--------\n",
            "EPOCH 22\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.2250\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6355, train_acc = 0.6391, val_acc = 0.6700\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6343, train_acc = 0.6362, val_acc = 0.6600\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6341, train_acc = 0.6399, val_acc = 0.6850\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6325, train_acc = 0.6406, val_acc = 0.6800\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6322, train_acc = 0.6421, val_acc = 0.6800\n",
            "\n",
            "--------\n",
            "EPOCH 23\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1770\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6248, train_acc = 0.6518, val_acc = 0.6950\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6205, train_acc = 0.6542, val_acc = 0.6900\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6205, train_acc = 0.6571, val_acc = 0.7000\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6177, train_acc = 0.6600, val_acc = 0.7150\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6190, train_acc = 0.6569, val_acc = 0.7150\n",
            "\n",
            "--------\n",
            "EPOCH 24\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1870\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6248, train_acc = 0.6569, val_acc = 0.7150\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6226, train_acc = 0.6520, val_acc = 0.6700\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6237, train_acc = 0.6510, val_acc = 0.6800\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6207, train_acc = 0.6569, val_acc = 0.6800\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6197, train_acc = 0.6530, val_acc = 0.6800\n",
            "\n",
            "--------\n",
            "EPOCH 25\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1853\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6306, train_acc = 0.6404, val_acc = 0.6150\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6296, train_acc = 0.6425, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6299, train_acc = 0.6450, val_acc = 0.6350\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6281, train_acc = 0.6486, val_acc = 0.6550\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6314, train_acc = 0.6425, val_acc = 0.6250\n",
            "\n",
            "--------\n",
            "EPOCH 26\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1308\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6170, train_acc = 0.6622, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6161, train_acc = 0.6557, val_acc = 0.6850\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6153, train_acc = 0.6547, val_acc = 0.6850\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6167, train_acc = 0.6586, val_acc = 0.6450\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6132, train_acc = 0.6627, val_acc = 0.6950\n",
            "\n",
            "--------\n",
            "EPOCH 27\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1280\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6260, train_acc = 0.6481, val_acc = 0.6550\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6221, train_acc = 0.6518, val_acc = 0.6850\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6249, train_acc = 0.6474, val_acc = 0.6550\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6215, train_acc = 0.6491, val_acc = 0.6550\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6199, train_acc = 0.6537, val_acc = 0.6700\n",
            "\n",
            "--------\n",
            "EPOCH 28\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1253\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6234, train_acc = 0.6540, val_acc = 0.6950\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6191, train_acc = 0.6525, val_acc = 0.7050\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6159, train_acc = 0.6583, val_acc = 0.6650\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6146, train_acc = 0.6668, val_acc = 0.7100\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6134, train_acc = 0.6632, val_acc = 0.6950\n",
            "\n",
            "--------\n",
            "EPOCH 29\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.1027\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6121, train_acc = 0.6603, val_acc = 0.6700\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6091, train_acc = 0.6661, val_acc = 0.7000\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6065, train_acc = 0.6705, val_acc = 0.6900\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6086, train_acc = 0.6622, val_acc = 0.7100\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6032, train_acc = 0.6731, val_acc = 0.7250\n",
            "\n",
            "--------\n",
            "EPOCH 30\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0724\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6140, train_acc = 0.6663, val_acc = 0.5900\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6114, train_acc = 0.6668, val_acc = 0.5850\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6097, train_acc = 0.6673, val_acc = 0.5900\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6104, train_acc = 0.6627, val_acc = 0.6100\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6051, train_acc = 0.6724, val_acc = 0.5900\n",
            "\n",
            "--------\n",
            "EPOCH 31\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0722\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6100, train_acc = 0.6646, val_acc = 0.6950\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6087, train_acc = 0.6603, val_acc = 0.7100\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6088, train_acc = 0.6693, val_acc = 0.6850\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6044, train_acc = 0.6724, val_acc = 0.7200\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6060, train_acc = 0.6688, val_acc = 0.7100\n",
            "\n",
            "--------\n",
            "EPOCH 32\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0549\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6030, train_acc = 0.6775, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6019, train_acc = 0.6712, val_acc = 0.6250\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6040, train_acc = 0.6734, val_acc = 0.6350\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.5991, train_acc = 0.6802, val_acc = 0.6600\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.5993, train_acc = 0.6756, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 33\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0686\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6009, train_acc = 0.6700, val_acc = 0.7450\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.5991, train_acc = 0.6707, val_acc = 0.6950\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.5995, train_acc = 0.6741, val_acc = 0.7300\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.5985, train_acc = 0.6702, val_acc = 0.7300\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.5952, train_acc = 0.6739, val_acc = 0.7350\n",
            "\n",
            "--------\n",
            "EPOCH 34\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0576\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.5948, train_acc = 0.6739, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.5898, train_acc = 0.6778, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.5928, train_acc = 0.6807, val_acc = 0.6200\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.5896, train_acc = 0.6787, val_acc = 0.6250\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.5889, train_acc = 0.6833, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 35\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0430\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6078, train_acc = 0.6586, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6023, train_acc = 0.6615, val_acc = 0.6600\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6052, train_acc = 0.6659, val_acc = 0.6650\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6047, train_acc = 0.6588, val_acc = 0.6650\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6053, train_acc = 0.6680, val_acc = 0.6300\n",
            "\n",
            "--------\n",
            "EPOCH 36\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0283\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6016, train_acc = 0.6603, val_acc = 0.6650\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.5980, train_acc = 0.6722, val_acc = 0.6850\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.5972, train_acc = 0.6654, val_acc = 0.6900\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.5972, train_acc = 0.6741, val_acc = 0.6600\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.5997, train_acc = 0.6717, val_acc = 0.6850\n",
            "\n",
            "--------\n",
            "EPOCH 37\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 1.9998\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6108, train_acc = 0.6532, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6065, train_acc = 0.6659, val_acc = 0.6800\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6051, train_acc = 0.6612, val_acc = 0.6650\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6030, train_acc = 0.6722, val_acc = 0.6850\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6039, train_acc = 0.6666, val_acc = 0.6950\n",
            "\n",
            "--------\n",
            "EPOCH 38\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0157\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6047, train_acc = 0.6629, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6017, train_acc = 0.6676, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.5998, train_acc = 0.6736, val_acc = 0.6650\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.5997, train_acc = 0.6778, val_acc = 0.6700\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6004, train_acc = 0.6649, val_acc = 0.6450\n",
            "\n",
            "--------\n",
            "EPOCH 39\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0103\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6104, train_acc = 0.6605, val_acc = 0.6750\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6084, train_acc = 0.6559, val_acc = 0.6950\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6087, train_acc = 0.6646, val_acc = 0.6800\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6074, train_acc = 0.6620, val_acc = 0.6750\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6057, train_acc = 0.6625, val_acc = 0.7150\n",
            "\n",
            "--------\n",
            "EPOCH 40\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0106\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6120, train_acc = 0.6569, val_acc = 0.6850\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6048, train_acc = 0.6591, val_acc = 0.6850\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6052, train_acc = 0.6603, val_acc = 0.6950\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6056, train_acc = 0.6620, val_acc = 0.6650\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6044, train_acc = 0.6646, val_acc = 0.6800\n",
            "\n",
            "--------\n",
            "EPOCH 41\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0012\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6121, train_acc = 0.6574, val_acc = 0.5900\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6109, train_acc = 0.6578, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6090, train_acc = 0.6537, val_acc = 0.5950\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6076, train_acc = 0.6612, val_acc = 0.6300\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6064, train_acc = 0.6569, val_acc = 0.5800\n",
            "\n",
            "--------\n",
            "EPOCH 42\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 2.0748\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : .......... average_loss = 0.6135, train_acc = 0.6569, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : .......... average_loss = 0.6132, train_acc = 0.6552, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : .......... average_loss = 0.6132, train_acc = 0.6578, val_acc = 0.6300\n",
            "d-step 1 epoch 4 : .......... average_loss = 0.6112, train_acc = 0.6578, val_acc = 0.6650\n",
            "d-step 1 epoch 5 : .......... average_loss = 0.6084, train_acc = 0.6634, val_acc = 0.6600\n",
            "\n",
            "--------\n",
            "EPOCH 43\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-4a1d34054896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAdversarial Training Generator : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain_generator_PG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_PG_BATCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# TRAIN DISCRIMINATOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-5a96faaf8d09>\u001b[0m in \u001b[0;36mtrain_generator_PG\u001b[0;34m(gen, gen_opt, oracle, dis, num_batches)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# sample from generator and compute oracle NLL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n\u001b[0;32m---> 55\u001b[0;31m                                                    start_letter=START_LETTER, gpu=CUDA)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mloss_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-fcc25996d6d4>\u001b[0m in \u001b[0;36mbatchwise_oracle_nll\u001b[0;34m(gen, oracle, num_samples, batch_size, max_seq_len, start_letter, gpu)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_generator_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moracle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0moracle_nll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moracle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-30730b89ccd3>\u001b[0m in \u001b[0;36mbatchNLLLoss\u001b[0;34m(self, inp, target)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-30730b89ccd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, hidden)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;31m# batch_size x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# 1 x batch_size x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# 1 x batch_size x hidden_dim (out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru2out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# batch_size x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 735\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hU8zk3o2wbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "06e71105-3f51-4cb9-f23f-fc3ad7d5c066"
      },
      "source": [
        "#Sample sequences from the Generator\n",
        "\n",
        "#gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=False)\n",
        "#dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=False)\n",
        "\n",
        "#gen.cuda()\n",
        "#dis.cuda()\n",
        "\n",
        "#gen.load_state_dict(torch.load('/content/gen.pth',  map_location=torch.device('cpu')))\n",
        "#dis.load_state_dict(torch.load('/content/dis.pth',  map_location=torch.device('cpu')))\n",
        "\n",
        "#gen.eval()\n",
        "#dis.eval()\n",
        "\n",
        "num_seqs = 25\n",
        "\n",
        "a = gen.sample(num_seqs).tolist()\n",
        "\n",
        "for i in range(num_seqs):\n",
        "  seq = (vector_to_sequence(a[i]))\n",
        "  seq = re.sub('[X]+$', '', seq)\n",
        "  check_x = re.search('[X]', seq)\n",
        "  if check_x:\n",
        "    print(seq + ', Invalid')\n",
        "  else:\n",
        "    print(\">pep\" + str(i))\n",
        "    print(seq)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WTMMMWWTMMAWYWYYXW, Invalid\n",
            ">pep1\n",
            "WYYMMATQTTATTTQTCT\n",
            ">pep2\n",
            "WWMYMMMMTWYMATWTWM\n",
            ">pep3\n",
            "WWMMTMLTTATKAWMMAT\n",
            ">pep4\n",
            "WYMMTTTTTTTTTTTTTT\n",
            "WMMMMMMTTXWTTTTYYY, Invalid\n",
            ">pep6\n",
            "WWMTTTTTTTTTTTTTTT\n",
            ">pep7\n",
            "WYWMMTMTYTYYYYLTTT\n",
            ">pep8\n",
            "RKWMKMYMYMMTKYMYYM\n",
            "MYXMMYMWMMLFTQQQQT, Invalid\n",
            ">pep10\n",
            "WYMMMMMQWQMMSIVQVQ\n",
            ">pep11\n",
            "WMMMMCLFQRQVGWWWWQ\n",
            ">pep12\n",
            "WMYMWMMMMMMMMAAWMQ\n",
            ">pep13\n",
            "WWWWMYMYWQTWATKTKY\n",
            ">pep14\n",
            "KYYMMMMWWWWMTWKWWW\n",
            ">pep15\n",
            "WFMATFWKQATEIMTATQ\n",
            ">pep16\n",
            "KWMMMMMTWWMMMMWWWR\n",
            ">pep17\n",
            "WYYMTWTTYYTYYMMYYY\n",
            "WYMMMMMXTTTMYYATAK, Invalid\n",
            "QYYCMXXAIVQQSQQVQR, Invalid\n",
            ">pep20\n",
            "YMMMWYWMMWQ0RMYWMT\n",
            ">pep21\n",
            "WWMMMYMMYWAYYMYTWW\n",
            ">pep22\n",
            "KQMMMTTTTTTTTTTTTT\n",
            ">pep23\n",
            "QYYATCCVQQQTQQTQIS\n",
            "WWMMWMXYMTATEQAAAT, Invalid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vPKOBIsfHU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9dcb1028-163e-49d1-fd51-5b5d5b335e17"
      },
      "source": [
        "#Writes output of main() cell to file \n",
        "#which can be downloaded if needed\n",
        "\n",
        "with open('losses.txt', 'w') as f:\n",
        "    f.write(cap.stdout)\n",
        "\n",
        "Download = True\n",
        "\n",
        "if Download:\n",
        "  files.download('losses.txt') \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1d80cd6d-de36-4b5e-8bb4-de7291c8c4cc\", \"losses.txt\", 162764)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWi77tY6CAiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "7778e064-3bdf-4d2b-9763-a87c8dabe455"
      },
      "source": [
        "oracle.sample(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c48be2813794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-30730b89ccd3>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, start_letter)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# out: num_samples x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# num_samples x 1 (sampling from each row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-30730b89ccd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# input dim                                             # batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;31m# batch_size x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# 1 x batch_size x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# 1 x batch_size x hidden_dim (out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7kAuBfImGd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open('losses.txt', 'r') \n",
        "lines = file1.readlines() \n",
        "\n",
        "loss_g = []\n",
        "loss_d = []\n",
        "\n",
        "for line in lines:\n",
        "\n",
        "  reg_lossg = re.search('oracle_sample_NLL = ([-+]?[0-9]*\\.?[0-9]+)', line)\n",
        "  reg_lossd = re.search('average_loss = ([-+]?[0-9]*\\.?[0-9]+)', line)  \n",
        "  if reg_lossg:\n",
        "    loss_g.append(float(reg_lossg.group(1)))\n",
        "  if reg_lossd:\n",
        "    loss_d.append(float(reg_lossd.group(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHI3oUu755P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0b4422f0-7a58-4142-bdbe-fafd311ac38d"
      },
      "source": [
        "#Which loss to plot? 'g' = Generator, 'd' = Discriminator\n",
        "\n",
        "plt_loss = 'g'\n",
        "\n",
        "if plt_loss == 'g':\n",
        "  plt.title('Learning Curve  - Generator')\n",
        "  plt.ylabel('NLL by Oracle')\n",
        "  plt.xlabel('Update')\n",
        "  plt.plot(loss_g)\n",
        "\n",
        "if plt_loss == 'd':\n",
        "  plt.title('Learning Curve - Discriminator')\n",
        "  plt.ylabel('BCE Loss')\n",
        "  plt.xlabel('Update')\n",
        "  plt.plot(loss_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBV5pqLg_pna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the Models\n",
        "\n",
        "torch.save(gen.state_dict(), '/content/gen.pth')\n",
        "torch.save(dis.state_dict(), '/content/dis.pth')\n",
        "\n",
        "#Load the Models\n",
        "# model = Generator(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5LdNaWQcpiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "beb0f947-a7b0-4fcf-8d2c-200d96aa76f6"
      },
      "source": [
        "files.download('dis.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5a58417c-985a-4262-bd89-f9eb21502699\", \"dis.pth\", 124417)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uka-EQ-ucvUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8cf0c997-761d-4908-dd06-b189d5e0a61b"
      },
      "source": [
        "# gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=True)\n",
        "# dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=True)\n",
        "\n",
        "# gen.cuda()\n",
        "# dis.cuda()\n",
        "\n",
        "# gen.load_state_dict(torch.load('/content/gen.pth'))\n",
        "# dis.load_state_dict(torch.load('/content/dis.pth'))\n",
        "\n",
        "# gen.eval()\n",
        "# dis.eval()\n",
        "\n",
        "print('\\nStarting Adversarial Training...')\n",
        "oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                            start_letter=START_LETTER, gpu=CUDA)\n",
        "print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
        "\n",
        "loss_frozen = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
        "    # TRAIN GENERATOR\n",
        "    print('\\nAdversarial Training Generator : ', end='')\n",
        "    sys.stdout.flush()\n",
        "    train_generator_PG(gen, gen_optimizer, oracle, dis, 1)\n",
        "\n",
        "    TRAIN DISCRIMINATOR\n",
        "   print('\\nSkkipping adversarial Training Discriminator : ')\n",
        "   train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, ADV_D_STEPS, ADV_D_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (embeddings): Embedding(22, 3)\n",
              "  (gru): GRU(3, 32, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (gru2hidden): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (dropout_linear): Dropout(p=0.2, inplace=False)\n",
              "  (hidden2out): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x2DUI6YfyQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "69661976-f097-4d11-a518-eea8fa7a9c69"
      },
      "source": [
        "def noise():\n",
        "  print(\"AAAAAAAHHHH\")\n",
        "\n",
        "a = noise()\n",
        "\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AAAAAAAHHHH\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC1RU9biq6sP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "af321cc6-96d1-4c39-a263-3414ee7c0f4d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10, 18,  6, 21,  6,  6, 18,  6,  6, 18,  6,  6,  6,  6, 18,  6,  6,  6],\n",
              "        [16, 15, 10,  8,  6, 17, 15, 18, 18,  6,  6, 17, 17, 18,  6,  6, 18,  6],\n",
              "        [10, 16, 15, 20,  8,  2,  2, 17, 16, 18, 18,  3, 17,  3, 15,  6, 18,  6],\n",
              "        [ 5,  6, 20,  6, 15, 10, 18, 18, 21,  6,  6, 18,  5,  6,  6,  5,  9,  6],\n",
              "        [10, 18,  6, 21,  6,  6,  6, 15, 20, 18,  2, 18,  6,  6,  6, 18,  3, 18],\n",
              "        [10, 15,  6, 15,  8,  2,  6, 17,  3, 17,  3, 17, 15,  8, 18,  2, 15,  8],\n",
              "        [10,  6,  6, 17, 18,  6,  6, 18, 18,  8, 18, 18,  6, 18,  6, 18,  2, 18],\n",
              "        [10,  6,  6,  6, 17, 17,  2, 18, 18,  8, 18,  6, 17,  2, 18, 18,  6, 18],\n",
              "        [10,  6, 15, 20, 18,  2,  2, 18,  2,  6, 17, 15,  8, 18,  6,  8, 18,  2],\n",
              "        [10,  6, 17, 16, 18, 16,  2,  2, 17,  2, 17,  2, 17,  2, 18,  5,  8,  2]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeBhUkijCIvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "\n",
        "gru = nn.GRU(2, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PQZqNNfB-mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9e60b4f4-1382-4d5b-9642-50b64c44625c"
      },
      "source": [
        "emb = torch.Tensor([0.3, 0.8])\n",
        "emb = emb.view(1, -1, 2)\n",
        "h = autograd.Variable(torch.zeros(1, 1, 10))\n",
        "\n",
        "out, hidden = gru(emb, h)\n",
        "\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1752, -0.1807, -0.1555, -0.1366, -0.0942, -0.1480, -0.0925,\n",
            "           0.1555,  0.0559, -0.0547]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOF0ghb6CwSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}